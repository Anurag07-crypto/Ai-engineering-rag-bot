{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d46d25",
   "metadata": {},
   "source": [
    "### Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388fea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Desktop\\Next Plan\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb \n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aacd4a8",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ebec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  6.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 0}, page_content='LangChain\\nVasilios Mavroudis\\nAlan Turing Institute\\nvmavroudis@turing.ac.uk\\nAbstract. LangChain is a rapidly emerging framework that offers a ver-\\nsatile and modular approach to developing applications powered by large\\nlanguage models (LLMs). By leveraging LangChain, developers can sim-\\nplify complex stages of the application lifecycle—such as development,\\nproductionization, and deployment—making it easier to build scalable,\\nstateful, and contextually aware applications. It provides tools for han-\\ndling chat models, integrating retrieval-augmented generation (RAG),\\nand offering secure API interactions. With LangChain, rapid deployment\\nof sophisticated LLM solutions across diverse domains becomes feasible.\\nHowever, despite its strengths, LangChain’s emphasis on modularity and\\nintegration introduces complexities and potential security concerns that\\nwarrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,\\nLangServe, and LangSmith. We explore how the framework facilitates the\\ndevelopment of LLM applications, discuss its applications across multi-\\nple domains, and critically evaluate its limitations in terms of usability,\\nsecurity, and scalability. By offering valuable insights into both the capa-\\nbilities and challenges of LangChain, this paper serves as a key resource\\nfor developers and researchers interested in leveraging LangChain for\\ninnovative and secure LLM-powered applications.\\nKeywords: LangChain · Large Language Models · LLM Applications ·\\nModular Framework\\nThe emergence of large language models (LLMs) such as OpenAI’s o1 [13],\\nGPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-\\nlocked unprecedented capabilities in understanding and generating human-like\\ntext, enabling applications that range from intelligent conversational agents to\\nsophisticated data analysis tools. However, harnessing the full potential of LLMs\\nin real-world applications presents significant challenges. Developers must nav-\\nigate complexities related to model integration, state management, scalability,\\ncontextual awareness, and security.\\nLangChain has rapidly gained prominence as a powerful framework designed\\nto address these challenges in developing LLM-powered applications [2]. By\\nproviding a modular and flexible architecture, LangChain simplifies the com-\\nplexities inherent in working with LLMs, enabling developers to build scalable,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 1}, page_content='2\\nVasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nable API deployment, and LangSmith for monitoring and evaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1\\nArchitecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 2}, page_content='LangChain\\n3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1. LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPrompt Templates: Structured templates that standardize and format queries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 3}, page_content='4\\nVasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks: Functions that log, monitor, and stream specific events within LangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1\\nChat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2\\nRetrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 4}, page_content='LangChain\\n5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwith vector storage solutions like Chroma and Milvus for optimized searches [3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3\\nSecurity and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4\\nIntegrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– Integration Packages: LangChain provides dedicated packages (e.g., langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– Custom Component Development: Developers can build custom plugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 5}, page_content='6\\nVasilios Mavroudis\\n1.5\\nAdvanced Components\\nBeyond these core elements, LangChain offers advanced modules that support\\ncomplex workflows, API deployments, and performance monitoring. These com-\\nponents are elaborated in the following sections:\\n– LangGraph for Stateful Process Modeling: Explored in Section 3,\\nLangGraph enables developers to structure applications with nodes and\\nedges, allowing for complex branching and multi-agent workflows.\\n– LangServe for API Deployment: Detailed in Section 4, LangServe facil-\\nitates the deployment of LangChain applications as REST APIs, supporting\\nscalability in production [5].\\n– LangSmith for Monitoring and Evaluation: Discussed in Section 2,\\nLangSmith offers tools for real-time performance monitoring, error tracking,\\nand version control to optimize applications iteratively [4].\\n2\\nLangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-\\ning essential tools for building production-grade systems. Integrated with the\\nLangChain ecosystem, it enables users to trace, evaluate, and refine applications,\\nenhancing precision in complex environments. The platform addresses key chal-\\nlenges in observability, testing, and optimization, allowing developers to monitor\\nperformance, troubleshoot issues, and maintain high standards over time [9].\\n2.1\\nTracing\\nTracing is a central feature of LangSmith, providing detailed visibility into how\\napplications interact with LLMs and external data sources. For developers using\\nLangChain, LangSmith offers tracing without the need for direct SDK inte-\\ngration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-\\nserve and analyze component behavior in real time. This capability is especially\\nuseful for debugging and identifying bottlenecks within complex workflows.\\nLangSmith supports multiple ways to log traces, including languages like\\nPython and TypeScript. Each trace logs every call made to an LLM, along with\\ninput parameters, function annotations, and generated outputs, making it easier\\nto identify where adjustments may be necessary. By using traceable wrappers\\nand decorators, developers can annotate functions or data pipelines, enabling\\nautomatic trace logging with minimal code alterations [9].\\n2.2\\nPerformance Testing\\nLangSmith’s evaluation tools enable developers to test and validate applica-\\ntions under real-world conditions. Evaluations require a defined dataset of test'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 6}, page_content='LangChain\\n7\\ncases, including inputs and expected outputs. Using these datasets, developers\\ncan conduct performance tests and assess how well their models meet expected\\noutcomes—an essential step for applications where accuracy and reliability are\\ncrucial. LangSmith supports custom evaluators, allowing developers to specify\\nscoring functions based on specific needs. For instance, an evaluator may mea-\\nsure the exact match between outputs and expected answers, or use metrics\\nlike cosine similarity for open-ended tasks. By supporting both built-in and cus-\\ntom evaluators, LangSmith provides flexibility in performance measurement for\\ndeterministic outputs or nuanced language generation tasks [9].\\n2.3\\nDataset Management\\nDatasets are foundational to LangSmith’s evaluation system. Organizing test\\ncases into structured datasets enables methodical testing and validation. De-\\nvelopers can create datasets manually or import them from existing sources,\\nallowing diverse testing scenarios that reflect real-world use cases. Datasets can\\ncontain structured or unstructured data evaluations, accommodating a variety of\\ntesting needs. LangSmith’s dataset version control allows developers to maintain\\nmultiple dataset versions as applications evolve. This feature is critical for ensur-\\ning consistency in evaluation, especially as application logic changes or models\\nare retrained, providing a robust foundation for testing and validation [9].\\n2.4\\nLangSmith Workflow\\nLangSmith integrates tracing, evaluation, and dataset management into a cohe-\\nsive framework, enabling developers to progress from debugging to optimization\\nin a structured manner. A typical LangSmith workflow includes the following\\nstages:\\n– Trace Logging: Developers activate tracing on application functions, pro-\\nviding insights into model-component interactions.\\n– Dataset Creation and Evaluation: Developers create datasets represent-\\ning different scenarios to conduct comprehensive testing.\\n– Evaluation and Iterative Optimization: Evaluation results indicate per-\\nformance areas for refinement, guiding iterative application improvements.\\n– Version Control and Historical Tracking: LangSmith logs all interac-\\ntions, dataset versions, and evaluation scores, allowing developers to assess\\nimprovements over time.\\n2.5\\nIntegration with LangChain and LangServe\\nLangSmith integrates seamlessly with LangChain and LangServe (Section 4) to\\nenhance the end-to-end LLM application development experience. For LangChain\\nusers, LangSmith can automatically log traces and integrate with existing work-\\nflows. Combined with LangServe, LangSmith provides robust observability for\\nAPI deployments, monitoring real-time usage patterns, tracking request laten-\\ncies, and identifying bottlenecks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 7}, page_content='8\\nVasilios Mavroudis\\n3\\nLangGraph\\nLangGraph is a low-level framework for building stateful, multi-actor appli-\\ncations with large language models (LLMs). It provides developers with fine-\\ngrained control over application flows, incorporating cycles, branching, and per-\\nsistence to support complex agent workflows. Inspired by frameworks such as\\nPregel [11] and Apache Beam [15], LangGraph enables advanced human-in-the-\\nloop applications and persistent state management, allowing for more reliable\\nand adaptable LLM-powered systems [7].\\n3.1\\nCore Features of LangGraph\\nCycles and Branching LangGraph distinguishes itself by supporting cycles\\nand branching in application workflows. This feature is particularly beneficial\\nfor agentic architectures that require iterative or conditional logic. By enabling\\ncycles within workflows, LangGraph provides a flexible structure that allows\\nnodes to execute repeatedly until a specified condition is met. This contrasts\\nwith typical directed acyclic graph (DAG)-based architectures, which are limited\\nto single-pass execution without feedback loops [7].\\nPersistence and State Management One of LangGraph’s key innovations is\\nits built-in support for persistence, which enables state to be saved and accessed\\nthroughout the application’s lifecycle. This persistent state management is cru-\\ncial for applications that require continuity across sessions, such as customer ser-\\nvice agents or educational tools that need to recall previous interactions. Lang-\\nGraph’s persistence feature also facilitates advanced human-in-the-loop work-\\nflows, allowing agents to pause, receive human input, and resume operations\\nseamlessly.\\nLangGraph utilizes a stateful execution model where each node in the graph\\nupdates the application state as it processes input. For instance, in a multi-turn\\nconversation, the graph maintains a memory of all previous messages, which can\\nbe accessed by subsequent nodes to ensure coherent responses. This persistent\\nstate can also be saved externally using the LangGraph Platform [8], ensuring\\nrobust memory management across long-running sessions [7].\\nHuman-in-the-Loop and Streaming Support LangGraph offers built-in\\nsupport for human-in-the-loop interactions, which is essential for applications\\nthat require manual intervention or approval at certain stages. For example, a\\nhuman operator can review an agent’s planned actions and approve, reject, or\\nmodify them before the agent proceeds. This level of control makes LangGraph\\nsuitable for high-stakes domains like healthcare or legal advice, where accuracy\\nand oversight are critical.\\nAdditionally, LangGraph supports streaming outputs from each node as they\\nare produced. This capability is especially useful for applications like chatbots'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 8}, page_content='LangChain\\n9\\nor real-time monitoring systems, where immediate feedback improves user expe-\\nrience. Streaming can be implemented within any node in the graph, enabling\\nreal-time updates as actions are processed [7].\\n3.2\\nLangGraph Platform\\nThe LangGraph Platform [8] is an infrastructure solution that extends the\\nopen-source LangGraph framework for production deployments. It includes com-\\nponents like LangGraph Server (for API access), LangGraph SDKs (client li-\\nbraries), and LangGraph CLI (a command-line interface for deployment manage-\\nment). The platform is designed to handle complex agent workflows, supporting\\nlong-running agents, background processing, and task queuing to ensure reliable\\nperformance even under heavy loads. The LangGraph Platform also includes\\nfeatures such as:\\n– Background Execution: Allows agents to run asynchronously, handling\\nuser requests in parallel without blocking other tasks.\\n– Support for Long-Running Agents: Provides infrastructure for agents\\nthat need to operate over extended periods, managing resource allocation\\nand monitoring agent health.\\n– Burst Handling and Task Queues: Uses queues to manage sudden in-\\ncreases in requests, ensuring that high-priority tasks are processed efficiently.\\n3.3\\nLangGraph Workflow\\nA typical LangGraph workflow begins by defining the state schema and nodes\\nrequired for the application. Each node represents an independent function, such\\nas calling an LLM, invoking a tool, or accessing external data. The developer sets\\nan entry point for graph execution and defines the transitions (edges) between\\nnodes, which can be conditional or sequential based on application requirements.\\n– Defining Nodes and State: Developers initialize nodes, such as an LLM\\nnode for responses or a tool node for external API calls, and specify the state\\nschema to manage conversation context.\\n– Setting Entry Points and Edges: Nodes are connected by edges, with\\nconditions determining the flow based on the application’s state.\\n– Compiling and Executing the Graph: Once nodes and edges are defined,\\nthe graph is compiled into a runnable format, enabling calls to functions such\\nas invoke() for execution and stream() for real-time updates.\\nLangGraph’s workflow design allows applications to cycle between nodes\\nbased on input conditions and dynamically update state, enabling applications\\nthat require complex interaction patterns.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 9}, page_content='10\\nVasilios Mavroudis\\n3.4\\nIntegration with LangChain and LangSmith\\nLangGraph integrates seamlessly with LangChain and LangSmith, although\\nit operates independently if desired. For users within the LangChain ecosys-\\ntem, LangGraph can utilize LangSmith’s tracing capabilities to monitor each\\nnode’s performance and capture detailed logs of agent interactions. Addition-\\nally, LangChain tools and APIs can be incorporated as graph nodes, expanding\\nLangGraph’s functionality with LangChain’s extensive library of tools and con-\\nnectors [7].\\n4\\nLangServe\\nLangServe [5] is an integral component of the LangChain ecosystem, specifically\\ndesigned to facilitate the deployment of large language model (LLM) applications\\nas scalable REST APIs. With LangServe, developers can create production-\\ngrade APIs that allow external systems and users to interact with LangChain\\napplications. It enables LLM-powered applications to be served in real-time with\\nrobust support for load balancing, monitoring, and scalability.\\n4.1\\nCore Features of LangServe\\nAPI Deployment and Management LangServe simplifies the process of\\nturning LangChain applications into APIs, making LLM models accessible for\\nvarious services and client applications. With LangServe, any LangChain work-\\nflow can be packaged and exposed via RESTful endpoints, enabling interactions\\nwith language models, data retrieval, and external tool integration. The API-\\ncentric design allows LangServe to support diverse use cases, from chatbots and\\nrecommendation systems to complex multi-agent interactions. It provides several\\ntools for managing API endpoints, such as configurable routing, request han-\\ndling, and response formatting, which make it easy to customize each endpoint\\nbased on application requirements. This flexibility allows developers to design\\nAPIs with specific functionalities, making LangServe suitable for applications of\\nvarying complexity [5].\\nScalability and Load Balancing LangServe includes built-in support for scal-\\nability, making it ideal for applications that experience high traffic volumes. It\\ncan handle multiple API requests simultaneously, ensuring consistent perfor-\\nmance by balancing the load across instances. This feature is critical for pro-\\nduction environments where maintaining low response times under heavy loads\\nis essential. To further enhance scalability, LangServe provides tools for setting\\nup auto-scaling, which dynamically adjusts the number of instances based on\\ndemand. This allows applications to handle traffic spikes without degradation\\nin performance, making LangServe suitable for real-world deployments with un-\\npredictable usage patterns [5].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 10}, page_content='LangChain\\n11\\nLatency and Error Management LangServe is designed to minimize latency,\\nensuring quick responses for each API request. It employs efficient request queu-\\ning and processing mechanisms to reduce waiting times. Additionally, LangServe\\nincludes error handling and retry logic, which helps maintain reliability by man-\\naging transient failures and minimizing downtime. This focus on latency and\\nerror resilience makes LangServe suitable for mission-critical applications that\\nrequire high availability. For instance, LangServe can automatically retry failed\\nrequests and log errors for further investigation, allowing developers to identify\\nissues promptly and maintain a stable API response rate [5].\\n4.2\\nLangServe Workflow\\nThe LangServe deployment workflow is straightforward, enabling developers to\\ngo from a LangChain application to a deployed API in a few steps. Here’s an\\noutline of a typical LangServe workflow:\\n1. Defining Endpoints: Developers define endpoints based on application re-\\nquirements, specifying which functions or models are exposed via API routes.\\nEach endpoint can have customized parameters, allowing for flexibility in\\nhow the API interacts with different components.\\n2. Configuring Request Handling and Routing: LangServe allows for fine-\\ngrained control over how requests are processed. Developers can set up rout-\\ning rules, parameter validation, and request parsing to tailor the API expe-\\nrience.\\n3. Setting Up Load Balancing and Scaling: For applications with high\\ntraffic, LangServe’s load balancing can be configured to distribute requests\\nacross multiple instances, ensuring consistent response times. Auto-scaling\\ncan also be set up to dynamically adjust resources based on demand.\\n4. Monitoring and Error Tracking: LangServe integrates with monitoring\\ntools, including LangSmith, to provide real-time insights into API perfor-\\nmance, usage metrics, and error rates. This monitoring helps developers\\nmaintain optimal performance and quickly resolve issues as they arise.\\nLangServe’s streamlined workflow ensures that developers can deploy robust\\nAPIs with minimal overhead, making it a practical choice for scaling LLM ap-\\nplications in production environments.\\n4.3\\nIntegration with LangSmith and LangChain\\nLangServe integrates seamlessly with LangSmith, which offers observability fea-\\ntures like tracing, logging, and performance monitoring. Through LangSmith,\\nLangServe users can track metrics such as request frequency, latency, and er-\\nror rates. This integration provides a comprehensive view of API performance,\\nenabling developers to optimize applications based on real-time data.\\nAdditionally, LangServe’s integration with LangChain allows developers to\\nleverage LangChain’s extensive library of tools, models, and connectors as part'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 11}, page_content='12\\nVasilios Mavroudis\\nof the API. LangChain workflows, tools, and chains can be directly exposed\\nvia LangServe endpoints, providing flexible API interactions and enhancing the\\nfunctionality of LLM applications [5].\\n5\\nLimitations and Criticisms\\nLangChain provides a versatile framework for the development of applications\\npowered by large language models (LLMs). However, several limitations warrant\\nattention, especially in the domains of complexity and security.\\n5.1\\nComplexity\\nLangChain’s modular architecture, while designed to simplify LLM-based ap-\\nplication development, can paradoxically increase complexity. Effective use of\\nLangChain often requires a nuanced understanding of its distinct components,\\nsuch as LangGraph and LangSmith, as well as familiarity with its API ecosys-\\ntem. Consequently, developers may face a steep learning curve, particularly those\\naiming for rapid prototyping or deployment. The need for comprehensive knowl-\\nedge of each module may present a barrier to new users, complicating onboarding\\nand initial implementation phases.\\n5.2\\nSecurity Concerns\\nGiven LangChain’s modular design and extensive reliance on external integra-\\ntions, security presents a notable challenge. Although LangChain incorporates a\\nrange of security measures, including fine-grained permission control and sand-\\nboxing, the complexity of securing LLM-based applications remains high, espe-\\ncially for applications managing sensitive data. Below, we outline several critical\\nsecurity risks associated with LangChain and explore strategies for risk mitiga-\\ntion.\\nRisks Associated with External Providers To enhance functionality, LangChain\\nintegrates with numerous external services, such as vector databases, API providers,\\nand cloud storage platforms. However, these integrations expose applications to\\nsecurity vulnerabilities:\\n– Data Exposure: Accessing external resources can inadvertently expose sen-\\nsitive data to third-party providers, a risk particularly relevant for applica-\\ntions handling personal or confidential information. Without stringent data\\nencryption and access control mechanisms, the potential for data leaks or\\nunauthorized access increases.\\n– Third-Party Dependency: Reliance on third-party services introduces de-\\npendencies on their security protocols. Any compromise within a provider’s\\ninfrastructure could affect LangChain applications, resulting in data breaches\\nor service interruptions. This underscores the importance of thoroughly vet-\\nting providers and monitoring them for potential security issues.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 12}, page_content='LangChain\\n13\\nLangChain’s security model addresses many of these concerns, yet challenges\\npersist, particularly in sectors with rigorous compliance standards, such as fi-\\nnance and healthcare. Key areas for ongoing improvement include:\\n– Dynamic Permission Adjustment: Current permission settings in LangChain\\nare defined at deployment, but in dynamic applications, permissions may\\nneed to adapt based on user interactions. Implementing adaptive permis-\\nsions responsive to application state or user roles could enhance security.\\n– Advanced Encryption Standards: For applications processing highly\\nsensitive data, adopting advanced encryption practices—such as end-to-end\\nor field-level encryption—could bolster data security within even trusted\\nenvironments.\\n– Proactive Security Analytics: Integrating predictive analytics to pre-\\nemptively identify risks could further secure applications. Machine learning\\nmodels analyzing application logs could flag anomalous patterns indicative\\nof potential breaches or misuse.\\nIn summary, LangChain’s security framework includes robust features such\\nas granular permissions, sandboxing, and real-time monitoring. While these mea-\\nsures provide a solid foundation, the ongoing challenge of securing LLM-driven\\napplications—particularly those relying on external providers—demands contin-\\nued advancements in security practices.\\n6\\nConclusion\\nLangChain significantly advances the development of applications powered by\\nlarge language models (LLMs). Its modular framework—including components\\nlike LangGraph for stateful process modeling, LangServe for scalable API de-\\nployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-\\nverse domains, including NLP, cybersecurity, healthcare, finance, and customer\\nservice.\\nWhile its versatility extends beyond NLP, allowing for applications in fields\\nlike cybersecurity (e.g., threat detection and automated incident response), the\\nframework’s emphasis on flexibility introduces complexities that may present a\\nlearning curve for developers new to LangChain. Additionally, reliance on exter-\\nnal integrations raises important security considerations, such as data exposure\\nand dependency vulnerabilities, which are critical in sensitive areas where data\\nintegrity and privacy are paramount.\\nIn summary, LangChain’s transformative potential lies in bridging the gap\\nbetween the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-\\nments in usability and security, LangChain can continue to serve as a valuable\\ntool for developers seeking to leverage LLMs in building innovative and secure\\napplications. As industries increasingly adopt AI technologies, frameworks like\\nLangChain are poised to play a pivotal role in shaping the next generation of\\nintelligent, scalable, and secure solutions across various sectors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 13}, page_content='14\\nVasilios Mavroudis\\nReferences\\n1. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-\\nrencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\\nAnadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.\\n2. Harrison Chase.\\nLangChain, Oct 2022.\\nAvailable at https://github.com/\\nlangchain-ai/langchain.\\n3. LangChain, Inc. LangChain Documentation: Integration Providers. LangChain,\\nInc., San Francisco, CA, 2024. Available at https://python.langchain.com/docs/\\nintegrations/providers/.\\n4. LangChain, Inc.\\nLangChain Documentation: Key Concepts.\\nLangChain, Inc.,\\nSan Francisco, CA, 2024.\\nAvailable at https://python.langchain.com/docs/\\nconcepts/.\\n5. LangChain, Inc.\\nLangChain Documentation: LangServe.\\nLangChain, Inc.,\\nSan Francisco, CA, 2024.\\nAvailable at https://python.langchain.com/docs/\\nlangserve/.\\n6. LangChain, Inc. LangChain Documentation: Security Best Practices. LangChain,\\nInc., San Francisco, CA, 2024. Available at https://python.langchain.com/docs/\\nsecurity/.\\n7. LangChain, Inc. LangGraph: Building Language Agents as Graphs, 2024. Accessed:\\n2024-11-04.\\n8. LangChain, Inc. LangGraph Platform Documentation, 2024. Accessed: 2024-11-04.\\n9. LangChain, Inc. LangSmith: A Developer Platform for LLM Applications, 2024.\\nAccessed: 2024-11-04.\\n10. Patrick\\nLewis,\\nEthan\\nPerez,\\nAleksandra\\nPiktus,\\nFabio\\nPetroni,\\nVladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.\\nAdvances in Neural Information Processing Systems, 33:9459–9474, 2020.\\n11. Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan\\nHorn, Naty Leiser, and Grzegorz Czajkowski. Pregel: A System for Large-Scale\\nGraph Processing. In Proceedings of the 2010 ACM SIGMOD International Con-\\nference on Management of Data, pages 135–146, 2010.\\n12. OpenAI. Hello GPT-4O, 05 2024.\\n13. OpenAI. Introducing OpenAI O1-Preview, 09 2024.\\n14. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui\\nYu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican,\\net al. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint\\narXiv:2312.11805, 2023.\\n15. The Apache Software Foundation. Apache Beam: An Advanced Unified Program-\\nming Model, 2024. Accessed: 2024-11-04.\\n16. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv\\npreprint arXiv:2302.13971, 2023.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 0}, page_content='ASTRONOMICAL DATA ANALYSIS SOFTWARE AND SYSTEMS XIV\\nASP Conference Series, Vol. 347, 2005\\nP. L. Shopbell, M. C. Britton, and R. Ebert, eds.\\nmatplotlib – A Portable Python Plotting Package\\nPaul Barrett\\nSpace Telescope Science Institute\\nJohn Hunter\\nUniversity of Chicago\\nJ. Todd Miller, Jin-Chung Hsu, and Perry Greenﬁeld\\nSpace Telescope Science Institute\\nAbstract.\\nmatplotlib is a portable 2D plotting and imaging package aimed\\nprimarily at visualization of scientiﬁc, engineering, and ﬁnancial data.\\nmat-\\nplotlib can be used interactively from the Python shell, called from python\\nscripts, or embedded in a GUI application (GTK, Wx, Tk, Windows). Many\\npopular hardcopy outputs are supported including JPEG, PNG, PostScript and\\nSVG. Features include the creation of multiple axes and ﬁgures per page, inter-\\nactive navigation, many predeﬁned line styles and symbols, images, antialiasing,\\nalpha blending, date and ﬁnancial plots, W3C compliant font management and\\nFreeType2 support, legends and tables, pseudocolor plots, mathematical text\\nand more. It works with both numarray and Numeric. The goals of the pack-\\nage, basic architecture, current features (illustrated with examples), and planned\\nenhancements will be described.\\n1.\\nIntroduction\\nmatplotlib is designed with the philosophy that you should be able to create\\nsimple plots with just a few commands, or just one!\\nIf you want to see a\\nhistogram of your data, you shouldn’t need to instantiate objects, call methods,\\nset properties, etc; it should just work.\\nThe initial goals of matplotlib were:\\n• Plots should be publication quality; particularly the text (antialiased, ro-\\ntated, etc.).\\n• PostScript output for inclusion with TEX documents.\\n• Embeddable in a graphical user interface for application development.\\n• Code should be understandable.\\n• Making plots should be easy.\\n• The software is Open Source, so it can be downloaded, used, and dis-\\ntributed freely.\\nmatplotlib can be used in a variety of settings. Most users are familiar with\\nthe command-line for interactively creating plots and images.\\nThis interface\\nprovides a simple pop-up window for displaying and manipulating the data.\\nHowever, the true power of matplotlib is the underlying plotting library, which\\n91'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 1}, page_content='92\\nBarrett et al.\\n\\x00\\x02\\x01\\x02\\x01\\x02\\x01\\n\\x00\\x02\\x01\\x02\\x03\\x02\\x01\\n\\x00\\x02\\x01\\x05\\x04\\x06\\x01\\n\\x00\\x02\\x01\\x02\\x07\\x02\\x01\\n\\x00\\x02\\x01\\x05\\x08\\x06\\x01\\n)\\n\\t\\nA\\n(\\n\\n\\x01\\n\\x03\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x04\\x02\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x07\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x08\\x02\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x00\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x03\\n\\x00\\x02\\x10\\n\\x03\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x03\\n\\x00\\x02\\x10\\n\\x04\\x02\\x0b\\x11\\x0c\\x12\\x00\\x02\\x03\\n\\x13\\x14\\n\\x15\\n\\x16\\n\\x17\\x19\\x18\\x1b\\x1a\\x1d\\x1c\\x1f\\x1e\\x06 !\\x17#\"%$\\'&\\x11(*),+.-0/.1*243\\r56\\x1c*78$:9*;\\nFigure 1.\\nA line plot of FITS binary table data containing 10k points.\\nis operating system independent and graphical user interface (GUI) agnostic. It\\ncan be used without a GUI as part of a web server to create plots and images in\\na variety of hardcopy outputs; or can be embedded in a larger application using\\none of several GUIs (e.g. GTK, Tk, or WXwindows) running on one of several\\nOSs (e.g. Windows, OS X, Solaris, and Linux).\\n2.\\nArchitecture\\nThe matplotlib code is conceptually divided into three parts:\\n• The matlab interface is the set of functions that allow a user to create\\nplots from the command line.\\n• The frontend or matplotlib API is the set of classes that do the heavy\\nlifting by creating and managing ﬁgures, text, lines, plots, etc. This is the\\nabstract interface that knows nothing about output.\\n• The backends are device dependent drawing devices or renderers that\\ntransform the frontend representation to hardcopy (JPEG, PNG, PDF,\\nPS, SVG, Paint, GD) or a display device (Agg, GTK/GTKAgg, TkAgg,\\nWX/WXAgg). Much of the critical rendering code is written in C/C++\\nand therefore provides very good performance.\\nAgg is the Anti-Grain Graphics library that enables writing vector graphics\\nto a buﬀer, which can then be block transfered (or BLTed) to the display de-\\nvice. This means that all interactive implementations based on Agg avoid the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 2}, page_content='matplotlib – A Portable Python Plotting Package\\n93\\n<\\x0e=?>\\n@BA\\nCED\\nF0G\\n@\\nCED\\nF0GIH\\nCJD\\nF\\x19K\\nA\\nCED\\nF\\n@\\x12L\\nM\\n=?N\\nG\\nK\\nM\\n=?N\\nG\\x12O\\nM\\n=\\nN\\nK\\nP\\nM\\n=?N\\n@\\n@\\nM\\n=?N\\n@IH\\nQSR\\nN\\nG\\x12P\\nQSR\\nN\\nK\\n@\\nQSR\\nN\\nK\\nH\\nT%U\\nT%V\\nT%W\\nT%X\\nY%Z\\nY\\\\[\\nY%T\\nFigure 2.\\nA ﬁnancial plot that uses the daily high, low, and closing values\\nof a stock price.\\ngraphical limitations of the GUI and render identical graphics regardless of the\\nGUI interface.\\n3.\\nPlotting\\nThe following Python session uses the matlab interface to create a quicklook\\nspectrum of FUSE data (see Figure 1).\\n> python\\nPython 2.3.3 (#1, Jan\\n5 2004, 16:22:13)}\\n[GCC 2.96 20000731 (Red Hat Linux 7.3 2.96-113)] on linux2\\nType \"help\", \"copyright\", \"credits\" or \"license\"\\nfor more information.\\n>>> import pyfits\\n>>> fits = pyfits.open(’fuse.fits’)\\n>>> wave = fits[1].data.field(’wave’)\\n>>> flux = fits[1].data.field(’flux’)\\n>>> from matplotlib.matlab import *\\n>>> ylim(0, 1.5e-12)\\n>>> xlim(985, 1085)\\n>>> xlabel(r’$\\\\lambda\\\\ (\\\\angstrom)$’)\\n<matplotlib.text.Text instance at 0x41118f8c>\\n>>> ylabel(r’Flux’)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 3}, page_content='94\\nBarrett et al.\\n]\\n^_]%]\\n`%].]\\na%]%]\\nb%]%]\\ncS]%]\\nd.egf.hjiEiEk\\n]\\n^_].]\\n`%].]\\na%].]\\nb%].]\\nc_].]\\nl\\nmn\\nop\\nq\\nq\\nr\\nsutwvyx\\\\twz|{~}jz\\n\\x7f\\x80z|{\\x19\\x81\\x1dtwvy\\x82jtw}\\x83\\x82\\x1b\\x84\\x85x\\x02\\x81\\nFigure 3.\\nA graphic of FITS image data. Note the labeled axes and title.\\n<matplotlib.text.Text instance at 0x4112108c>\\n>>> title(’FUSE LiF 1A spectrum of EG And’)\\n<matplotlib.text.Text instance at 0x4112420c>\\n>>> plot(wave, flux)\\nOther available plot types are: 2-D vector plots, high-low-close plots (see\\nFigure 2), histogram plots, log plots, pie charts and bar charts.\\n4.\\nImages\\nThe matlab interface has two functions for displaying image data: ﬁgimage,\\nwhich will preserve the size and shape of the image; and imshow, which will\\nresample the image to ﬁt the size of the ﬁgure (see Figure 3). Images can be\\nenhanced by annotations or graphical overlays.\\n5.\\nFeatures\\nKey features that make matplotlib easy to use are:\\n• Integrated support for numarray or Numeric – the Python multi-dimen-\\nsional array libraries.\\n• The plot window contains a simple interactive GUI with support for pan-\\nand-zoom, history recall, and saving to hardcopy.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 4}, page_content='matplotlib – A Portable Python Plotting Package\\n95\\n• The command line interface is modeled after the easy to use MatLab in-\\nterface.\\n• Support for multiple plots and images per page.\\n• TrueType/FreeType fonts are available in the GD, Agg, Paint, and Post-\\nScript backends. SVG support is coming soon.\\n• Mathematical text ala TEX math mode is available whenever TrueType\\nfonts are available.\\n• Images are automatically resampled to the size of the ﬁgure.\\n• A fully object-oriented design to ease programming and development.\\n6.\\nEnchancements\\nEnhancements to matplotlib that are expected in the near future are:\\n• Contour plots which can be used for image overlays.\\n• The ability to handle general 2-D transforms, which are useful for map\\nprojections and world coordinate systems.\\nTo learn more about matplotlib and to download the latest version, go to\\nhttp://matplotlib.sourceforge.net.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 0}, page_content='Copyright: © the author(s), publisher and licensee Technoscience Academy. This is an open-access article distributed under the \\nterms of the Creative Commons Attribution Non-Commercial License, which permits unrestricted non-commercial use,\\ndistribution, and reproduction in any medium, provided the original work is properly cited \\n \\nInternational Journal of Scientific Research in Computer Science, Engineering and Information Technology \\nISSN : 2456-3307 (www.ijsrcseit.com) \\ndoi : https://doi.org/10.32628/CSEIT2173105 \\n \\n \\n \\n \\n \\n67 \\nStudy On Machine Learning Algorithms \\nPraba. R1, Darshan. G2, Roshanraj. K. T2, Surya Prakash. B2 \\n1Assistant Professor, Department of Information Technology, Dr.N.G.P. Arts and Science College, Coimbatore, \\nTamil Nadu, India \\n2UG Candidate, Department of Information Technology, Dr.N.G.P. Arts and Science College, Coimbatore, \\nTamil Nadu, India \\n \\n \\n \\nArticle Info \\nVolume  7, Issue 4 \\nPage Number: 67-72 \\nPublication Issue : \\nJuly-August-2021 \\nArticle History \\nAccepted :  02 July 2021 \\nPublished : 08 July 2021 \\nABSTRACT \\n \\nVarious machine learning algorithms are described in this work. These \\nalgorithms are used for a variety of applications, including data mining, image \\nprocessing, predictive analytics, and so on. The fundamental benefit of \\nemploying machine learning is that once an algorithm learns what to do with \\ndata, it can complete tasks on its own. \\n \\nKeywords : Machine Learning, Algorithms \\n  \\n \\nI. INTRODUCTION \\n \\nMachine learning is used to teach machines how to \\nhandle the data more efficiently. We may be unable \\nto interpret the pattern or extract information from \\nthe data after examining it.In that case, we apply \\nmachine learning. With the abundance of datasets \\navailable, the demand for machine learning is in rise. \\nMany industries from medicine to military apply \\nmachine learning to extract relevant information. \\n \\nThe phrase \"Machine Learning\" was coined by \\nArthur Samuel, a pioneer in the fields of artificial \\nintelligence and computer games. Machine learning, \\nhe stated, is a “field of research that enables \\ncomputers \\nto \\nlearn \\nwithout \\nbeing \\nexplicitly \\nprogrammed.” \\n \\nMachine Learning (ML) can be defined as the process \\nof automating and refining the learning process of \\ncomputers based on their experiences without the \\nneed for programming, i.e. without the use of \\nhumans. The process begins with providing high-\\nquality data, which is then used to train our machines \\n(computers) by creating machine learning models \\nbased on the data and other methods. The algorithms \\nwe use are determined by the type of data we have \\nand the task we are attempting to automate. \\n \\nMachine learning (ML) is the study of computer \\nalgorithms that improve themselves over time as a \\nresult of experience and data. 1st It is consideredto be \\na component of artificial intelligence. Machine \\nlearning algorithms create a model based on sample'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 1}, page_content='Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n68 \\ndata, referred to as \"training data,\" in order to make \\npredictions or judgments without being explicitly \\nprogrammed. \\n \\nMachine learning algorithms are utilized in a wide \\nrange of applications, including medicine, email \\nfiltering, and computer vision, where developing \\ntraditional algorithms to do the required tasks is \\ndifficult or impossible. \\n \\nHowever, not all machine learning is statistical \\nlearning. A subset of machine learning is strongly \\nrelated to computational statistics, which focuses on \\nmaking predictions using computers. The discipline \\nof machine learning benefits from the study of \\nmathematical optimization since it provides tools, \\ntheory, and application domains. Data mining is a \\nsimilar \\nbranch \\nof \\nresearch \\nthat \\nfocuses \\non \\nunsupervised learning for exploratory data analysis. \\nMachine learning is also known as predictive \\nanalytics when it is used to solve business challenges. \\n \\nII. LITERATURE REVIEW \\n \\nMachine learning is the process of computers figuring \\nout how to do things without being specifically \\nprogrammed to do so. It entails computers learning \\nfrom data in order to do specific jobs. It is possible to \\nbuild algorithms that teach the computer how to \\nperform all steps required to solve the problem at \\nhand for basic jobs entrusted to computers; no \\nlearning is necessary on the computer\\'s behalf. \\nManually creating the required algorithms for more \\ncomplicated tasks can be difficult for a human. In \\npractise, assisting the computer in developing its own \\nalgorithm rather than having human programmers \\nexplain each required step can prove to be more \\nproductive. Machine learning is a discipline that uses \\na variety of ways to train computers how to complete \\ntasks for which no entirely suitable solution exists. \\nWhen there are a large number of possible replies, \\none strategy is to classify some of the correct \\nresponses as valid.  The computer can then utilize \\nthis as training data to refine the algorithm(s) it uses \\nto determine right answers. The MNIST dataset of \\nhandwritten digits, for example, has frequently been \\nused to train a system for the task of digital character \\nrecognition.  \\n \\nIII. TYPES OF LEARNING \\n \\nA. Supervised Learning \\n \\nSupervised learning algorithms create a mathematical \\nmodel of a set of data that includes both the inputs \\nand the outputs that are sought a The information is \\nreferred to as training data, and it consists of a \\ncollection of training instances. Each training \\nexample has one or more inputs and a supervisory \\nsignal as the desired output. Each training sample is \\nrepresented by an array or vector, sometimes referred \\nto as a feature vector, and the training data is \\nrepresented by a matrix in the mathematical model. \\nSupervised learning techniques develop a function \\nthat may be used to predict the output associated \\nwith fresh inputs by iteratively optimizing an \\nobjective function. Active learning, classification, and \\nregression are examples of supervised learning \\nalgorithms. When the outputs are limited to a small \\nset of values, classification techniques are employed, \\nand regression methods are used when the outputs \\ncan have any numerical value within a range.  An \\nincoming email, for example, would be the input to a \\nclassification algorithm that filters emails, and the \\noutput would be the name of the folder to file the \\nemail in.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 2}, page_content='Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n69 \\n \\nA supervised learning model called a support-vector \\nmachine separates data into areas separated by a \\nlinear border. The black circles are separated from \\nthe white circles by a linear barrier. \\n \\nB.Unsupervised Learning \\nUnsupervised learning methods take a collection of \\ndata with only inputs and detect structure in it, such \\nas data point grouping or clustering. As a result, the \\nalgorithms learn from unlabeled, unclassified, and \\nuncategorized test data.  Unsupervised learning \\nalgorithms discover commonalities in the data and \\nreact depending on the existence or lack of such \\ncommonalities in each new piece of data, rather than \\nresponding to feedback. The field of density \\nestimation in statistics, such as calculating the \\nprobability density function, is a key application of \\nunsupervised learning. Unsupervised learning, on the \\nother hand, comprises various domains that need \\nsummarising and explaining data aspects. \\n \\nCluster analysis divides a set of observations into \\nsubsets (called clusters) so that observations within \\nthe same cluster are comparable based on one or \\nmore predetermined criteria, while observations from \\ndifferent clusters are distinct. Different clustering \\napproaches make different assumptions about the \\nstructure \\nof \\nthe \\ndata, \\nwhich \\nis \\ncommonly \\ncharacterized \\nby \\nsome \\nsimilarity \\nmetric \\nand \\nevaluated, for example, by internal compactness, or \\nthe similarity between cluster members, and \\nseparation, or the difference between clusters. \\nEstimated density and graph connectedness are used \\nin other approaches. \\n \\n \\nC. Semi-supervised Learning \\nUnsupervised learning (without any labelled training \\ndata) and supervised learning (with labelled training \\ndata) are the two types of learning (with completely \\nlabelled training data).  Although some of the \\ntraining examples lack training labels, several \\nmachine-learning researchers have discovered that \\nunlabeled data, when combined with a modest \\namount of labelled data, can enhance learning \\naccuracy significantly. The training labels in weakly \\nsupervised learning are noisy, limited, or imprecise, \\nyet they are generally cheaper to obtain, resulting in \\nlarger effective training sets. \\n \\nD. Reinforcement learning \\nReinforcement learning is a branch of machine \\nlearning that studies how software agents should \\nbehave in a given environment in order to maximize \\nsome metric of cumulative reward. Game theory, \\ncontrol theory, operations research, information \\ntheory, simulation-based optimization, multi-agent \\nsystems, swarm intelligence, statistics, and genetic \\nalgorithms are among the numerous disciplines that \\nstudy the field due to its generality. The environment \\nis generally represented as a Markov decision process \\nin machine learning (MDP). Dynamic programming \\ntechniques are used in many reinforcement learning \\nsystems. When exact mathematical models of the \\nMDP \\nare \\ninfeasible, \\nreinforcement \\nlearning \\nprocedures are applied. Reinforcement learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 3}, page_content='Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n70 \\nalgorithms are employed in autonomous vehicles and \\nin teaching humans how to play a game.  \\n \\nE. Dimensionality Reduction \\nThe technique of lowering the number of random \\nvariables under consideration by generating a set of \\nprimary variables is known as dimensionality \\nreduction. In other words, it\\'s a method of \\nminimising the size of your feature set, also known as \\nthe \\nnumber \\nof \\nfeatures. \\nThe \\nmajority \\nof \\ndimensionality reduction approaches fall into one of \\ntwo categories: feature deletion or extraction. \\nPrincipal component analysis is one of the most \\nwidely used methods for dimensionality reduction.  \\n \\nAnalyze the Principal Components (PCA) \\nPCA entails converting higher-dimensional data \\n(such as 3D) to a smaller space (eg. 2D). This results \\nin a decreased data dimension (2D rather than 3D), \\nwhile preserving all of the original variables in the \\nmodel and not modifying the data. \\n \\nA supervised learning model called a support-vector \\nmachine separates data into areas separated by a \\nlinear border. The black circles are separated from \\nthe white circles by a linear barrier.  \\n \\nIV. MODELS \\n \\n1.Artificial neural networks \\nArtificial neural networks (ANNs), also known as \\nconnectionist systems, are computing systems that \\nare based on biological neural networks found in \\nanimal brains. Such systems \"learn\" to execute tasks \\nby considering examples, usually without any task-\\nspecific rules being coded. An artificial neural \\nnetwork (ANN) is a model built on a set of connected \\nunits or nodes known as \"artificial neurons,\" which \\nare roughly modelled after the neurons in a biological \\nbrain. Each link, like the synapses in a human brain, \\ncan send information, or a \"signal,\" from one artificial \\nneuron to the next.  \\n \\n2. Decision Tree \\nTo get from observations about an item (represented \\nin the branches) to conclusions about the item\\'s goal \\nvalue, decision tree learning employs a decision tree \\nas a predictive model (represented in the leaves). In \\nstatistics, data mining, and machine learning, it is one \\nof \\nthe \\npredictive \\nmodelling \\nmethodologies. \\nClassification trees are tree models in which the goal \\nvariable can take a discrete set of values; in these tree \\nstructures, leaves indicate class labels and branches \\nrepresent feature combinations that lead to those \\nclass labels.       \\n                  \\n \\n \\n3. Support-Vector Networks  \\nSVMs, also known as support-vector networks, are a \\ngroup of similar supervised learning algorithms for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 4}, page_content=\"Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n71 \\nclassification and regression. An SVM training \\nmethod creates a model that predicts whether a new \\nexample falls into one of two categories given a set of \\ntraining examples that are individually labelled as \\nbelonging to one of two categories.  Although \\nmethods such as Platt scaling exist to employ SVM in \\na probabilistic classification environment, an SVM \\ntraining algorithm is a non-probabilistic, binary, \\nlinear classifier. SVMs may perform non-linear \\nclassification as well as linear classification by \\nimplicitly \\nmapping \\ntheir \\ninputs \\ninto \\nhigh-\\ndimensional feature spaces, which is known as the \\nkernel trick. \\n \\n \\n \\n4. Regression analysis \\nRegression analysis is a broad term that refers to a \\nnumber of statistical techniques for estimating the \\nrelationship between input variables and their \\nassociated characteristics. Linear regression is the \\nmost frequent type, in which a single line is \\ngenerated to best match the available data using a \\nmathematical criterion such ordinary least squares.  \\n \\n \\n5. Bayesian network \\nA Bayesian network, also known as a belief network \\nor a directed acyclic graphical model, is a \\nprobabilistic graphical model that uses a directed \\nacyclic graph to describe a set of random variables \\nand their conditional independence (DAG).  A \\nBayesian network, for example, could be used to \\nillustrate \\nthe \\nprobability \\ncorrelations \\nbetween \\ndiseases and symptoms. The network may be used to \\ncalculate the chances of certain diseases being present \\nbased on symptoms. There are efficient algorithms \\nfor inference and learning. Dynamic Bayesian \\nnetworks \\nare \\nBayesian \\nnetworks \\nthat \\nmodel \\nsequences of variables, such as speech signals or \\nprotein sequences.  \\n \\n \\n \\nV. CONCLUSION \\n \\nThis study examines a number of different machine \\nlearning algorithms. Today, everyone, intentionally \\nor unconsciously, employs machine learning. From \\nonline shopping for a recommended product to \\nuploading images on social networking sites, there's a \\nlot to do. This document provides an overview of the \\nmost widely used machine learning algorithms. \\n \\nVI. REFERENCES \\n \\n[1]. \\nR. Praba, “A Study on Data Science Basics with \\nPython Concepts”, Volume-4, Issue-14, ISSN: \\n2582-3930.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 5}, page_content='Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n72 \\n[2]. \\nW. Richert, L. P. Coelho, “Building Machine \\nLearning \\nSystems \\nwith \\nPython”, \\nPackt \\nPublishing Ltd., ISBN 978-1-78216-140-0 \\n[3]. \\nM. \\nWelling, \\n“A \\nFirst \\nEncounter \\nwith \\nMachineLearning” \\n[4]. \\nM. Bowles, “Machine Learning in Python: \\nEssential Techniques for Predictive Analytics”, \\nJohn Wiley & Sons Inc., ISBN: 978-1-118-\\n96174-2 \\n[5]. \\nS.B. Kotsiantis, “Supervised Machine Learning: \\nA \\nReview \\nof \\nClassification \\nTechniques”, \\nInformatica 31 (2007) 249-268 \\n[6]. \\nL. Rokach, O. Maimon, “Top – Down \\nInduction of Decision Trees Classifiers – A \\nSurvey”, IEEE Transactions on Systems, \\n[7]. \\nD. Lowd, P. Domingos, “Naïve Bayes Models \\nfor Probability Estimation” \\n[8]. \\nhttps://webdocs.cs.ualberta.ca/~greiner/C- \\n651/Homework2_Fall2008.html \\n[9]. \\nD. Meyer, “Support Vector Machines – The \\nInterface to libsvm in package e1071”, August \\n2015 \\n[10]. S. S. Shwartz, Y. Singer, N. Srebro, “Pegasos: \\nPrimal Estimated sub - Gradient Solver for \\nSVM”, \\nProceedings \\nof \\nthe \\n24th \\nInternationalConference on Machine Learning, \\nCorvallis, OR, 2007 \\n[11]. http://www.simplilearn.com/what-is-machine-\\nlearning-and-why-itmatters- article \\n[12]. P. Harrington, “Machine Learning in action”, \\nManning Publications Co., Shelter Island, New \\nYork, 2012 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCite this article as : \\n \\nPraba. R, Darshan. G, Roshanraj. K. T, Surya Prakash. \\nB, \"Study On Machine Learning Algorithms\", \\nInternational Journal of Scientific Research in \\nComputer Science, Engineering and Information \\nTechnology (IJSRCSEIT), ISSN : 2456-3307, Volume \\n7 Issue 4, pp. 67-72, July-August 2021. Available at \\ndoi : https://doi.org/10.32628/CSEIT2173105         \\n  \\nJournal URL : https://ijsrcseit.com/CSEIT2173105'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 0}, page_content='Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\u2002 |\\u2002 357\\nReview\\nArray programming with NumPy\\nCharles R. Harris1, K. Jarrod Millman2,3,4\\u2009✉, Stéfan J. van\\xa0der Walt2,4,5\\u2009✉, Ralf Gommers6\\u2009✉, \\nPauli Virtanen7,8, David Cournapeau9, Eric Wieser10, Julian Taylor11, Sebastian Berg4, \\nNathaniel J. Smith12, Robert Kern13, Matti Picus4, Stephan Hoyer14, Marten H. van Kerkwijk15, \\nMatthew Brett2,16, Allan Haldane17, Jaime Fernández del Río18, Mark Wiebe19,20,  \\nPearu Peterson6,21,22, Pierre Gérard-Marchant23,24, Kevin Sheppard25, Tyler Reddy26,  \\nWarren Weckesser4, Hameer Abbasi6, Christoph Gohlke27 & Travis E. Oliphant6\\nArray programming provides a powerful, compact and expressive syntax for \\naccessing, manipulating and operating on data in vectors, matrices and \\nhigher-dimensional arrays. NumPy is the primary array programming library for the \\nPython language. It has an essential role in research analysis pipelines in fields as \\ndiverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials \\nscience, engineering, finance and economics. For example, in astronomy, NumPy was \\nan important part of the software stack used in the discovery of gravitational waves1 \\nand in the first imaging of a black hole2. Here we review how a few fundamental array \\nconcepts lead to a simple and powerful programming paradigm for organizing, \\nexploring and analysing scientific data. NumPy is the foundation upon which the \\nscientific Python ecosystem is constructed. It is so pervasive that several projects, \\ntargeting audiences with specialized needs, have developed their own NumPy-like \\ninterfaces and array objects. Owing to its central position in the ecosystem, NumPy \\nincreasingly acts as an interoperability layer between such array computation \\nlibraries and, together with its application programming interface (API), provides a \\nflexible framework to support the next decade of scientific and industrial analysis.\\nTwo Python array packages existed before NumPy. The Numeric pack-\\nage was developed in the mid-1990s and provided array objects and \\narray-aware functions in Python. It was written in C and linked to stand-\\nard fast implementations of linear algebra3,4. One of its earliest uses was \\nto steer C++ applications for inertial confinement fusion research at \\nLawrence Livermore National Laboratory5. To handle large astronomi-\\ncal images coming from the Hubble Space Telescope, a reimplementa-\\ntion of Numeric, called Numarray, added support for structured arrays, \\nflexible indexing, memory mapping, byte-order variants, more efficient \\nmemory use, flexible IEEE 754-standard error-handling capabilities, and \\nbetter type-casting rules6. Although Numarray was highly compatible \\nwith Numeric, the two packages had enough differences that it divided \\nthe community; however, in 2005 NumPy emerged as a ‘best of both \\nworlds’ unification7—combining the features of Numarray with the \\nsmall-array performance of Numeric and its rich C API.\\nNow, 15 years later, NumPy underpins almost every Python library \\nthat does scientific or numerical computation8–11, including SciPy12, \\nMatplotlib13, pandas14, scikit-learn15 and scikit-image16. NumPy is a \\ncommunity-developed, open-source library, which provides a mul-\\ntidimensional Python array object along with array-aware functions \\nthat operate on it. Because of its inherent simplicity, the NumPy array \\nis the de facto exchange format for array data in Python.\\nNumPy operates on in-memory arrays using the central processing \\nunit (CPU). To utilize modern, specialized storage and hardware, there \\nhas been a recent proliferation of Python array packages. Unlike with \\nthe Numarray–Numeric divide, it is now much harder for these new \\nlibraries to fracture the user community—given how much work is \\nalready built on top of NumPy. However, to provide the community with \\naccess to new and exploratory technologies, NumPy is transitioning \\ninto a central coordinating mechanism that specifies a well defined \\narray programming API and dispatches it, as appropriate, to special-\\nized array implementations.\\nNumPy arrays\\nThe NumPy array is a data structure that efficiently stores and accesses \\nmultidimensional arrays17 (also known as tensors), and enables a wide \\nvariety of scientific computation. It consists of a pointer to memory, \\nalong with metadata used to interpret the data stored there, notably \\n‘data type’, ‘shape’ and ‘strides’ (Fig.\\xa01a).\\nhttps://doi.org/10.1038/s41586-020-2649-2\\nReceived: 21 February 2020\\nAccepted: 17 June 2020\\nPublished online: 16 September 2020\\nOpen access\\n Check for updates\\n1Independent researcher, Logan, UT, USA. 2Brain Imaging Center, University of California, Berkeley, Berkeley, CA, USA. 3Division of Biostatistics, University of California, Berkeley, Berkeley, CA, \\nUSA. 4Berkeley Institute for Data Science, University of California, Berkeley, Berkeley, CA, USA. 5Applied Mathematics, Stellenbosch University, Stellenbosch, South Africa. 6Quansight, Austin, \\nTX, USA. 7Department of Physics, University of Jyväskylä, Jyväskylä, Finland. 8Nanoscience Center, University of Jyväskylä, Jyväskylä, Finland. 9Mercari JP, Tokyo, Japan. 10Department of \\nEngineering, University of Cambridge, Cambridge, UK. 11Independent researcher, Karlsruhe, Germany. 12Independent researcher, Berkeley, CA, USA. 13Enthought, Austin, TX, USA. 14Google \\nResearch, Mountain View, CA, USA. 15Department of Astronomy and Astrophysics, University of Toronto, Toronto, Ontario, Canada. 16School of Psychology, University of Birmingham, \\nEdgbaston, Birmingham, UK. 17Department of Physics, Temple University, Philadelphia, PA, USA. 18Google, Zurich, Switzerland. 19Department of Physics and Astronomy, The University of \\nBritish Columbia, Vancouver, British Columbia, Canada. 20Amazon, Seattle, WA, USA. 21Independent researcher, Saue, Estonia. 22Department of Mechanics and Applied Mathematics, Institute \\nof Cybernetics at Tallinn Technical University, Tallinn, Estonia. 23Department of Biological and Agricultural Engineering, University of Georgia, Athens, GA, USA. 24France-IX Services, Paris, \\nFrance. 25Department of Economics, University of Oxford, Oxford, UK. 26CCS-7, Los Alamos National Laboratory, Los Alamos, NM, USA. 27Laboratory for Fluorescence Dynamics, Biomedical \\nEngineering Department, University of California, Irvine, Irvine, CA, USA. ✉e-mail: millman@berkeley.edu; stefanv@berkeley.edu; ralf.gommers@gmail.com'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='358\\u2002 |\\u2002 Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\nReview\\nThe data type describes the nature of elements stored in an array. \\nAn array has a single data type, and each element of an array occupies \\nthe same number of bytes in memory. Examples of data types include \\nreal and complex numbers (of lower and higher precision), strings, \\ntimestamps and pointers to Python objects.\\nThe shape of an array determines the number of elements along \\neach axis, and the number of axes is the dimensionality of the array. \\nFor example, a vector of numbers can be stored as a one-dimensional \\narray of shape N, whereas colour videos are four-dimensional arrays \\nof shape (T,\\xa0M,\\xa0N,\\xa03).\\nStrides are necessary to interpret computer memory, which stores \\nelements linearly, as multidimensional arrays. They describe the num-\\nber of bytes to move forward in memory to jump from row to row, col-\\numn to column, and so forth. Consider, for example, a two-dimensional \\narray of floating-point numbers with shape (4,\\xa03), where each element \\noccupies 8\\xa0bytes in memory. To move between consecutive columns, \\nwe need to jump forward 8\\xa0bytes in memory, and to access the next row, \\n3\\xa0×\\xa08\\xa0=\\xa024\\xa0bytes. The strides of that array are therefore (24,\\xa08). NumPy \\ncan store arrays in either C or Fortran memory order, iterating first over \\neither rows or columns. This allows external libraries written in those \\nlanguages to access NumPy array data in memory directly.\\nUsers interact with NumPy arrays using ‘indexing’ (to access sub-\\narrays or individual elements), ‘operators’ (for example, +, − and × \\nfor vectorized operations and @ for matrix multiplication), as well \\nas ‘array-aware functions’; together, these provide an easily readable, \\nexpressive, high-level API for array programming while NumPy deals \\nwith the underlying mechanics of making operations fast.\\nIndexing an array returns single elements, subarrays or elements \\nthat satisfy a specific condition (Fig.\\xa01b). Arrays can even be indexed \\nusing other arrays (Fig.\\xa01c). Wherever possible, indexing that retrieves a \\nsubarray returns a ‘view’ on the original array such that data are shared \\nbetween the two arrays. This provides a powerful way to operate on \\nsubsets of array data while limiting memory usage.\\nTo complement the array syntax, NumPy includes functions that \\nperform vectorized calculations on arrays, including arithmetic, \\nstatistics and trigonometry (Fig.\\xa01d). Vectorization—operating on \\nentire arrays rather than their individual elements—is essential to array \\nprogramming. This means that operations that would take many tens \\nof lines to express in languages such as C can often be implemented as \\na single, clear Python expression. This results in concise code and frees \\nusers to focus on the details of their analysis, while NumPy handles \\nlooping over array elements near-optimally—for example, taking \\nstrides into consideration to best utilize the computer’s fast cache \\nmemory.\\nWhen performing a vectorized operation (such as addition) on two \\narrays with the same shape, it is clear what should happen. Through \\n‘broadcasting’ NumPy allows the dimensions to differ, and produces \\nresults that appeal to intuition. A trivial example is the addition of a \\nscalar value to an array, but broadcasting also generalizes to more com-\\nplex examples such as scaling each column of an array or generating \\na grid of coordinates. In broadcasting, one or both arrays are virtually \\nduplicated (that is, without copying any data in memory), so that the \\nshapes of the operands match (Fig.\\xa01d). Broadcasting is also applied \\nwhen an array is indexed using arrays of indices (Fig.\\xa01c).\\nOther array-aware functions, such as sum, mean and maximum, \\nperform element-by-element ‘reductions’, aggregating results across \\none, multiple or all axes of a single array. For example, summing an \\nn-dimensional array over d axes results in an array of dimension n\\xa0−\\xa0d \\n(Fig.\\xa01f).\\nNumPy also includes array-aware functions for creating, reshaping, \\nconcatenating and padding arrays; searching, sorting and counting \\ndata; and reading and writing files. It provides extensive support for \\ngenerating pseudorandom numbers, includes an assortment of prob-\\nability distributions, and performs accelerated linear algebra, using \\none of several backends such as OpenBLAS18,19 or Intel MKL optimized \\nfor the CPUs at hand (see Supplementary Methods for more details).\\nAltogether, the combination of a simple in-memory array repre-\\nsentation, a syntax that closely mimics mathematics, and a variety \\nof array-aware utility functions forms a productive and powerfully \\nexpressive array programming language.\\nIn [1]: import numpy as np\\nIn [2]: x = np.arange(12)\\nIn [3]: x = x.reshape(4, 3)\\nIn [4]: x\\nOut[4]:\\narray([[ 0,  1,  2],\\n       [ 3,  4,  5],\\n       [ 6,  7,  8],\\n       [ 9, 10, 11]])\\nIn [5]: np.mean(x, axis=0)\\nOut[5]: array([4.5, 5.5, 6.5])\\nIn [6]: x = x - np.mean(x, axis=0)\\nIn [7]: x\\nOut[7]:\\narray([[-4.5, -4.5, -4.5],\\n       [-1.5, -1.5, -1.5],\\n       [ 1.5,  1.5,  1.5],\\n       [ 4.5,  4.5,  4.5]])\\na Data structure\\ng Example\\nx =\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10 11\\ndata\\ndata type\\nshape\\nstrides\\n8-byte integer\\n(4, 3)\\n(24, 8)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n8\\n9 10 11\\n8 bytes\\nper element\\n3 × 8 = 24 bytes\\nto jump one\\nrow down\\nb Indexing (view)\\n10 11\\n9\\nx[:,1:] →\\nwith slices\\n1\\n2\\n4 5\\n7 8\\n0\\n3\\n6\\nx[:,::2]→\\nwith slices\\nwith steps\\n0\\n2\\n3\\n5\\n6\\n8\\n9\\n11\\n0 1\\n2\\n3 4\\n5\\n6\\n7 8\\n9 10\\n10 11\\nSlices are start:end:step,\\nany of which can be left blank\\nd Vectorization\\n+\\n→\\n0\\n1\\n3\\n4\\n6\\n7\\n9 10\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n4\\n5\\n7\\n8\\n10 11\\ne Broadcasting\\n×\\n3\\n6\\n0\\n9\\n1\\n2\\n→\\n0\\n0\\n3\\n6\\n6 12\\n9 18\\nf Reduction\\n0\\n1\\n3\\n4\\n6\\n7\\n9 10\\n2\\n5\\n8\\n11\\n3\\n12\\n21\\n30\\nsum\\naxis 1\\n18 22 26\\nsum\\naxis 0\\n66\\nsum\\naxis (0,1)\\nc Indexing (copy)\\n4\\n3\\n7\\n6\\nwith arrays\\nwith broadcasting\\n→\\nx\\n→\\n,\\n2\\n1\\n1\\n0\\nx\\n,\\n1 1\\n2 2\\n1 0\\n1 0\\nx \\nwith arrays\\nx[0,1],x[1,2]\\n1\\n5\\n→\\n→\\n0\\n1\\n1\\n2\\n,\\nx[x > 9]\\nwith masks\\n10 11\\n→\\n→5\\nwith scalars\\nx[1,2] \\nFig. 1 | The NumPy array incorporates several fundamental array concepts. \\na, The NumPy array data structure and its associated metadata fields.  \\nb, Indexing an array with slices and steps. These operations return a ‘view’ of \\nthe original data. c, Indexing an array with masks, scalar coordinates or other \\narrays, so that it returns a ‘copy’ of the original data. In the bottom example, an \\narray is indexed with other arrays; this broadcasts the indexing arguments \\nbefore performing the lookup. d, Vectorization efficiently applies operations \\nto groups of elements. e, Broadcasting in the multiplication of two-dimensional \\narrays. f, Reduction operations act along one or more axes. In this example,  \\nan array is summed along select axes to produce a vector, or along two axes \\nconsecutively to produce a scalar. g, Example NumPy code, illustrating some of \\nthese concepts.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 2}, page_content='Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\u2002 |\\u2002 359\\nScientific Python ecosystem\\nPython is an open-source, general-purpose interpreted programming \\nlanguage well suited to standard programming tasks such as cleaning \\ndata, interacting with web resources and parsing text. Adding fast array \\noperations and linear algebra enables scientists to do all their work \\nwithin a single programming language—one that has the advantage of \\nbeing famously easy to learn and teach, as witnessed by its adoption \\nas a primary learning language in many universities.\\nEven though NumPy is not part of Python’s standard library, it ben-\\nefits from a good relationship with the Python developers. Over the \\nyears, the Python language has added new features and special syntax \\nso that NumPy would have a more succinct and easier-to-read array \\nnotation. However, because it is not part of the standard library, NumPy \\nis able to dictate its own release policies and development patterns.\\nSciPy and Matplotlib are tightly coupled with NumPy in terms of his-\\ntory, development and use. SciPy provides fundamental algorithms for \\nscientific computing, including mathematical, scientific and engineer-\\ning routines. Matplotlib generates publication-ready figures and visu-\\nalizations. The combination of NumPy, SciPy and Matplotlib, together \\nwith an advanced interactive environment such as IPython20 or Jupy-\\nter21, provides a solid foundation for array programming in Python. The \\nscientific Python ecosystem (Fig.\\xa02) builds on top of this foundation to \\nprovide several, widely used technique-specific libraries15,16,22, that in \\nturn underlie numerous domain-specific projects23–28. NumPy, at the \\nbase of the ecosystem of array-aware libraries, sets documentation \\nstandards, provides array testing infrastructure and adds build sup-\\nport for Fortran and other compilers.\\nMany research groups have designed large, complex scientific librar-\\nies that add application-specific functionality to the ecosystem. For \\nexample, the eht-imaging library29, developed by the Event Horizon \\nTelescope collaboration for radio interferometry imaging, analysis \\nand simulation, relies on many lower-level components of the scientific \\nPython ecosystem. In particular, the EHT collaboration used this library \\nfor the first imaging of a black hole. Within eht-imaging, NumPy arrays \\nare used to store and manipulate numerical data at every step in the \\nprocessing chain: from raw data through calibration and image recon-\\nstruction. SciPy supplies tools for general image-processing tasks such \\nas filtering and image alignment, and scikit-image, an image-processing \\nlibrary that extends SciPy, provides higher-level functionality such \\nas edge filters and Hough transforms. The ‘scipy.optimize’ module \\nperforms mathematical optimization. NetworkX22, a package for com-\\nplex network analysis, is used to verify image comparison consistency. \\nAstropy23,24 handles standard astronomical file formats and computes \\ntime–coordinate transformations. Matplotlib is used to visualize data \\nand to generate the final image of the black hole.\\nThe interactive environment created by the array\\xa0programming foun-\\ndation and the surrounding ecosystem of tools—inside of IPython or \\nJupyter—is ideally suited to exploratory data analysis. Users can fluidly \\ninspect, manipulate and visualize their data, and rapidly iterate to refine \\nprogramming statements. These statements are then stitched together \\ninto imperative or functional programs, or notebooks containing both \\ncomputation and narrative. Scientific computing beyond exploratory \\nwork is often done in a text editor or an integrated development envi-\\nronment (IDE) such as Spyder. This rich and productive environment \\nhas made Python popular for scientific research.\\nTo complement this facility for exploratory work and rapid proto-\\ntyping, NumPy has developed a culture of using time-tested software \\nengineering practices to improve collaboration and reduce error30. This \\nculture is not only adopted by leaders in the project but also enthusi-\\nastically taught to newcomers. The NumPy team was early to adopt \\ndistributed revision control and code review to improve collaboration \\ncantera\\nChemistry\\nBiopython\\nBiology\\nAstropy\\nAstronomy\\nsimpeg\\nGeophysics\\nNLTK\\nLinguistics\\nQuantEcon\\nEconomics\\nSciPy\\nAlgorithms\\nMatplotlib\\nPlots\\nscikit-learn\\nMachine learning\\nNetworkX\\nNetwork analysis\\npandas, statsmodels\\nStatistics\\nscikit-image\\nImage processing\\nPsychoPy\\nkhmer\\nQiime2\\nFiPy\\ndeepchem\\nlibrosa\\nPyWavelets\\nSunPy\\nQuTiP\\nyt\\nnibabel\\nyellowbrick\\nmne-python \\nscikit-HEP\\neht-imaging\\nMDAnalysis\\niris\\ncesium\\nPyChrono\\nFoundation\\nApplication-speciﬁc\\nDomain-speciﬁc\\nTechnique-speciﬁc\\nArray Protocols\\nNumPy API\\nPython\\nLanguage\\nIPython / Jupyter\\nInteractive environments\\nNumPy\\nArrays\\nNew array implementations\\nFig. 2 | NumPy is the base of the scientific Python ecosystem. Essential libraries and projects that depend on NumPy’s API gain access to new array \\nimplementations that support NumPy’s array protocols (Fig.\\xa03).'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content=\"360\\u2002 |\\u2002 Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\nReview\\non code, and continuous testing that runs an extensive battery of auto-\\nmated tests for every proposed change to NumPy. The project also \\nhas comprehensive, high-quality documentation, integrated with the \\nsource code31–33.\\nThis culture of using best practices for producing reliable scientific \\nsoftware has been adopted by the ecosystem of libraries that build on \\nNumPy. For example, in a recent award given by the Royal Astronomi-\\ncal Society to Astropy, they state: “The Astropy Project has provided \\nhundreds of junior scientists with experience in professional-standard \\nsoftware development practices including use of version control, unit \\ntesting, code review and issue tracking procedures. This is a vital skill \\nset for modern researchers that is often missing from formal university \\neducation in physics or astronomy”34. Community members explicitly \\nwork to address this lack of formal education through courses and \\nworkshops35–37.\\nThe recent rapid growth of data science, machine learning and arti-\\nficial intelligence has further and dramatically boosted the scientific \\nuse of Python. Examples of its important applications, such as the \\neht-imaging library, now exist in almost every discipline in the natu-\\nral and social sciences. These tools have become the primary software \\nenvironment in many fields. NumPy and its ecosystem are commonly \\ntaught in university courses, boot camps and summer schools, and \\nare the focus of community conferences and workshops worldwide. \\nNumPy and its API have become truly ubiquitous.\\nArray proliferation and interoperability\\nNumPy provides in-memory, multidimensional, homogeneously typed \\n(that is, single-pointer and strided) arrays on CPUs. It runs on machines \\nranging from embedded devices to the world’s largest supercomputers, \\nwith performance approaching that of compiled languages. For most \\nits existence, NumPy addressed the vast majority of array computa-\\ntion use cases.\\nHowever, scientific datasets now routinely exceed the memory capac-\\nity of a single machine and may be stored on multiple machines or in \\nthe cloud. In addition, the recent need to accelerate deep-learning and \\nartificial intelligence applications has led to the emergence of special-\\nized accelerator hardware, including graphics processing units (GPUs), \\ntensor processing units (TPUs) and field-programmable gate arrays \\n(FPGAs). Owing to its in-memory data model, NumPy is currently unable \\nto directly utilize such storage and specialized hardware. However, \\nboth distributed data and also the parallel execution of GPUs, TPUs \\nand FPGAs map well to the paradigm of array programming: therefore \\nleading to a gap between available modern hardware architectures and \\nthe tools necessary to leverage their computational power.\\nThe community’s efforts to fill this gap led to a proliferation of new \\narray implementations. For example, each deep-learning framework \\ncreated its own arrays; the PyTorch38, Tensorflow39, Apache MXNet40 \\nand JAX arrays all have the capability to run on CPUs and GPUs in a \\ndistributed fashion, using lazy evaluation to allow for additional per-\\nformance optimizations. SciPy and PyData/Sparse both provide sparse \\narrays, which typically contain few non-zero values and store only those \\nin memory for efficiency. In addition, there are projects that build on \\nNumPy arrays as data containers, and extend its capabilities. Distrib-\\nuted arrays are made possible that way by Dask, and labelled arrays—\\nreferring to dimensions of an array by name rather than by index for \\nclarity, compare x[:,\\xa01] versus x.loc[:,\\xa0'time']—by xarray41.\\nSuch libraries often mimic the NumPy API, because this lowers the \\nbarrier to entry for newcomers and provides the wider community with \\na stable array\\xa0programming interface. This, in turn, prevents disruptive \\nschisms such as the divergence between\\xa0Numeric and Numarray. But \\nexploring new ways of working with arrays is experimental by nature \\nand, in fact, several promising libraries (such as Theano and Caffe) have \\nalready ceased development. And each time that a user decides to try a \\nnew technology, they must change import statements and ensure that the \\nnew library implements all the parts of the NumPy API they currently use.\\nIdeally, operating on specialized arrays using NumPy functions or \\nsemantics would simply work, so that users could write code once, \\nand would then benefit from switching between NumPy arrays, GPU \\narrays, distributed arrays and so forth as appropriate. To support array \\noperations between external array objects, NumPy therefore added \\nthe capability to act as a central coordination mechanism with a well \\nspecified API (Fig.\\xa02).\\nTo facilitate this interoperability, NumPy provides ‘protocols’ (or \\ncontracts of operation), that allow for specialized arrays to be passed to \\nNumPy functions (Fig.\\xa03). NumPy, in turn, dispatches operations to the \\noriginating library, as required. Over four hundred of the most popular \\nNumPy functions are supported. The protocols are implemented by \\nwidely used libraries such as Dask, CuPy, xarray and PyData/Sparse. \\nThanks to these developments, users can now, for example, scale their \\ncomputation from a single machine to distributed systems using Dask. \\nThe protocols also compose well, allowing users to redeploy NumPy \\ncode at scale on distributed, multi-GPU systems via, for instance, CuPy \\narrays embedded in Dask arrays. Using NumPy’s high-level API, users \\ncan leverage highly parallel code execution on multiple systems with \\nmillions of cores, all with minimal code changes42.\\nThese array protocols are now a key feature of NumPy, and are \\nexpected to only increase in importance. The NumPy developers—\\nmany of whom are authors of this Review—iteratively refine and add \\nprotocol designs to improve utility and simplify adoption.\\nOutput\\narrays\\nInput\\narrays\\nNumPy\\nAPI\\nnp.stack\\nnp.reshape\\nnp.transpose\\nnp.argmin\\nnp.mean\\nnp.std\\nnp.max\\nnp.cos\\nnp.arctan\\nnp.log\\nnp.cumsum\\nnp.diff\\n...\\nNumPy array protocols\\nIn [1]: import numpy as np\\nIn [2]: import dask.array as da\\nIn [3]: x = da.arange(12)\\nIn [4]: x = np.reshape(x, (4, 3))\\nIn [5]: x\\nOut[5]: dask.array<..., shape=(4, 3), ...>\\nIn [6]: np.mean(x, axis=0)\\nOut[6]: dask.array<..., shape=(3,), ...>\\nIn [7]: x = x - np.mean(x, axis=0)\\nIn [8]: x\\nOut[8]: dask.array<..., shape=(4, 3), ...>\\nArray\\nimplementation\\nNumPy\\nDask\\nCuPy\\nPyData/\\nSparse\\n...\\n...\\nDask\\nNumPy\\nCuPy\\nPyData\\nSparse\\n...\\nDask\\nNumPy\\nCuPy\\nPyData\\nSparse\\nFig. 3 | NumPy’s API and array protocols expose new arrays to the \\necosystem. In this example, NumPy’s ‘mean’ function is called on a Dask array. \\nThe call succeeds by dispatching to the appropriate library implementation (in \\nthis case, Dask) and results in a new Dask array. Compare this code to the \\nexample code in Fig.\\xa01g.\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\u2002 |\\u2002 361\\nDiscussion\\nNumPy combines the expressive power of array programming, the \\nperformance of C, and the readability, usability and versatility of Python \\nin a mature, well tested, well documented and community-developed \\nlibrary. Libraries in the scientific Python ecosystem provide fast imple-\\nmentations of most important algorithms. Where extreme optimiza-\\ntion is warranted, compiled languages can be used, such as Cython43, \\nNumba44 and Pythran45; these languages extend Python and trans-\\nparently accelerate bottlenecks. Owing to NumPy’s simple memory \\nmodel, it is easy to write low-level, hand-optimized code, usually in C \\nor Fortran, to manipulate NumPy arrays and pass them back to Python. \\nFurthermore, using array protocols, it is possible to utilize the full \\nspectrum of specialized hardware acceleration with minimal changes \\nto existing code.\\nNumPy was initially developed by students, faculty and researchers \\nto provide an advanced, open-source array programming library for \\nPython, which was free to use and unencumbered by license servers and \\nsoftware protection dongles. There was a sense of building something \\nconsequential together for the benefit of many others. Participating \\nin such an endeavour, within a welcoming community of like-minded \\nindividuals, held a powerful attraction for many early contributors.\\nThese user–developers frequently had to write code from scratch \\nto solve their own or their colleagues’ problems—often in low-level \\nlanguages that preceded Python, such as Fortran46 and C. To them, \\nthe advantages of an interactive, high-level array library were evident. \\nThe design of this new tool was informed by other powerful interactive \\nprogramming languages for scientific computing such as Basis47–50, \\nYorick51, R52 and APL53, as well as commercial languages and environ-\\nments such as IDL (Interactive Data Language) and MATLAB.\\nWhat began as an attempt to add an array object to Python became \\nthe foundation of a vibrant ecosystem of tools. Now, a large amount of \\nscientific work depends on NumPy being correct, fast and stable. It is \\nno longer a small community project, but core scientific infrastructure.\\nThe developer culture has matured: although initial development was \\nhighly informal, NumPy now has a roadmap and a process for propos-\\ning and discussing large changes. The project has formal governance \\nstructures and is fiscally sponsored by NumFOCUS, a nonprofit that \\npromotes open practices in research, data and scientific computing. \\nOver the past few years, the project attracted its first funded develop-\\nment, sponsored by the Moore and Sloan Foundations, and received \\nan award as part of the Chan Zuckerberg Initiative’s Essentials of Open \\nSource Software programme. With this funding, the project was (and \\nis) able to have sustained focus over multiple months to implement \\nsubstantial new features and improvements. That said, the develop-\\nment of NumPy still depends heavily on contributions made by gradu-\\nate students and researchers in their free time (see Supplementary \\nMethods for more details).\\nNumPy is no longer merely the foundational array library underlying \\nthe scientific Python ecosystem, but it has become the standard API for \\ntensor computation and a central coordinating mechanism between \\narray types and technologies in Python. Work continues to expand on \\nand improve these interoperability features.\\nOver the next decade, NumPy developers will face several challenges. \\nNew devices will be developed, and existing specialized hardware will \\nevolve to meet diminishing returns on Moore’s law. There will be more, \\nand a wider variety of, data science practitioners, a large proportion of \\nwhom will use NumPy. The scale of scientific data gathering will con-\\ntinue to increase, with the adoption of devices and instruments such \\nas light-sheet microscopes and the Large Synoptic Survey Telescope \\n(LSST)54. New generation languages, interpreters and compilers, such as \\nRust55, Julia56 and LLVM57, will create new concepts and data structures, \\nand determine their viability.\\nThrough the mechanisms described in this Review, NumPy is poised \\nto embrace such a changing landscape, and to continue playing a \\nleading part in interactive scientific computation, although to do so \\nwill require sustained funding from government, academia and indus-\\ntry. But, importantly, for NumPy to meet the needs of the next decade \\nof data science, it will also need a new generation of graduate students \\nand community contributors to drive it forward.\\n1.\\t\\nAbbott, B. P. et\\xa0al. Observation of gravitational waves from a binary black hole merger. \\nPhys. Rev. Lett. 116, 061102 (2016).\\n2.\\t\\nChael, A. et\\xa0al. High-resolution linear polarimetric imaging for the Event Horizon \\nTelescope. Astrophys. J. 286, 11 (2016).\\n3.\\t\\nDubois, P. F., Hinsen, K. & Hugunin, J. Numerical Python. Comput. Phys. 10, 262–267 (1996).\\n4.\\t\\nAscher, D., Dubois, P. F., Hinsen, K., Hugunin, J. & Oliphant, T. E. An Open Source Project: \\nNumerical Python (Lawrence Livermore National Laboratory, 2001).\\n5.\\t\\nYang, T.-Y., Furnish, G. & Dubois, P. F. Steering object-oriented scientific computations. In \\nProc. TOOLS USA 97. Intl Conf. Technology of Object Oriented Systems and Languages \\n(eds Ege,\\xa0R., Singh,\\xa0M.\\xa0& Meyer,\\xa0B.) 112–119 (IEEE, 1997).\\n6.\\t\\nGreenfield, P., Miller, J. T., Hsu, J. & White, R. L. numarray: a new scientific array package \\nfor Python. In PyCon DC 2003 http://citeseerx.ist.psu.edu/viewdoc/download?d\\noi=10.1.1.112.9899 (2003).\\n7.\\t\\nOliphant, T. E. Guide to NumPy 1st edn (Trelgol Publishing, 2006).\\n8.\\t\\nDubois, P. F. Python: batteries included. Comput. Sci. Eng. 9, 7–9 (2007).\\n9.\\t\\nOliphant, T. E. Python for scientific computing. Comput. Sci. Eng. 9, 10–20 (2007).\\n10.\\t\\nMillman, K. J. & Aivazis, M. Python for scientists and engineers. Comput. Sci. Eng. 13, 9–12 \\n(2011).\\n11.\\t\\nPérez, F., Granger, B. E. & Hunter, J. D. Python: an ecosystem for scientific computing. \\nComput. Sci. Eng. 13, 13–21 (2011).  \\nExplains why the scientific Python ecosystem is a highly productive environment for \\nresearch.\\n12.\\t\\nVirtanen, P. et\\xa0al. SciPy 1.0—fundamental algorithms for scientific computing in Python. \\nNat. Methods 17, 261–272 (2020); correction 17, 352 (2020).  \\nIntroduces the SciPy library and includes a more detailed history of NumPy and SciPy.\\n13.\\t\\nHunter, J. D. Matplotlib: a 2D graphics environment. Comput. Sci. Eng. 9, 90–95 (2007).\\n14.\\t\\nMcKinney, W. Data structures for statistical computing in Python. In Proc. 9th Python in \\nScience Conf. (eds van\\xa0der\\xa0Walt,\\xa0S.\\xa0& Millman,\\xa0K.\\xa0J.) 56–61 (2010).\\n15.\\t\\nPedregosa, F. et\\xa0al. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, \\n2825–2830 (2011).\\n16.\\t\\nvan\\xa0der Walt, S. et\\xa0al. scikit-image: image processing in Python. PeerJ 2, e453 (2014).\\n17.\\t\\nvan\\xa0der Walt, S., Colbert, S. C. & Varoquaux, G. The NumPy array: a structure for efficient \\nnumerical computation. Comput. Sci. Eng. 13, 22–30 (2011).  \\nDiscusses the NumPy array data structure with a focus on how it enables efficient \\ncomputation.\\n18.\\t\\nWang, Q., Zhang, X., Zhang, Y. & Yi, Q. AUGEM: automatically generate high performance \\ndense linear algebra kernels on x86 CPUs. In SC’13: Proc. Intl Conf. High Performance \\nComputing, Networking, Storage and Analysis 25 (IEEE, 2013).\\n19.\\t\\nXianyi, Z., Qian, W. & Yunquan, Z. Model-driven level 3 BLAS performance optimization \\non Loongson 3A processor. In 2012 IEEE 18th Intl Conf. Parallel and Distributed Systems \\n684–691 (IEEE, 2012).\\n20.\\t Pérez, F. & Granger, B. E. IPython: a system for interactive scientific computing. Comput. \\nSci. Eng. 9, 21–29 (2007).\\n21.\\t\\nKluyver, T. et\\xa0al. Jupyter Notebooks—a publishing format for reproducible computational \\nworkflows. In Positioning and Power in Academic Publishing: Players, Agents and Agendas \\n(eds Loizides,\\xa0F.\\xa0& Schmidt,\\xa0B.) 87–90 (IOS Press, 2016).\\n22.\\t Hagberg, A. A., Schult, D. A. & Swart, P. J. Exploring network structure, dynamics, and \\nfunction using NetworkX. In Proc. 7th Python in Science Conf. (eds Varoquaux,\\xa0G., \\nVaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 11–15 (2008).\\n23.\\t Astropy Collaboration et\\xa0al. Astropy: a community Python package for astronomy. Astron. \\nAstrophys. 558, A33 (2013).\\n24.\\t Price-Whelan, A. M. et\\xa0al. The Astropy Project: building an open-science project and \\nstatus of the v2.0 core package. Astron. J. 156, 123 (2018).\\n25.\\t Cock, P. J. et\\xa0al. Biopython: freely available Python tools for computational molecular \\nbiology and bioinformatics. Bioinformatics 25, 1422–1423 (2009).\\n26.\\t Millman, K. J. & Brett, M. Analysis of functional magnetic resonance imaging in Python. \\nComput. Sci. Eng. 9, 52–55 (2007).\\n27.\\t\\nThe SunPy Community et\\xa0al. SunPy—Python for solar physics. Comput. Sci. Discov. 8, \\n014009 (2015).\\n28.\\t Hamman, J., Rocklin, M. & Abernathy, R. Pangeo: a big-data ecosystem for scalable Earth \\nsystem science. In EGU General Assembly Conf. Abstracts 12146 (2018).\\n29.\\t Chael, A. A. et\\xa0al. ehtim: imaging, analysis, and simulation software for radio \\ninterferometry. Astrophysics Source Code Library https://ascl.net/1904.004 (2019).\\n30.\\t Millman, K. J. & Pérez, F. Developing open source scientific practice. In Implementing \\nReproducible Research (eds Stodden,\\xa0V., Leisch,\\xa0F.\\xa0& Peng,\\xa0R.\\xa0D.) 149–183 (CRC Press, 2014). \\nDescribes the software engineering practices embraced by the NumPy and SciPy \\ncommunities with a focus on how these practices improve research.\\n31.\\t\\nvan\\xa0der Walt, S. The SciPy Documentation Project (technical overview). In Proc. 7th Python \\nin Science Conf. (SciPy 2008) (eds Varoquaux,\\xa0G., Vaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 27–28 (2008).\\n32.\\t Harrington, J. The SciPy Documentation Project. In Proc. 7th Python in Science \\nConference (SciPy 2008) (eds Varoquaux,\\xa0G., Vaught,\\xa0T.\\xa0& Millman, K.\\xa0J.) 33–35 (2008).\\n33.\\t Harrington, J. & Goldsmith, D. Progress report: NumPy and SciPy documentation in 2009. \\nIn Proc. 8th Python in Science Conf. (SciPy 2009) (eds Varoquaux,\\xa0G., van\\xa0der\\xa0Walt,\\xa0S.\\xa0& \\nMillman,\\xa0K.\\xa0J.) 84–87 (2009).\\n34.\\t Royal Astronomical Society Report of the RAS ‘A’ Awards Committee 2020: Astropy \\nProject: 2020 Group Achievement Award (A) https://ras.ac.uk/sites/default/files/2020-01/\\nGroup%20Award%20-%20Astropy.pdf (2020).\\n35.\\t Wilson, G. Software carpentry: getting scientists to write better code by making them \\nmore productive. Comput. Sci. Eng. 8, 66–69 (2006).'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 5}, page_content='362\\u2002 |\\u2002 Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\nReview\\n36.\\t Hannay, J. E. et\\xa0al. How do scientists develop and use scientific software? In Proc. 2009 \\nICSE Workshop on Software Engineering for Computational Science and Engineering 1–8 \\n(IEEE, 2009).\\n37.\\t\\nMillman, K. J., Brett, M., Barnowski, R. & Poline, J.-B. Teaching computational \\nreproducibility for neuroimaging. Front. Neurosci. 12, 727 (2018).\\n38.\\t Paszke, A. et\\xa0al. Pytorch: an imperative style, high-performance deep learning library. In \\nAdvances in Neural Information Processing Systems 32 (eds Wallach,\\xa0H.\\xa0et al.) 8024–8035 \\n(Neural Information Processing Systems, 2019).\\n39.\\t Abadi, M. et\\xa0al. TensorFlow: a system for large-scale machine learning. In OSDI’16: Proc. \\n12th USENIX Conf. Operating Systems Design and Implementation (chairs Keeton, K. & \\nRoscoe, T.) 265–283 (USENIX Association, 2016).\\n40.\\t Chen, T. et\\xa0al. MXNet: a flexible and efficient machine learning library for heterogeneous \\ndistributed systems. Preprint at http://www.arxiv.org/abs/1512.01274 (2015).\\n41.\\t\\nHoyer, S. & Hamman, J. xarray: N–D labeled arrays and datasets in Python. J. Open Res. \\nSoftw. 5, 10 (2017).\\n42.\\t Entschev, P. Distributed multi-GPU computing with Dask, CuPy and RAPIDS. In EuroPython \\n2019 https://ep2019.europython.eu/media/conference/slides/\\nfX8dJsD-distributed-multi-gpu-computing-with-dask-cupy-and-rapids.pdf (2019).\\n43.\\t Behnel, S. et\\xa0al. Cython: the best of both worlds. Comput. Sci. Eng. 13, 31–39 (2011).\\n44.\\t Lam, S. K., Pitrou, A. & Seibert, S. Numba: a LLVM-based Python JIT compiler. In Proc. \\nSecond Workshop on the LLVM Compiler Infrastructure in HPC, LLVM ’15 7:1–7:6 (ACM, 2015).\\n45.\\t Guelton, S. et\\xa0al. Pythran: enabling static optimization of scientific Python programs. \\nComput. Sci. Discov. 8, 014001 (2015).\\n46.\\t Dongarra, J., Golub, G. H., Grosse, E., Moler, C. & Moore, K. Netlib and NA-Net: building a \\nscientific computing community. IEEE Ann. Hist. Comput. 30, 30–41 (2008).\\n47.\\t\\nBarrett, K. A., Chiu, Y. H., Painter, J. F., Motteler, Z. C. & Dubois, P. F. Basis System, Part I: \\nRunning a Basis Program—A Tutorial for Beginners UCRL-MA-118543, Vol.\\xa01 (Lawrence \\nLivermore National Laboratory 1995).\\n48.\\t Dubois, P. F. & Motteler, Z. Basis System, Part II: Basis Language Reference Manual \\nUCRL-MA-118543, Vol.\\xa02 (Lawrence Livermore National Laboratory, 1995).\\n49.\\t Chiu, Y. H. & Dubois, P. F. Basis System, Part III: EZN User Manual UCRL-MA-118543, Vol.\\xa03 \\n(Lawrence Livermore National Laboratory, 1995).\\n50.\\t Chiu, Y. H. & Dubois, P. F. Basis System, Part IV: EZD User Manual UCRL-MA-118543, Vol.\\xa04 \\n(Lawrence Livermore National Laboratory, 1995).\\n51.\\t\\nMunro, D. H. & Dubois, P. F. Using the Yorick interpreted language. Comput. Phys. 9, \\n609–615 (1995).\\n52.\\t Ihaka, R. & Gentleman, R. R: a language for data analysis and graphics. J. Comput. Graph. \\nStat. 5, 299–314 (1996).\\n53.\\t Iverson, K. E. A programming language. In Proc. 1962 Spring Joint Computer Conf. \\n345–351 (1962).\\n54.\\t Jenness, T. et\\xa0al. LSST data management software development practices and tools. In \\nProc. SPIE 10707, Software and Cyberinfrastructure for Astronomy V 1070709 (SPIE and \\nInternational Society for Optics and Photonics, 2018).\\n55.\\t Matsakis, N. D. & Klock, F. S. The Rust language. Ada Letters 34, 103–104 (2014).\\n56.\\t Bezanson, J., Edelman, A., Karpinski, S. & Shah, V. B. Julia: a fresh approach to numerical \\ncomputing. SIAM Rev. 59, 65–98 (2017).\\n57.\\t\\nLattner, C. & Adve, V. LLVM: a compilation framework for lifelong program analysis and \\ntransformation. In Proc. 2004 Intl Symp. Code Generation and Optimization (CGO’04) \\n75–88 (IEEE, 2004).\\nAcknowledgements We thank R.\\xa0Barnowski, P.\\xa0Dubois, M.\\xa0Eickenberg, and P.\\xa0Greenfield, who \\nsuggested text and provided helpful feedback on the manuscript. K.J.M. and S.J.v.d.W. were \\nfunded in part by the Gordon and Betty Moore Foundation through grant GBMF3834 and by \\nthe Alfred P. Sloan Foundation through grant 2013-10-27 to the University of California, \\nBerkeley. S.J.v.d.W., S.B., M.P. and W.W. were funded in part by the Gordon and Betty Moore \\nFoundation through grant GBMF5447 and by the Alfred P. Sloan Foundation through grant \\nG-2017-9960 to the University of California, Berkeley.\\nAuthor contributions K.J.M. and S.J.v.d.W. composed the manuscript with input from \\nothers. S.B., R.G., K.S., W.W., M.B. and T.R. contributed text. All authors contributed \\nsubstantial code, documentation and/or expertise to the NumPy project. All authors \\nreviewed the manuscript.\\nCompeting interests The authors declare no competing interests.\\nAdditional information\\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\\n2649-2.\\nCorrespondence and requests for materials should be addressed to K.J.M., S.J.v.W. or R.G.\\nPeer review information Nature thanks Edouard Duchesnay,\\xa0Alan Edelman and the other, \\nanonymous, reviewer(s) for their contribution to the peer review of this work.\\nReprints and permissions information is available at http://www.nature.com/reprints.\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \\npublished maps and institutional affiliations.\\nOpen Access This article is licensed under a Creative Commons Attribution \\n4.0 International License, which permits use, sharing, adaptation, distribution \\nand reproduction in any medium or format, as long as you give appropriate \\ncredit to the original author(s) and the source, provide a link to the Creative Commons license, \\nand indicate if changes were made. The images or other third party material in this article are \\nincluded in the article’s Creative Commons license, unless indicated otherwise in a credit line \\nto the material. If material is not included in the article’s Creative Commons license and your \\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \\nneed to obtain permission directly from the copyright holder. To view a copy of this license, \\nvisit http://creativecommons.org/licenses/by/4.0/.\\n© The Author(s) 2020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 0}, page_content='1\\npandas: a Foundational Python Library for Data\\nAnalysis and Statistics\\nWes McKinney\\n!\\nAbstract—In this paper we will discuss pandas, a Python library of rich\\ndata structures and tools for working with structured data sets common to\\nstatistics, ﬁnance, social sciences, and many other ﬁelds. The library provides\\nintegrated, intuitive routines for performing common data manipulations and\\nanalysis on such data sets. It aims to be the foundational layer for the future of\\nstatistical computing in Python. It serves as a strong complement to the existing\\nscientiﬁc Python stack while implementing and improving upon the kinds of data\\nmanipulation tools found in other statistical programming languages such as\\nR. In addition to detailing its design and features of pandas, we will discuss\\nfuture avenues of work and growth opportunities for statistics and data analysis\\napplications in the Python language.\\nIntroduction\\nPython is being used increasingly in scientiﬁc applications\\ntraditionally dominated by [R], [MATLAB], [Stata], [SAS],\\nother commercial or open-source research environments. The\\nmaturity and stability of the fundamental numerical li-\\nbraries ([NumPy], [SciPy], and others), quality of documenta-\\ntion, and availability of “kitchen-sink” distributions ([EPD],\\n[Pythonxy]) have gone a long way toward making Python\\naccessible and convenient for a broad audience. Additionally\\n[matplotlib] integrated with [IPython] provides an interactive\\nresearch and development environment with data visualization\\nsuitable for most users. However, adoption of Python for\\napplied statistical modeling has been relatively slow compared\\nwith other areas of computational science.\\nOne major issue for would-be statistical Python program-\\nmers in the past has been the lack of libraries implementing\\nstandard models and a cohesive framework for specifying\\nmodels. However, in recent years there have been signiﬁcant\\nnew developments in econometrics ([StaM]), Bayesian statis-\\ntics ([PyMC]), and machine learning ([SciL]), among others\\nﬁelds. However, it is still difﬁcult for many statisticians to\\nchoose Python over R given the domain-speciﬁc nature of the\\nR language and breadth of well-vetted open-source libraries\\navailable to R users ([CRAN]). In spite of this obstacle, we\\nbelieve that the Python language and the libraries and tools\\ncurrently available can be leveraged to make Python a superior\\nenvironment for data analysis and statistical computing.\\nAnother issue preventing many from using Python in the\\npast for data analysis applications has been the lack of rich data\\nstructures with integrated handling of metadata. By metadata\\nwe mean labeling information about data points. For example,\\nCorresponding author can be contacted at: wesmckinn@gmail.com.\\nc○2011 Wes McKinney\\na table or spreadsheet of data will likely have labels for the\\ncolumns and possibly also the rows. Alternately, some columns\\nin a table might be used for grouping and aggregating data into\\na pivot or contingency table. In the case of a time series data\\nset, the row labels could be time stamps. It is often necessary\\nto have the labeling information available to allow many kinds\\nof data manipulations, such as merging data sets or performing\\nan aggregation or “group by” operation, to be expressed in an\\nintuitive and concise way. Domain-speciﬁc database languages\\nlike SQL and statistical languages like R and SAS have a\\nwealth of such tools. Until relatively recently, Python had few\\ntools providing the same level of richness and expressiveness\\nfor working with labeled data sets.\\nThe pandas library, under development since 2008, is\\nintended to close the gap in the richness of available data\\nanalysis tools between Python, a general purpose systems\\nand scientiﬁc computing language, and the numerous domain-\\nspeciﬁc statistical computing platforms and database lan-\\nguages. We not only aim to provide equivalent functionality\\nbut also implement many features, such as automatic data\\nalignment and hierarchical indexing, which are not readily\\navailable in such a tightly integrated way in any other libraries\\nor computing environments to our knowledge. While initially\\ndeveloped for ﬁnancial data analysis applications, we hope that\\npandas will enable scientiﬁc Python to be a more attractive\\nand practical statistical computing environment for academic\\nand industry practitioners alike. The library’s name derives\\nfrom panel data, a common term for multidimensional data\\nsets encountered in statistics and econometrics.\\nWhile we offer a vignette of some of the main features of\\ninterest in pandas, this paper is by no means comprehensive.\\nFor more, we refer the interested reader to the online docu-\\nmentation at http://pandas.sf.net ([pandas]).\\nStructured data sets\\nStructured data sets commonly arrive in tabular format, i.e.\\nas a two-dimensional list of observations and names for the\\nﬁelds of each observation. Usually an observation can be\\nuniquely identiﬁed by one or more values or labels. We show\\nan example data set for a pair of stocks over the course of\\nseveral days. The NumPy ndarray with structured dtype can\\nbe used to hold this data:\\n>>> data\\narray([(’GOOG’, ’2009-12-28’, 622.87, 1697900.0),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 1}, page_content='2\\n(’GOOG’, ’2009-12-29’, 619.40, 1424800.0),\\n(’GOOG’, ’2009-12-30’, 622.73, 1465600.0),\\n(’GOOG’, ’2009-12-31’, 619.98, 1219800.0),\\n(’AAPL’, ’2009-12-28’, 211.61, 23003100.0),\\n(’AAPL’, ’2009-12-29’, 209.10, 15868400.0),\\n(’AAPL’, ’2009-12-30’, 211.64, 14696800.0),\\n(’AAPL’, ’2009-12-31’, 210.73, 12571000.0)],\\ndtype=[(’item’, ’|S4’), (’date’, ’|S10’),\\n(’price’, ’<f8’), (’volume’, ’<f8’)])\\n>>> data[’price’]\\narray([622.87, 619.4, 622.73, 619.98, 211.61, 209.1,\\n211.64, 210.73])\\nStructured (or record) NumPy arrays such as this can be\\neffective in many applications, but in our experience they do\\nnot provide the same level of ﬂexibility and ease of use as\\nother statistical environments. One major issue is that they do\\nnot integrate well with the rest of NumPy, which is mainly\\nintended for working with arrays of homogeneous dtype.\\nR provides the data.frame class which stores mixed-\\ntype data as a collection of independent columns. The core\\nR language and its 3rd-party libraries were built with the\\ndata.frame object in mind, so most operations on such\\na data set are very natural. A data.frame is also ﬂexible\\nin size, an important feature when assembling a collection of\\ndata. The following code fragment loads the data stored in the\\nCSV ﬁle data into the variable df and adds a new column\\nof boolean values:\\n> df <- read.csv(’data’)\\nitem\\ndate\\nprice\\nvolume\\n1 GOOG 2009-12-28 622.87\\n1697900\\n2 GOOG 2009-12-29 619.40\\n1424800\\n3 GOOG 2009-12-30 622.73\\n1465600\\n4 GOOG 2009-12-31 619.98\\n1219800\\n5 AAPL 2009-12-28 211.61 23003100\\n6 AAPL 2009-12-29 209.10 15868400\\n7 AAPL 2009-12-30 211.64 14696800\\n8 AAPL 2009-12-31 210.73 12571000\\n> df$ind <- df$item == \"GOOG\"\\n> df\\nitem\\ndate\\nprice\\nvolume\\nind\\n1 GOOG 2009-12-28 622.87\\n1697900\\nTRUE\\n2 GOOG 2009-12-29 619.40\\n1424800\\nTRUE\\n3 GOOG 2009-12-30 622.73\\n1465600\\nTRUE\\n4 GOOG 2009-12-31 619.98\\n1219800\\nTRUE\\n5 AAPL 2009-12-28 211.61 23003100 FALSE\\n6 AAPL 2009-12-29 209.10 15868400 FALSE\\n7 AAPL 2009-12-30 211.64 14696800 FALSE\\n8 AAPL 2009-12-31 210.73 12571000 FALSE\\npandas provides a similarly-named DataFrame class\\nwhich implements much of the functionality of its R coun-\\nterpart, though with some important enhancements which we\\nwill discuss. Here we convert the structured array above into\\na pandas DataFrame object and similarly add the same\\ncolumn:\\n>>> from pandas import DataFrame\\n>>> data = DataFrame(data)\\n>>> data\\nitem\\ndate\\nprice\\nvolume\\n0\\nGOOG\\n2009-12-28\\n622.9\\n1.698e+06\\n1\\nGOOG\\n2009-12-29\\n619.4\\n1.425e+06\\n2\\nGOOG\\n2009-12-30\\n622.7\\n1.466e+06\\n3\\nGOOG\\n2009-12-31\\n620\\n1.22e+06\\n4\\nAAPL\\n2009-12-28\\n211.6\\n2.3e+07\\n5\\nAAPL\\n2009-12-29\\n209.1\\n1.587e+07\\n6\\nAAPL\\n2009-12-30\\n211.6\\n1.47e+07\\n7\\nAAPL\\n2009-12-31\\n210.7\\n1.257e+07\\n>>> data[’ind’] = data[’item’] == ’GOOG’\\n>>> data\\nitem\\ndate\\nprice\\nvolume\\nind\\n0\\nGOOG\\n2009-12-28\\n622.9\\n1.698e+06\\nTrue\\n1\\nGOOG\\n2009-12-29\\n619.4\\n1.425e+06\\nTrue\\n2\\nGOOG\\n2009-12-30\\n622.7\\n1.466e+06\\nTrue\\n3\\nGOOG\\n2009-12-31\\n620\\n1.22e+06\\nTrue\\n4\\nAAPL\\n2009-12-28\\n211.6\\n2.3e+07\\nFalse\\n5\\nAAPL\\n2009-12-29\\n209.1\\n1.587e+07\\nFalse\\n6\\nAAPL\\n2009-12-30\\n211.6\\n1.47e+07\\nFalse\\n7\\nAAPL\\n2009-12-31\\n210.7\\n1.257e+07\\nFalse\\nThis data can be reshaped or “pivoted” on the date and\\nitem columns into a different form for future examples by\\nmeans of the DataFrame method pivot:\\n>>> del data[’ind’] # delete ind column\\n>>> data.pivot(’date’, ’item’)\\nprice\\nvolume\\nitem\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\ndate\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\nThe result of the pivot operation has a hierarchical index\\nfor the columns. As we will show in a later section, this is\\na powerful and ﬂexible way of representing and manipulat-\\ning multidimensional data. Currently the pivot method of\\nDataFrame only supports pivoting on two columns to reshape\\nthe data, but could be augmented to consider more than just\\ntwo columns. By using hierarchical indexes, we can guarantee\\nthat the result will always be two-dimensional. Later in the\\npaper we will demonstrate the pivot_table function which\\ncan produce spreadsheet-style pivot table data summaries as\\nDataFrame objects with hierarchical rows and columns.\\nBeyond observational data, one will also frequently en-\\ncounter categorical data, which can be used to partition identi-\\nﬁers into broader groupings. For example, stock tickers might\\nbe categorized by their industry or country of incorporation.\\nHere we have created a DataFrame object cats storing\\ncountry and industry classiﬁcations for a group of stocks:\\n>>> cats\\ncountry\\nindustry\\nAAPL\\nUS\\nTECH\\nIBM\\nUS\\nTECH\\nSAP\\nDE\\nTECH\\nGOOG\\nUS\\nTECH\\nC\\nUS\\nFIN\\nSCGLY\\nFR\\nFIN\\nBAR\\nUK\\nFIN\\nDB\\nDE\\nFIN\\nVW\\nDE\\nAUTO\\nRNO\\nFR\\nAUTO\\nF\\nUS\\nAUTO\\nTM\\nJP\\nAUTO\\npandas data model\\nEach axis of a pandas data structure has an Index object\\nwhich stores labeling information about each tick along that\\naxis. The most general Index is simply a 1-dimensional\\nvector of labels (stored in a NumPy ndarray). It’s convenient\\nto think about the Index as an implementation of an ordered\\nset. In the stock data above, the row index contains simply'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 2}, page_content='3\\nsequential observation numbers, while the column index con-\\ntains the column names. The labels are not required to be\\nsorted, though a subclass of Index could be implemented to\\nrequire sortedness and provide operations optimized for sorted\\ndata (e.g. time series data).\\nThe Index object is used for many purposes:\\n• Performing lookups to select subsets of slices of an object\\n• Providing fast data alignment routines for aligning one\\nobject with another\\n• Enabling intuitive slicing / selection to form new Index\\nobjects\\n• Forming unions and intersections of Index objects\\nHere are some examples of how the index is used internally:\\n>>> index = Index([’a’, ’b’, ’c’, ’d’, ’e’])\\n>>> ’c’ in index\\nTrue\\n>>> index.get_loc(’d’)\\n3\\n>>> index.slice_locs(’b’, ’d’)\\n(1, 4)\\n# for aligning data\\n>>> index.get_indexer([’c’, ’e’, ’f’])\\narray([ 2,\\n4, -1], dtype=int32)\\nThe basic Index uses a Python dict internally to map\\nlabels to their respective locations and implement these fea-\\ntures, though subclasses could take a more specialized and\\npotentially higher performance approach.\\nMultidimensional objects like DataFrame are not proper\\nsubclasses of NumPy’s ndarray nor do they use arrays\\nwith structured dtype. In recent releases of pandas there is a\\nnew internal data structure known as BlockManager which\\nmanipulates a collection of n-dimensional ndarray objects\\nwe refer to as blocks. Since DataFrame needs to be able to\\nstore mixed-type data in the columns, each of these internal\\nBlock objects contains the data for a set of columns all\\nhaving the same type. In the example from above, we can\\nexamine the BlockManager, though most users would never\\nneed to do this:\\n>>> data._data\\nBlockManager\\nItems: [item date price volume ind]\\nAxis 1: [0 1 2 3 4 5 6 7]\\nFloatBlock: [price volume], 2 x 8, dtype float64\\nObjectBlock: [item date], 2 x 8, dtype object\\nBoolBlock: [ind], 1 x 8, dtype bool\\nThe key importance of BlockManager is that many\\noperations, e.g. anything row-oriented (as opposed to column-\\noriented), especially in homogeneous DataFrame objects,\\nare signiﬁcantly faster when the data are all stored in a\\nsingle ndarray. However, as it is common to insert and\\ndelete columns, it would be wasteful to have a reallocate-\\ncopy step on each column insertion or deletion step. As\\na result, the BlockManager effectively provides a lazy\\nevaluation scheme where-in newly inserted columns are stored\\nin new Block objects. Later, either explicitly or when certain\\nmethods are called in DataFrame, blocks having the same\\ntype will be consolidated, i.e. combined together, to form a\\nsingle homogeneously-typed Block:\\n>>> data[’newcol’] = 1.\\n>>> data._data\\nBlockManager\\nItems: [item date price volume ind newcol]\\nAxis 1: [0 1 2 3 4 5 6 7]\\nFloatBlock: [price volume], 2 x 8\\nObjectBlock: [item date], 2 x 8\\nBoolBlock: [ind], 1 x 8\\nFloatBlock: [newcol], 1 x 8\\n>>> data.consolidate()._data\\nBlockManager\\nItems: [item date price volume ind newcol]\\nAxis 1: [0 1 2 3 4 5 6 7]\\nBoolBlock: [ind], 1 x 8\\nFloatBlock: [price volume newcol], 3 x 8\\nObjectBlock: [item date], 2 x 8\\nThe separation between the internal BlockManager ob-\\nject and the external, user-facing DataFrame gives the pan-\\ndas developers a signiﬁcant amount of freedom to modify the\\ninternal structure to achieve better performance and memory\\nusage.\\nLabel-based data access\\nWhile standard []-based indexing (using __getitem__\\nand __setitem__) is reserved for column access in\\nDataFrame, it is useful to be able to index both axes of\\na DataFrame in a matrix-like way using labels. We would\\nlike to be able to get or set data on any axis using one of the\\nfollowing:\\n• A list or array of labels or integers\\n• A slice, either with integers (e.g. 1:5) or labels (e.g.\\nlab1:lab2)\\n• A boolean vector\\n• A single label\\nTo avoid excessively overloading the []-related methods,\\nleading to ambiguous indexing semantics in some cases, we\\nhave implemented a special label-indexing attribute ix on all\\nof the pandas data structures. Thus, we can pass a tuple of\\nany of the above indexing objects to get or set values.\\n>>> df\\nA\\nB\\nC\\nD\\n2000-01-03 -0.2047\\n1.007\\n-0.5397 -0.7135\\n2000-01-04\\n0.4789 -1.296\\n0.477\\n-0.8312\\n2000-01-05 -0.5194\\n0.275\\n3.249\\n-2.37\\n2000-01-06 -0.5557\\n0.2289 -1.021\\n-1.861\\n2000-01-07\\n1.966\\n1.353\\n-0.5771 -0.8608\\n>>> df.ix[:2, [’D’, ’C’, ’A’]]\\nD\\nC\\nA\\n2000-01-03 -0.7135 -0.5397 -0.2047\\n2000-01-04 -0.8312\\n0.477\\n0.4789\\n>>> df.ix[-2:, ’B’:]\\nB\\nC\\nD\\n2000-01-06\\n0.2289 -1.021\\n-1.861\\n2000-01-07\\n1.353\\n-0.5771 -0.8608\\nSetting values also works as expected.\\n>>> date1, date2 = df.index[[1, 3]]\\n>>> df.ix[date1:date2, [’A’, ’C’]] = 0\\n>>> df\\nA\\nB\\nC\\nD\\n2000-01-03 -0.6856\\n0.1362\\n0.3996\\n1.585\\n2000-01-04\\n0\\n0.8863\\n0\\n1.907\\n2000-01-05\\n0\\n-1.351\\n0\\n0.104\\n2000-01-06\\n0\\n-0.8863\\n0\\n0.1741\\n2000-01-07 -0.05927 -1.013\\n0.9923 -0.4395'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 3}, page_content='4\\nData alignment\\nOperations between related, but differently-sized data sets can\\npose a problem as the user must ﬁrst ensure that the data points\\nare properly aligned. As an example, consider time series over\\ndifferent date ranges or economic data series over varying sets\\nof entities:\\n>>> s1\\n>>> s2\\nAAPL\\n0.044\\nAAPL\\n0.025\\nIBM\\n0.050\\nBAR\\n0.158\\nSAP\\n0.101\\nC\\n0.028\\nGOOG\\n0.113\\nDB\\n0.087\\nC\\n0.138\\nF\\n0.004\\nSCGLY\\n0.037\\nGOOG\\n0.154\\nBAR\\n0.200\\nIBM\\n0.034\\nDB\\n0.281\\nVW\\n0.040\\nOne might choose to explicitly align (or reindex) one of\\nthese 1D Series objects with the other before adding them,\\nusing the reindex method:\\n>>> s1.reindex(s2.index)\\nAAPL\\n0.0440877763224\\nBAR\\n0.199741007422\\nC\\n0.137747485628\\nDB\\n0.281070058049\\nF\\nNaN\\nGOOG\\n0.112861123629\\nIBM\\n0.0496445829129\\nHowever, we often ﬁnd it preferable to simply ignore the\\nstate of data alignment:\\n>>> s1 + s2\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nF\\nNaN\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\nSAP\\nNaN\\nSCGLY\\nNaN\\nVW\\nNaN\\nHere, the data have been automatically aligned based on\\ntheir labels and added together. The result object contains\\nthe union of the labels between the two objects so that no\\ninformation is lost. We will discuss the use of NaN (Not a\\nNumber) to represent missing data in the next section.\\nClearly, the user pays linear overhead whenever automatic\\ndata alignment occurs and we seek to minimize that overhead\\nto the extent possible. Reindexing can be avoided when\\nIndex objects are shared, which can be an effective strategy\\nin performance-sensitive applications. [Cython], a widely-\\nused tool for creating Python C extensions and interfacing\\nwith C/C++ code, has been utilized to speed up these core\\nalgorithms.\\nData alignment using DataFrame occurs automatically\\non both the column and row labels. This deeply integrated\\ndata alignment differs from any other tools outside of Python\\nthat we are aware of. Similar to the above, if the columns\\nthemselves are different, the resulting object will contain the\\nunion of the columns:\\n>>> df\\n>>> df2\\nAAPL\\nGOOG\\nAAPL\\n2009-12-28\\n211.6\\n622.9\\n2009-12-28\\n2.3e+07\\n2009-12-29\\n209.1\\n619.4\\n2009-12-29\\n1.587e+07\\n2009-12-30\\n211.6\\n622.7\\n2009-12-30\\n1.47e+07\\n2009-12-31\\n210.7\\n620\\n>>> df / df2\\nAAPL\\nGOOG\\n2009-12-28\\n9.199e-06\\nNaN\\n2009-12-29\\n1.318e-05\\nNaN\\n2009-12-30\\n1.44e-05\\nNaN\\n2009-12-31\\nNaN\\nNaN\\nThis may seem like a simple feature, but in practice it grants\\nimmense freedom as there is no longer a need to sanitize\\ndata from an untrusted source. For example, if you loaded\\ntwo data sets from a database and the columns and rows,\\nthey can be added together, say, without having to do any\\nchecking whether the labels are aligned. Of course, after doing\\nan operation between two data sets, you can perform an ad\\nhoc cleaning of the results using such functions as fillna\\nand dropna:\\n>>> (df / df2).fillna(0)\\nAAPL\\nGOOG\\n2009-12-28\\n9.199e-06\\n0\\n2009-12-29\\n1.318e-05\\n0\\n2009-12-30\\n1.44e-05\\n0\\n2009-12-31\\n0\\n0\\n>>> (df / df2).dropna(axis=1, how=’all’)\\nAAPL\\n2009-12-28\\n9.199e-06\\n2009-12-29\\n1.318e-05\\n2009-12-30\\n1.44e-05\\n2009-12-31\\nNaN\\nHandling missing data\\nIt is common for a data set to have missing observations.\\nFor example, a group of related economic time series stored\\nin a DataFrame may start on different dates. Carrying\\nout calculations in the presence of missing data can lead\\nboth to complicated code and considerable performance loss.\\nWe chose to use NaN as opposed to using the NumPy\\nMaskedArray object for performance reasons (which are\\nbeyond the scope of this paper), as NaN propagates in ﬂoating-\\npoint operations in a natural way and can be easily detected\\nin algorithms. While this leads to good performance, it comes\\nwith drawbacks: namely that NaN cannot be used in integer-\\ntype arrays, and it is not an intuitive “null” value in object or\\nstring arrays (though it is used in these arrays regardless).\\nWe regard the use of NaN as an implementation detail and\\nattempt to provide the user with appropriate API functions for\\nperforming common operations on missing data points. From\\nthe above example, we can use the dropna method to drop\\nmissing data, or we could use fillna to replace missing data\\nwith a speciﬁc value:\\n>>> (s1 + s2).dropna()\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\n>>> (s1 + s2).fillna(0)\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 4}, page_content='5\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nF\\n0.0\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\nSAP\\n0.0\\nSCGLY\\n0.0\\nVW\\n0.0\\nThe reindex and fillna methods are equipped with\\na couple simple interpolation options to propagate values\\nforward and backward, which is especially useful for time\\nseries data:\\n>>> ts\\n>>> ts2\\n2000-01-03\\n0.03825\\n2000-01-03\\n0.03825\\n2000-01-04\\n-1.9884\\n2000-01-06\\n-0.0588\\n2000-01-05\\n0.73255\\n2000-01-11\\n0.04410\\n2000-01-06\\n-0.0588\\n2000-01-14\\n-0.1786\\n2000-01-07\\n-0.4767\\n2000-01-10\\n1.98008\\n2000-01-11\\n0.04410\\n>>> ts3 = ts + ts2\\n>>> ts3\\n>>> ts3.fillna(method=’ffill’)\\n2000-01-03\\n0.07649\\n2000-01-03\\n0.07649\\n2000-01-04\\nNaN\\n2000-01-04\\n0.07649\\n2000-01-05\\nNaN\\n2000-01-05\\n0.07649\\n2000-01-06\\n-0.1177\\n2000-01-06\\n-0.1177\\n2000-01-07\\nNaN\\n2000-01-07\\n-0.1177\\n2000-01-10\\nNaN\\n2000-01-10\\n-0.1177\\n2000-01-11\\n0.08821\\n2000-01-11\\n0.08821\\n2000-01-14\\nNaN\\n2000-01-14\\n0.08821\\nSeries and DataFrame also have explicit arithmetic\\nmethods with which a fill_value can be used to specify\\na treatment of missing data in the computation. An occasional\\nchoice is to treat missing values as 0 when adding two\\nSeries objects:\\n>>> ts.add(ts2, fill_value=0)\\n2000-01-03\\n0.0764931953608\\n2000-01-04\\n-1.98842046359\\n2000-01-05\\n0.732553684194\\n2000-01-06\\n-0.117727627078\\n2000-01-07\\n-0.476754320696\\n2000-01-10\\n1.9800873096\\n2000-01-11\\n0.0882102892097\\n2000-01-14\\n-0.178640361674\\nCommon ndarray methods have been rewritten to auto-\\nmatically exclude missing data from calculations:\\n>>> (s1 + s2).sum()\\n1.3103630754662747\\n>>> (s1 + s2).count()\\n6\\nSimilar to R’s is.na function, which detects NA (Not Avail-\\nable) values, pandas has special API functions isnull and\\nnotnull for determining the validity of a data point. These\\ncontrast with numpy.isnan in that they can be used with\\ndtypes other than float and also detect some other markers\\nfor “missing” occurring in the wild, such as the Python None\\nvalue.\\n>>> isnull(s1 + s2)\\nAAPL\\nFalse\\nBAR\\nFalse\\nC\\nFalse\\nDB\\nFalse\\nF\\nTrue\\nGOOG\\nFalse\\nIBM\\nFalse\\nSAP\\nTrue\\nSCGLY\\nTrue\\nVW\\nTrue\\nNote that R’s NA value is distinct from NaN. NumPy core\\ndevelopers are currently working on an NA value implementa-\\ntion that will hopefully suit the needs of libraries like pandas\\nin the future.\\nHierarchical Indexing\\nA relatively recent addition to pandas is the ability for an\\naxis to have a hierarchical index, known in the library as a\\nMultiIndex. Semantically, this means that each a location\\non a single axis can have multiple labels associated with it.\\n>>> hdf\\nA\\nB\\nC\\nfoo\\none\\n-0.9884\\n0.09406\\n1.263\\ntwo\\n1.29\\n0.08242 -0.05576\\nthree\\n0.5366\\n-0.4897\\n0.3694\\nbar\\none\\n-0.03457 -2.484\\n-0.2815\\ntwo\\n0.03071\\n0.1091\\n1.126\\nbaz\\ntwo\\n-0.9773\\n1.474\\n-0.06403\\nthree -1.283\\n0.7818\\n-1.071\\nqux\\none\\n0.4412\\n2.354\\n0.5838\\ntwo\\n0.2215\\n-0.7445\\n0.7585\\nthree\\n1.73\\n-0.965\\n-0.8457\\nHierarchical indexing can be viewed as a way to represent\\nhigher-dimensional data in a lower-dimensional data structure\\n(here, a 2D DataFrame). For example, we can select rows\\nfrom the above DataFrame by specifying only a label from\\nthe left-most level of the index:\\n>>> hdf.ix[’foo’]\\nA\\nB\\nC\\none\\n-0.9884\\n0.09406\\n1.263\\ntwo\\n1.29\\n0.08242 -0.05576\\nthree\\n0.5366\\n-0.4897\\n0.3694\\nOf course, if all of the levels are speciﬁed, we can select a\\nrow or column just as with a regular Index.\\n>>> hdf.ix[’foo’, ’three’]\\nA\\n0.5366\\nB\\n-0.4897\\nC\\n0.3694\\n# same result\\n>>> hdf.ix[’foo’].ix[’three’]\\nThe hierarchical index can be used with any axis. From the\\npivot example earlier in the paper we obtained:\\n>>> pivoted = data.pivot(’date’, ’item’)\\n>>> pivoted\\nprice\\nvolume\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\n>>> pivoted[’volume’]\\nAAPL\\nGOOG\\n2009-12-28\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n1.257e+07\\n1.22e+06\\nThere are several utility methods for manipulating a\\nMultiIndex such as swaplevel and sortlevel:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 5}, page_content='6\\n>>> swapped = pivoted.swaplevel(0, 1, axis=1)\\n>>> swapped\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\nprice\\nprice\\nvolume\\nvolume\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\n>>> swapped[’AAPL’]\\nprice\\nvolume\\n2009-12-28\\n211.6\\n2.3e+07\\n2009-12-29\\n209.1\\n1.587e+07\\n2009-12-30\\n211.6\\n1.47e+07\\n2009-12-31\\n210.7\\n1.257e+07\\nHere is an example for sortlevel:\\n>>> pivoted.sortlevel(1, axis=1)\\nprice\\nvolume\\nprice\\nvolume\\nAAPL\\nAAPL\\nGOOG\\nGOOG\\n2009-12-28\\n211.6\\n2.3e+07\\n622.9\\n1.698e+06\\n2009-12-29\\n209.1\\n1.587e+07\\n619.4\\n1.425e+06\\n2009-12-30\\n211.6\\n1.47e+07\\n622.7\\n1.466e+06\\n2009-12-31\\n210.7\\n1.257e+07\\n620\\n1.22e+06\\nAdvanced pivoting and reshaping\\nClosely related to hierarchical indexing and the earlier pivoting\\nexample, we illustrate more advanced reshaping of data using\\nthe stack and unstack methods. stack reshapes by\\nremoving a level from the columns of a DataFrame object\\nand moving that level to the row labels, producing either a\\n1D Series or another DataFrame (if the columns were a\\nMultiIndex).\\n>>> df\\nAAPL\\nGOOG\\n2009-12-28\\n211.6\\n622.9\\n2009-12-29\\n209.1\\n619.4\\n2009-12-30\\n211.6\\n622.7\\n2009-12-31\\n210.7\\n620\\n>>> df.stack()\\n2009-12-28\\nAAPL\\n211.61\\nGOOG\\n622.87\\n2009-12-29\\nAAPL\\n209.1\\nGOOG\\n619.4\\n2009-12-30\\nAAPL\\n211.64\\nGOOG\\n622.73\\n2009-12-31\\nAAPL\\n210.73\\nGOOG\\n619.98\\n>>> pivoted\\nprice\\nvolume\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\n>>> pivoted.stack()\\nprice\\nvolume\\n2009-12-28\\nAAPL\\n211.6\\n2.3e+07\\nGOOG\\n622.9\\n1.698e+06\\n2009-12-29\\nAAPL\\n209.1\\n1.587e+07\\nGOOG\\n619.4\\n1.425e+06\\n2009-12-30\\nAAPL\\n211.6\\n1.47e+07\\nGOOG\\n622.7\\n1.466e+06\\n2009-12-31\\nAAPL\\n210.7\\n1.257e+07\\nGOOG\\n620\\n1.22e+06\\nBy default, the innermost level is stacked. The level to stack\\ncan be speciﬁed explicitly:\\n>>> pivoted.stack(0)\\nAAPL\\nGOOG\\n2009-12-28\\nprice\\n211.6\\n622.9\\nvolume\\n2.3e+07\\n1.698e+06\\n2009-12-29\\nprice\\n209.1\\n619.4\\nvolume\\n1.587e+07\\n1.425e+06\\n2009-12-30\\nprice\\n211.6\\n622.7\\nvolume\\n1.47e+07\\n1.466e+06\\n2009-12-31\\nprice\\n210.7\\n620\\nvolume\\n1.257e+07\\n1.22e+06\\nThe unstack method is the inverse of stack:\\n>>> df.stack()\\n>>> df.stack().unstack()\\n2009-12-28\\nAAPL\\n211.61\\nAAPL\\nGOOG\\nGOOG\\n622.87\\n2009-12-28\\n211.6\\n622.9\\n2009-12-29\\nAAPL\\n209.1\\n2009-12-29\\n209.1\\n619.4\\nGOOG\\n619.4\\n2009-12-30\\n211.6\\n622.7\\n2009-12-30\\nAAPL\\n211.64\\n2009-12-31\\n210.7\\n620\\nGOOG\\n622.73\\n2009-12-31\\nAAPL\\n210.73\\nGOOG\\n619.98\\nThese reshaping methods can be combined with built-in\\nDataFrame and Series method to select or aggregate data\\nat a level. Here we take the maximum among AAPL and GOOG\\nfor each date / ﬁeld pair:\\n>>> pivoted.stack(0)\\nAAPL\\nGOOG\\n2009-12-28\\nprice\\n211.6\\n622.9\\nvolume\\n2.3e+07\\n1.698e+06\\n2009-12-29\\nprice\\n209.1\\n619.4\\nvolume\\n1.587e+07\\n1.425e+06\\n2009-12-30\\nprice\\n211.6\\n622.7\\nvolume\\n1.47e+07\\n1.466e+06\\n2009-12-31\\nprice\\n210.7\\n620\\nvolume\\n1.257e+07\\n1.22e+06\\n>>> pivoted.stack(0).max(1).unstack()\\nprice\\nvolume\\n2009-12-28\\n622.9\\n2.3e+07\\n2009-12-29\\n619.4\\n1.587e+07\\n2009-12-30\\n622.7\\n1.47e+07\\n2009-12-31\\n620\\n1.257e+07\\nThese kinds of aggregations are closely related to “group\\nby” operations which we discuss in the next section.\\nGroup By: grouping and aggregating data\\nA very common operation in SQL-like languages and gen-\\nerally in statistical data analysis is to group data by some\\nidentiﬁers and perform either an aggregation or transformation\\nof the data. For example, suppose we had a simple data set\\nlike this:\\n>>> df\\nA\\nB\\nC\\nD\\n0\\nfoo\\none\\n-1.834\\n1.903\\n1\\nbar\\none\\n1.772\\n-0.7472\\n2\\nfoo\\ntwo\\n-0.67\\n-0.309\\n3\\nbar\\nthree\\n0.04931\\n0.3939\\n4\\nfoo\\ntwo\\n-0.5215\\n1.861\\n5\\nbar\\ntwo\\n-3.202\\n0.9365\\n6\\nfoo\\none\\n0.7927\\n1.256\\n7\\nfoo\\nthree\\n0.1461\\n-2.655\\nWe could compute group means using the A column like\\nso:\\n>>> df.groupby(’A’).mean()\\nC\\nD\\nbar -0.4602\\n0.1944'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 6}, page_content='7\\nfoo -0.4173\\n0.4112\\nThe object returned by groupby is a special intermediate\\nobject with a lot of nice features. For example, you can use\\nit to iterate through the portions of the data set corresponding\\nto each group:\\n>>> for key, group in df.groupby(’A’):\\n...\\nprint key\\n...\\nprint group\\nbar\\nA\\nB\\nC\\nD\\n1\\nbar\\none\\n1.772\\n-0.7472\\n3\\nbar\\nthree\\n0.04931\\n0.3939\\n5\\nbar\\ntwo\\n-3.202\\n0.9365\\nfoo\\nA\\nB\\nC\\nD\\n0\\nfoo\\none\\n-1.834\\n1.903\\n2\\nfoo\\ntwo\\n-0.67\\n-0.309\\n4\\nfoo\\ntwo\\n-0.5215\\n1.861\\n6\\nfoo\\none\\n0.7927\\n1.256\\n7\\nfoo\\nthree\\n0.1461 -2.65\\nGrouping by multiple columns is also possible:\\ndf.groupby([’A’, ’B’]).mean()\\nC\\nD\\nbar\\none\\n1.772\\n-0.7472\\nthree\\n0.04931\\n0.3939\\ntwo\\n-3.202\\n0.9365\\nfoo\\none\\n-0.5205\\n1.579\\nthree\\n0.1461\\n-2.655\\ntwo\\n-0.5958\\n0.7762\\nThe default result of a multi-key groupby aggregation\\nis a hierarchical index. This can be disabled when calling\\ngroupby which may be useful in some settings:\\ndf.groupby([’A’, ’B’], as_index=False).mean()\\nA\\nB\\nC\\nD\\n0\\nbar\\none\\n1.772\\n-0.7472\\n1\\nbar\\nthree\\n0.04931\\n0.3939\\n2\\nbar\\ntwo\\n-3.202\\n0.9365\\n3\\nfoo\\none\\n-0.5205\\n1.579\\n4\\nfoo\\nthree\\n0.1461\\n-2.655\\n5\\nfoo\\ntwo\\n-0.5958\\n0.7762\\nIn a completely general setting, groupby operations are\\nabout mapping axis labels to buckets. In the above examples,\\nwhen we pass column names we are simply establishing a cor-\\nrespondence between the row labels and the group identiﬁers.\\nThere are other ways to do this; the most general is to pass a\\nPython function (for single-key) or list of functions (for multi-\\nkey) which will be invoked on each each label, producing a\\ngroup speciﬁcation:\\n>>> dat\\nA\\nB\\nC\\nD\\n2000-01-03\\n0.6371\\n0.672\\n0.9173\\n1.674\\n2000-01-04 -0.8178 -1.865\\n-0.23\\n0.5411\\n2000-01-05\\n0.314\\n0.2931 -0.6444 -0.9973\\n2000-01-06\\n1.913\\n-0.5867\\n0.273\\n0.4631\\n2000-01-07\\n1.308\\n0.426\\n-1.306\\n0.04358\\n>>> mapping\\n{’A’: ’Group 1’, ’B’: ’Group 2’,\\n’C’: ’Group 1’, ’D’: ’Group 2’}\\n>>> for name, group in dat.groupby(mapping.get,\\n...\\naxis=1):\\n...\\nprint name; print group\\nGroup 1\\nA\\nC\\n2000-01-03\\n0.6371\\n0.9173\\n2000-01-04 -0.8178 -0.23\\n2000-01-05\\n0.314\\n-0.6444\\n2000-01-06\\n1.913\\n0.273\\n2000-01-07\\n1.308\\n-1.306\\nGroup 2\\nB\\nD\\n2000-01-03\\n0.672\\n1.674\\n2000-01-04 -1.865\\n0.5411\\n2000-01-05\\n0.2931 -0.9973\\n2000-01-06 -0.5867\\n0.4631\\n2000-01-07\\n0.426\\n0.04358\\nSome creativity with grouping functions will enable the\\nuser to perform quite sophisticated operations. The object re-\\nturned by groupby can either iterate, aggregate (with an\\narbitrary function), transform (compute a modiﬁed same-\\nsize version of each data group), or do a general apply-by-\\ngroup. While we do not have space to go into great detail with\\nexamples of each of these, the apply function is interesting in\\nthat it attempts to combine the results of the aggregation into\\na pandas object. For example, we could group the df object\\nabove by column A, select just the C column, and apply the\\ndescribe function to each subgroup like so:\\n>>> df.groupby(’A’)[’C’].describe().T\\nbar\\nfoo\\ncount\\n3\\n5\\nmean\\n-0.4602\\n-0.4173\\nstd\\n2.526\\n0.9827\\nmin\\n-3.202\\n-1.834\\n10%\\n-2.552\\n-1.368\\n50%\\n0.04931 -0.5215\\n90%\\n1.427\\n0.5341\\nmax\\n1.772\\n0.7927\\nNote that, under the hood, calling describe generates\\nand passes a dynamic function to apply which invokes\\ndescribe on each group and glues the results together. We\\ntransposed the result with .T to make it more readable.\\nEasy spreadsheet-style pivot tables\\nAn obvious application combining groupby and reshaping\\noperations is creating pivot tables, a common way of sum-\\nmarizing data in spreadsheet applications such as Microsoft\\nExcel. We’ll take a brief look at a tipping data set collected\\nfrom a restaurant ([Bryant]):\\n>>> tips.head()\\nsex\\nsmoker\\ntime\\nday\\nsize\\ntip_pct\\n1\\nFemale\\nNo\\nDinner\\nSun\\n2\\n0.05945\\n2\\nMale\\nNo\\nDinner\\nSun\\n3\\n0.1605\\n3\\nMale\\nNo\\nDinner\\nSun\\n3\\n0.1666\\n4\\nMale\\nNo\\nDinner\\nSun\\n2\\n0.1398\\n5\\nFemale\\nNo\\nDinner\\nSun\\n4\\n0.1468\\nThe pivot_table function in pandas takes a set of\\ncolumn names to group on the pivot table rows, another set to\\ngroup on the columns, and optionally an aggregation function\\nfor each group (which defaults to mean):\\n>>> import numpy as np\\n>>> from pandas import pivot_table\\n>>> pivot_table(tips, ’tip_pct’, rows=[’time’, ’sex’],\\ncols=’smoker’)\\nsmoker\\nNo\\nYes\\ntime\\nsex\\nDinner Female\\n0.1568\\n0.1851'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 7}, page_content='8\\nMale\\n0.1594\\n0.1489\\nLunch\\nFemale\\n0.1571\\n0.1753\\nMale\\n0.1657\\n0.1667\\nConveniently, the returned object is a DataFrame, so it can\\nbe further reshaped and manipulated by the user:\\n>>> table = pivot_table(tips, ’tip_pct’,\\nrows=[’sex’, ’day’],\\ncols=’smoker’, aggfunc=len)\\n>>> table\\nsmoker\\nNo\\nYes\\nsex\\nday\\nFemale Fri\\n2\\n7\\nSat\\n13\\n15\\nSun\\n14\\n4\\nThur\\n25\\n7\\nMale\\nFri\\n2\\n8\\nSat\\n32\\n27\\nSun\\n43\\n15\\nThur\\n20\\n10\\n>>> table.unstack(’sex’)\\nsmoker\\nNo\\nYes\\nsex\\nFemale\\nMale\\nFemale\\nMale\\nday\\nFri\\n2\\n2\\n7\\n8\\nSat\\n13\\n32\\n15\\n27\\nSun\\n14\\n43\\n4\\n15\\nThur\\n25\\n20\\n7\\n10\\nFor many users, this will be an attractive alternative to\\ndumping a data set into a spreadsheet for the sole purpose\\nof creating a pivot table.\\n>>> pivot_table(tips, ’size’,\\nrows=[’time’, ’sex’, ’smoker’],\\ncols=’day’, aggfunc=np.sum,\\nfill_value=0)\\nday\\nFri\\nSat\\nSun\\nThur\\ntime\\nsex\\nsmoker\\nDinner Female No\\n2\\n30\\n43\\n2\\nYes\\n8\\n33\\n10\\n0\\nDinner Male\\nNo\\n4\\n85\\n124\\n0\\nYes\\n12\\n71\\n39\\n0\\nLunch\\nFemale No\\n3\\n0\\n0\\n60\\nYes\\n6\\n0\\n0\\n17\\nLunch\\nMale\\nNo\\n0\\n0\\n0\\n50\\nYes\\n5\\n0\\n0\\n23\\nCombining or joining data sets\\nCombining, joining, or merging related data sets is a quite\\ncommon operation. In doing so we are interested in associating\\nobservations from one data set with another via a merge key\\nof some kind. For similarly-indexed 2D data, the row labels\\nserve as a natural key for the join function:\\n>>> df1\\n>>> df2\\nAAPL\\nGOOG\\nMSFT\\nYHOO\\n2009-12-24\\n209\\n618.5\\n2009-12-24\\n31\\n16.72\\n2009-12-28\\n211.6\\n622.9\\n2009-12-28\\n31.17\\n16.88\\n2009-12-29\\n209.1\\n619.4\\n2009-12-29\\n31.39\\n16.92\\n2009-12-30\\n211.6\\n622.7\\n2009-12-30\\n30.96\\n16.98\\n2009-12-31\\n210.7\\n620\\n>>> df1.join(df2)\\nAAPL\\nGOOG\\nMSFT\\nYHOO\\n2009-12-24\\n209\\n618.5\\n31\\n16.72\\n2009-12-28\\n211.6\\n622.9\\n31.17\\n16.88\\n2009-12-29\\n209.1\\n619.4\\n31.39\\n16.92\\n2009-12-30\\n211.6\\n622.7\\n30.96\\n16.98\\n2009-12-31\\n210.7\\n620\\nNaN\\nNaN\\nOne might be interested in joining on something other than\\nthe index as well, such as the categorical data we presented\\nin an earlier section:\\n>>> data.join(cats, on=’item’)\\ncountry\\ndate\\nindustry item\\nvalue\\n0\\nUS\\n2009-12-28\\nTECH\\nGOOG\\n622.9\\n1\\nUS\\n2009-12-29\\nTECH\\nGOOG\\n619.4\\n2\\nUS\\n2009-12-30\\nTECH\\nGOOG\\n622.7\\n3\\nUS\\n2009-12-31\\nTECH\\nGOOG\\n620\\n4\\nUS\\n2009-12-28\\nTECH\\nAAPL\\n211.6\\n5\\nUS\\n2009-12-29\\nTECH\\nAAPL\\n209.1\\n6\\nUS\\n2009-12-30\\nTECH\\nAAPL\\n211.6\\n7\\nUS\\n2009-12-31\\nTECH\\nAAPL\\n210.7\\nThis is akin to a SQL join operation between two tables\\nor a VLOOKUP operation in a spreadsheet such as Excel. It\\nis possible to join on multiple keys, in which case the table\\nbeing joined is currently required to have a hierarchical index\\ncorresponding to those keys. We will be working on more\\njoining and merging methods in a future release of pandas.\\nPerformance and use for Large Data Sets\\nUsing DataFrame objects over homogeneous NumPy arrays\\nfor computation incurs overhead from a number of factors:\\n• Computational functions like sum, mean, and std have\\nbeen overridden to omit missing data\\n• Most of the axis Index data structures are reliant on the\\nPython dict for performing lookups and data alignment.\\nThis also results in a slightly larger memory footprint as\\nthe dict containing the label mapping is created once\\nand then stored.\\n• The internal BlockManager data structure consolidates\\nthe data of each type (ﬂoating point, integer, boolean,\\nobject) into 2-dimensional arrays. However, this is an\\nupfront cost that speeds up row-oriented computations\\nand data alignment later.\\n• Performing repeated lookups of values by label passes\\nthrough much more Python code than simple integer-\\nbased lookups on ndarray objects.\\nThe savvy user will learn what operations are not very\\nefﬁcient in DataFrame and Series and fall back on working\\ndirectly with the underlying ndarray objects (accessible\\nvia the values attribute) in such cases. What DataFrame\\nsacriﬁces in performance it makes up for in ﬂexibility and\\nexpressiveness.\\nWith 64-bit integers representing timestamps, pandas in\\nfact provides some of the fastest data alignment routines for\\ndifferently-indexed time series to be found in open source soft-\\nware. As working with large, irregularly time series requires\\nhaving a timestamp index, pandas is well-positioned to become\\nthe gold standard for high performance open source time series\\nprocessing.\\nWith regard to memory usage and large data sets, pandas\\nis currently only designed for use with in-memory data sets.\\nWe would like to expand its capability to work with data\\nsets that do not ﬁt into memory, perhaps transparently using\\nthe multiprocessing module or a parallel computing\\nbackend to orchestrate large scale computations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 8}, page_content='9\\npandas for R users\\nGiven the “DataFrame” name and feature overlap with the [R]\\nproject and its 3rd party packages, pandas will draw inevitable\\ncomparisons with R. pandas brings a robust, full-featured, and\\nintegrated data analysis toolset to Python while maintaining a\\nsimple and easy-to-use API. As nearly all data manipulations\\ninvolving data.frame objects in R can be easily expressed\\nusing the pandas DataFrame, it is relatively straightforward\\nin most cases to port R functions to Python. It would be\\nuseful to provide a migration guide for R users as we have\\nnot copied R’s naming conventions or syntax in most places,\\nrather naming based on common-sense and making the syntax\\nand API as “Pythonic” as possible.\\nR does not provide indexing functionality in nearly such a\\ndeeply integrated way as pandas does. For example, operations\\nbetween data.frame objects will proceed in R without\\nregard to whether the labels match as long as they are the\\nsame length and width. Some R packages, such as zoo and\\nxts provides indexed data structures with data alignment,\\nbut they are largely specialized to ordered time series data.\\nHierarchical indexing with constant-time subset selection is\\nanother signiﬁcant feature missing from R’s data structures.\\nOutside of the scope of this paper is a rigorous performance\\ncomparison of R and pandas. In almost all of the benchmarks\\nwe have run comparing R and pandas, pandas signiﬁcantly\\noutperforms R.\\nOther features of note\\nThere are many other features in pandas worth exploring for\\nthe interested users:\\n• Time series functionality: date range generation, shifting\\nand lagging, frequency conversion and forward/backward\\nﬁlling\\n• Integration with [matplotlib] to concisely generate plots\\nwith metadata\\n• Moving window statistics (e.g. moving standard devia-\\ntion, exponentially weighted moving average) and moving\\nwindow linear and panel regression\\n• 3-dimensional Panel data structure for manipulating\\ncollections of DataFrame objects\\n• Sparse versions of the data structures\\n• Robust IO tools for reading and writing pandas objects to\\nﬂat ﬁles (delimited text, CSV, Excel) and HDF5 format\\nRelated packages\\nA number of other Python packages have some degree of\\nfeature overlap with pandas. Among these, la ([Larry]) is\\nthe most similar, as it implements a labeled ndarray object\\nintending to closely mimic NumPy arrays. Since ndarray\\nis only applicable many problems in its homogeneous (non-\\nstructured dtype) form, in pandas we have distanced our-\\nselves from ndarray to instead provide a more ﬂexible,\\n(potentially) heterogeneous, size-mutable data structure. The\\nreferences include a some other packages of interest.\\npandas will soon become a dependency of statsmodels\\n([StaM]), the main statistics and econometric library in Python,\\nto make statistical modeling and data analysis tools in Python\\nmore cohesive and integrated. We plan to combine pandas\\nwith a formula framework to make specifying statistical mod-\\nels easy and intuitive when working with a DataFrame of\\ndata, for example.\\nConclusions\\nWe believe that in the coming years there will be great oppor-\\ntunity to attract users in need of statistical data analysis tools\\nto Python who might have previously chosen R, MATLAB,\\nor another research environment. By designing robust, easy-\\nto-use data structures that cohere with the rest of the scientiﬁc\\nPython stack, we can make Python a compelling choice for\\ndata analysis applications. In our opinion, pandas provides\\na solid foundation upon which a very powerful data analysis\\necosystem can be established.\\nREFERENCES\\n[pandas]\\nW. McKinney, pandas: a python data analysis library, http:\\n//pandas.sourceforge.net\\n[scipy2010]\\nW. McKinney, Data Structures for Statistical Computing in\\nPython Proceedings of the 9th Python in Science Conference,\\nhttp://http://conference.scipy.org/. 2010\\n[Larry]\\nK. Goodman. la / larry: ndarray with labeled axes, http://larry.\\nsourceforge.net/\\n[SciTS]\\nM. Knox, P. Gerard-Marchant, scikits.timeseries: python time\\nseries analysis, http://pytseries.sourceforge.net/\\n[StaM]\\nS. Seabold, J. Perktold, J. Taylor, statsmodels: statistical\\nmodeling in Python, http://statsmodels.sourceforge.net\\n[SciL]\\nD. Cournapeau, et al., scikit-learn: machine learning in\\nPython, http://scikit-learn.sourceforge.net\\n[PyMC]\\nC. Fonnesbeck, A. Patil, D. Huard, PyMC: Markov Chain\\nMonte Carlo for Python, http://code.google.com/p/pymc/\\n[Tab]\\nD. Yamins, E. Angelino, tabular: tabarray data structure for\\n2D data, http://parsemydata.com/tabular/\\n[NumPy]\\nT. Oliphant, http://numpy.scipy.org\\n[SciPy]\\nE. Jones, T. Oliphant, P. Peterson, http://scipy.org\\n[matplotlib]\\nJ. Hunter, et al., matplotlib: Python plotting, http://matplotlib.\\nsourceforge.net/\\n[EPD]\\nEnthought, Inc., EPD: Enthought Python Distribution, http:\\n//www.enthought.com/products/epd.php\\n[Pythonxy]\\nP. Raybaut, Python(x,y): Scientiﬁc-oriented Python distribu-\\ntion, http://www.pythonxy.com/\\n[CRAN]\\nThe R Project for Statistical Computing, http://cran.r-project.\\norg/\\n[Cython]\\nG. Ewing, R. W. Bradshaw, S. Behnel, D. S. Seljebotn, et al.,\\nThe Cython compiler, http://cython.org\\n[IPython]\\nFernando Pérez, Brian E. Granger, IPython: A System for\\nInteractive Scientiﬁc Computing, Computing in Science and\\nEngineering, vol. 9, no. 3, pp. 21-29, May/June 2007,\\ndoi:10.1109/MCSE.2007.53. http://ipython.org\\n[Grun]\\nBatalgi,\\nGrunfeld\\ndata\\nset,\\nhttp://www.wiley.com/legacy/\\nwileychi/baltagi/\\n[nipy]\\nJ. Taylor, F. Perez, et al., nipy: Neuroimaging in Python, http:\\n//nipy.sourceforge.net\\n[pydataframe] A. Straw, F. Finkernagel, pydataframe, http://code.google.com/\\np/pydataframe/\\n[R]\\nR Development Core Team. 2010, R: A Language and Envi-\\nronment for Statistical Computing, http://www.R-project.org\\n[MATLAB]\\nThe MathWorks Inc. 2010, MATLAB, http://www.mathworks.\\ncom\\n[Stata]\\nStatCorp. 2010, Stata Statistical Software: Release 11 http:\\n//www.stata.com\\n[SAS]\\nSAS Institute Inc., SAS System, http://www.sas.com\\n[Bryant]\\nBryant, P. G. and Smith, M (1995) Practical Data Analysis:\\nCase Studies in Business Statistics. Homewood, IL: Richard\\nD. Irwin Publishing:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 0}, page_content='PyTorch: An Imperative Style, High-Performance\\nDeep Learning Library\\nAdam Paszke\\nUniversity of Warsaw\\nadam.paszke@gmail.com\\nSam Gross\\nFacebook AI Research\\nsgross@fb.com\\nFrancisco Massa\\nFacebook AI Research\\nfmassa@fb.com\\nAdam Lerer\\nFacebook AI Research\\nalerer@fb.com\\nJames Bradbury\\nGoogle\\njekbradbury@gmail.com\\nGregory Chanan\\nFacebook AI Research\\ngchanan@fb.com\\nTrevor Killeen\\nSelf Employed\\nkilleent@cs.washington.edu\\nZeming Lin\\nFacebook AI Research\\nzlin@fb.com\\nNatalia Gimelshein\\nNVIDIA\\nngimelshein@nvidia.com\\nLuca Antiga\\nOrobix\\nluca.antiga@orobix.com\\nAlban Desmaison\\nOxford University\\nalban@robots.ox.ac.uk\\nAndreas Köpf\\nXamla\\nandreas.koepf@xamla.com\\nEdward Yang\\nFacebook AI Research\\nezyang@fb.com\\nZach DeVito\\nFacebook AI Research\\nzdevito@cs.stanford.edu\\nMartin Raison\\nNabla\\nmartinraison@gmail.com\\nAlykhan Tejani\\nTwitter\\natejani@twitter.com\\nSasank Chilamkurthy\\nQure.ai\\nsasankchilamkurthy@gmail.com\\nBenoit Steiner\\nFacebook AI Research\\nbenoitsteiner@fb.com\\nLu Fang\\nFacebook\\nlufang@fb.com\\nJunjie Bai\\nFacebook\\njbai@fb.com\\nSoumith Chintala\\nFacebook AI Research\\nsoumith@gmail.com\\nAbstract\\nDeep learning frameworks have often focused on either usability or speed, but\\nnot both. PyTorch is a machine learning library that shows that these two goals\\nare in fact compatible: it provides an imperative and Pythonic programming style\\nthat supports code as a model, makes debugging easy and is consistent with other\\npopular scientiﬁc computing libraries, while remaining efﬁcient and supporting\\nhardware accelerators such as GPUs.\\nIn this paper, we detail the principles that drove the implementation of PyTorch\\nand how they are reﬂected in its architecture. We emphasize that every aspect of\\nPyTorch is a regular Python program under the full control of its user. We also\\nexplain how the careful and pragmatic implementation of the key components of\\nits runtime enables them to work together to achieve compelling performance.\\nWe demonstrate the efﬁciency of individual subsystems, as well as the overall\\nspeed of PyTorch on several common benchmarks.\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\narXiv:1912.01703v1  [cs.LG]  3 Dec 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 1}, page_content='1\\nIntroduction\\nWith the increased interest in deep learning in recent years, there has been an explosion of machine\\nlearning tools. Many popular frameworks such as Caffe [1], CNTK [2], TensorFlow [3], and\\nTheano [4], construct a static dataﬂow graph that represents the computation and which can then be\\napplied repeatedly to batches of data. This approach provides visibility into the whole computation\\nahead of time, and can theoretically be leveraged to improve performance and scalability. However, it\\ncomes at the cost of ease of use, ease of debugging, and ﬂexibility of the types of computation that\\ncan be represented.\\nPrior work has recognized the value of dynamic eager execution for deep learning, and some recent\\nframeworks implement this deﬁne-by-run approach, but do so either at the cost of performance\\n(Chainer [5]) or using a less expressive, faster language (Torch [6], DyNet [7]), which limits their\\napplicability.\\nHowever, with careful implementation and design choices, dynamic eager execution can be achieved\\nlargely without sacriﬁcing performance. This paper introduces PyTorch, a Python library that\\nperforms immediate execution of dynamic tensor computations with automatic differentiation and\\nGPU acceleration, and does so while maintaining performance comparable to the fastest current\\nlibraries for deep learning. This combination has turned out to be very popular in the research\\ncommunity with, for instance, 296 ICLR 2019 submissions mentioning PyTorch.\\n2\\nBackground\\nFour major trends in scientiﬁc computing have become increasingly important for deep learning.\\nFirst, starting in the 1960s, the development of domain speciﬁc languages such as APL [8], MATLAB\\n[9], R [10] and Julia [11], turned multidimensional arrays (often referred to as tensors) into ﬁrst-class\\nobjects supported by a comprehensive set of mathematical primitives (or operators) to manipulate\\nthem. Separately, libraries such as NumPy[12], Torch[6], Eigen[13] and Lush[14] made array-based\\nprogramming productive in general purpose languages such as Python, Lisp, C++ and Lua.\\nSecond, the development of automatic differentiation [15] made it possible to fully automate\\nthe daunting labor of computing derivatives. This made it signiﬁcantly easier to experiment with\\ndifferent machine learning approaches while still allowing for efﬁcient gradient based optimization.\\nThe autograd [16] package popularized the use of this technique for NumPy arrays, and similar\\napproaches are used in frameworks such as Chainer [5], DyNet [7], Lush [14], Torch [6], Jax [17]\\nand Flux.jl [18].\\nThird, with the advent of the free software movement, the scientiﬁc community moved away from\\nclosed proprietary software such as Matlab[9], and towards the open-source Python ecosystem\\nwith packages like NumPy [12], SciPy [19], and Pandas [20]. This fulﬁlled most of the numerical\\nanalysis needs of researchers while allowing them to take advantage of a vast repository of libraries\\nto handle dataset preprocessing, statistical analysis, plotting, and more. Moreover, the openness,\\ninteroperability, and ﬂexibility of free software fostered the development of vibrant communities that\\ncould quickly address new or changing needs by extending the existing functionality of a library or if\\nneeded by developing and releasing brand new ones. While there is a rich offering of open-source\\nsoftware for neural networks in languages other than Python, starting with Lush [14] in Lisp, Torch [6]\\nin C++, Objective-C and Lua, EBLearn [21] in C++, Caffe [1] in C++, the network effects of a large\\necosystem such as Python made it an essential skill to jumpstart one’s research. Hence, since 2014,\\nmost deep learning frameworks converged on a Python interface as an essential feature.\\nFinally, the availability and commoditization of general-purpose massively parallel hardware such\\nas GPUs provided the computing power required by deep learning methods. Specialized libraries\\nsuch as cuDNN [22], along with a body of academic work (such as [23] and [24]), produced a\\nset of high-performance reusable deep learning kernels that enabled frameworks such as Caffe [1],\\nTorch7 [25], or TensorFlow [3] to take advantage of these hardware accelerators.\\nPyTorch builds on these trends by providing an array-based programming model accelerated by GPUs\\nand differentiable via automatic differentiation integrated in the Python ecosystem.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 2}, page_content='3\\nDesign principles\\nPyTorch’s success stems from weaving previous ideas into a design that balances speed and ease of\\nuse. There are four main principles behind our choices:\\nBe Pythonic\\nData scientists are familiar with the Python language, its programming model, and its\\ntools. PyTorch should be a ﬁrst-class member of that ecosystem. It follows the commonly established\\ndesign goals of keeping interfaces simple and consistent, ideally with one idiomatic way of doing\\nthings. It also integrates naturally with standard plotting, debugging, and data processing tools.\\nPut researchers ﬁrst\\nPyTorch strives to make writing models, data loaders, and optimizers as\\neasy and productive as possible. The complexity inherent to machine learning should be handled\\ninternally by the PyTorch library and hidden behind intuitive APIs free of side-effects and unexpected\\nperformance cliffs.\\nProvide pragmatic performance\\nTo be useful, PyTorch needs to deliver compelling performance,\\nalthough not at the expense of simplicity and ease of use. Trading 10% of speed for a signiﬁcantly\\nsimpler to use model is acceptable; 100% is not. Therefore, its implementation accepts added\\ncomplexity in order to deliver that performance. Additionally, providing tools that allow researchers\\nto manually control the execution of their code will empower them to ﬁnd their own performance\\nimprovements independent of those that the library provides automatically.\\nWorse is better [26]\\nGiven a ﬁxed amount of engineering resources, and all else being equal, the\\ntime saved by keeping the internal implementation of PyTorch simple can be used to implement\\nadditional features, adapt to new situations, and keep up with the fast pace of progress in the ﬁeld of\\nAI. Therefore it is better to have a simple but slightly incomplete solution than a comprehensive but\\ncomplex and hard to maintain design.\\n4\\nUsability centric design\\n4.1\\nDeep learning models are just Python programs\\nIn a surprisingly short amount of time, machine learning grew from recognizing individual digits [27]\\ninto autonomously playing StarCraft [28]. Consequently, the neural networks themselves evolved\\nrapidly from simple sequences of feed forward layers into incredibly varied numerical programs\\noften composed of many loops and recursive functions. To support this growing complexity, PyTorch\\nforegoes the potential beneﬁts of a graph-metaprogramming based approach to preserve the imperative\\nprogramming model of Python. This design was pioneered for model authoring by Chainer[5] and\\nDynet[7]. PyTorch extends this to all aspects of deep learning workﬂows. Deﬁning layers, composing\\nmodels, loading data, running optimizers, and parallelizing the training process are all expressed\\nusing the familiar concepts developed for general purpose programming.\\nThis solution ensures that any new potential neural network architecture can be easily implemented\\nwith PyTorch. For instance, layers (which in modern machine learning should really be understood\\nas stateful functions with implicit parameters) are typically expressed as Python classes whose\\nconstructors create and initialize their parameters, and whose forward methods process an input\\nactivation. Similarly, models are usually represented as classes that compose individual layers, but let\\nus state again that nothing forces the user to structure their code in that way. Listing 1 demonstrates\\nhow an entire model can be created by composing functionality provided by PyTorch such as 2d\\nconvolution, matrix multiplication, dropout, and softmax to classify gray-scale images. Note that\\nlinear layers are of course part of the library, but we show an example implementation to highlight\\nhow simple it is.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 3}, page_content='class LinearLayer(Module):\\nclass FullBasicModel(nn.Module):\\ndef __init__(self, in_sz, out_sz):\\ndef __init__(self):\\nsuper().__init__()\\nsuper().__init__()\\nt1 = torch.randn(in_sz, out_sz)\\nself.conv = nn.Conv2d(1, 128, 3)\\nself.w = nn.Parameter(t1)\\nself.fc = LinearLayer(128, 10)\\nt2 = torch.randn(out_sz)\\nself.b = nn.Parameter(t2)\\ndef forward(self, x):\\nt1 = self.conv(x)\\ndef forward(self, activations):\\nt2 = nn.functional.relu(t1)\\nt = torch.mm(activations, self.w)\\nt3 = self.fc(t1)\\nreturn t + self.b\\nreturn nn.functional.softmax(t3)\\nListing 1: A custom layer used as a building block for a simple but complete neural network.\\nThis “everything is a just a program” philosophy is not limited to just the models, and applies to\\noptimizers and data loaders as well. This facilitates the experimentation of new training techniques.\\nFor example, to implement the very popular generative adversarial networks, one needs to specify\\ntwo separate models (the generator and the discriminator), and two loss functions that depend on both\\nmodels at the same time. Rigid APIs would struggle with this setup, but the simple design employed\\nin PyTorch easily adapts to this setting as shown in Listing 2.\\ndiscriminator = create_discriminator()\\ngenerator = create_generator()\\noptimD = optim.Adam(discriminator.parameters())\\noptimG = optim.Adam(generator.parameters())\\ndef step(real_sample):\\n# (1) Update Discriminator\\nerrD_real = loss(discriminator(real_sample), real_label)\\nerrD_real.backward()\\nfake = generator(get_noise())\\nerrD_fake = loss(discriminator(fake.detach(), fake_label)\\nerrD_fake.backward()\\noptimD.step()\\n# (2) Update Generator\\nerrG = loss(discriminator(fake), real_label)\\nerrG.backward()\\noptimG.step()\\nListing 2: Simpliﬁed training of a generative adversarial networks.\\nSince PyTorch programs execute eagerly, all the features of Python are available throughout the\\nwhole design process. Print statements, standard debuggers, and common visualization tools like\\nmatplotlib all work as expected. Users do not have to wait for lengthy compilation before they can\\nstart running their programs, and more importantly intermediate computations can be observed to\\nunderstand how a model works and whether its results are correct.\\n4.2\\nInteroperability and extensibility\\nEasy and efﬁcient interoperability is one of the top priorities for PyTorch because it opens the\\npossibility to leverage the rich ecosystem of Python libraries as part of user programs. Hence,\\nPyTorch allows for bidirectional exchange of data with external libraries. For example, it provides\\na mechanism to convert between NumPy arrays and PyTorch tensors using the torch.from_numpy()\\nfunction and .numpy() tensor method. Similar functionality is also available to exchange data stored\\nusing the DLPack [29] format. Note that this exchange happens in both cases without any data\\ncopying – objects on both sides only describe how to interpret a memory region which is shared\\namong them. Hence, those operations are actually extremely cheap, and take constant time no matter\\nhow large the converted arrays are.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 4}, page_content='Moreover, many of the critical systems are designed speciﬁcally to be extensible. For instance, the\\nautomatic differentiation system allows users to add support for custom differentiable functions.\\nTo do that users can deﬁne a new subclass of torch.autograd.Function that implements forward()\\nand backward() methods, which specify the function and its derivative (or more formally the vector-\\nJacobian product). Similarly new datasets can be added by subclassing torch.utils.data.Dataset\\nand implementing two methods: __getitem__ (the indexing operator) and __len__ (the length op-\\nerator), making datasets behave like (possibly lazy) lists. How these work is completely up to the\\nimplementer, and many users leverage other Python packages for data loading. The DataLoader class\\nconsumes objects conforming to this interface and provides an iterator over the data which takes\\ncare of shufﬂing, batching, parallelization, and management of pinned CUDA memory to improve\\nthroughput.\\nMost importantly, users are free to replace any component of PyTorch that does not meet the needs or\\nperformance requirements of their project. They are all designed to be completely interchangeable,\\nand PyTorch takes great care not to impose any particular solution.\\n4.3\\nAutomatic differentiation\\nSince gradient based optimization is vital to deep learning, PyTorch must be able to automatically\\ncompute gradients of models speciﬁed by our users, and those can be arbitrary Python programs.\\nHowever, Python is a dynamic programming language that allows changing most behaviors at\\nruntime, making ahead of time source-to-source differentiation cumbersome. Instead, PyTorch uses\\nthe operator overloading approach, which builds up a representation of the computed function every\\ntime it is executed. In its current implementation [30], PyTorch performs reverse-mode automatic\\ndifferentiation, which computes the gradient of a scalar output with respect to a multivariate input.\\nDifferentiating functions with more outputs than inputs is more efﬁciently executed using forward-\\nmode automatic differentiation, but this use case is less common for machine learning applications.\\nPyTorch can be easily extended to perform forward-mode differentiation using array-level dual\\nnumbers [31, 32].\\nAnother interesting and uncommon feature of our system is that it can differentiate through code\\nemploying mutation on tensors, which is one of the basic building blocks of imperative programs.\\nTo ensure safety, we have implemented a versioning system for tensors, which lets us track their\\nmodiﬁcations and ensure that we always use the data we expect. One interesting tradeoff is that\\nwhile we could utilize techniques like copy-on-write to support arbitrary programs, we chose to not\\ngo down this path, as performance-wise it is usually beneﬁcial for the users to rewrite their code\\nto ensure that no copies have to be performed. Hence, while most mutations are benign and can\\nbe handled automatically, the really complicated cases result in a user error, which lets them know\\nthat they likely want to restructure the program. This allows us to avoid introducing subtle and\\nhard-to-ﬁnd performance cliffs.\\n5\\nPerformance focused implementation\\nRunning deep learning algorithms efﬁciently from a Python interpreter is notoriously challenging: for\\ninstance, the global interpreter lock [33] effectively ensures that only one of any number of concurrent\\nthreads is running at any given time. Deep learning frameworks based on the construction of a static\\ndata-ﬂow graph sidestep this problem by deferring the evaluation of the computation to a custom\\ninterpreter.\\nPyTorch solved the problem differently, by carefully optimizing every aspect of its execution while\\nsimultaneously empowering its users to easily leverage additional optimization strategies.\\n5.1\\nAn efﬁcient C++ core\\nDespite being closely integrated in the Python ecosystem, most of PyTorch is written in C++ to\\nachieve high performance. This core libtorch library implements the tensor data structure, the GPU\\nand CPU operators, and basic parallel primitives. It also provides the automatic differentiation system,\\nincluding the gradient formulas for most built-in functions. This ensures that the computation of the\\nderivatives of functions composed of core PyTorch operators is executed entirely in a multithreaded\\nevaluator which does not require holding the Python global interpreter lock [33]. Python bindings\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 5}, page_content='are generated using YAML meta-data ﬁles. An interesting side-effect of this approach is that it\\nallowed our community to quickly create bindings to multiple other languages resulting in projects\\nlike NimTorch [34], hasktorch [35] and others.\\nThis design also allowed us to create ﬁrst-class C++ bindings and modeling libraries that can be\\nused in places where Python is inconvenient, such as the game engine for Starcraft [36] or on mobile\\nplatforms. It is even possible to take the Python code describing a PyTorch model and run it without\\nPython using the TorchScript engine [37].\\n5.2\\nSeparate control and data ﬂow\\nPyTorch maintains a strict separation between its control (i.e. program branches, loops) and data ﬂow\\n(i.e. tensors and the operations performed on them). The resolution of the control ﬂow is handled\\nby Python and optimized C++ code executed on the host CPU, and result in a linear sequence of\\noperator invocations on the device. Operators can be run either on CPU or on GPU.\\nPyTorch is designed to execute operators asynchronously on GPU by leveraging the CUDA stream\\nmechanism [38] to queue CUDA kernel invocations to the GPUs hardware FIFO. This allows the\\nsystem to overlap the execution of Python code on CPU with tensor operators on GPU. Because\\nthe tensor operations usually take a signiﬁcant amount of time, this lets us saturate the GPU and\\nreach peak performance even in an interpreted language with fairly high overhead like Python. Note\\nthat this mechanism is nearly invisible to the user. Unless they implement their own multi-stream\\nprimitives all of the CPU-GPU synchronization is handled by the library.\\nPyTorch could leverage a similar mechanism to also execute operators asynchronously on the CPU.\\nHowever the costs of cross-thread communication and synchronization would negate the performance\\nbeneﬁt of such an optimization.\\n5.3\\nCustom caching tensor allocator\\nAlmost every operator must dynamically allocate an output tensor to hold the result of its execution.\\nIt is therefore critical to optimize the speed of the dynamic memory allocators. PyTorch can rely on\\noptimized libraries [39–41] to handle this task on CPU. However, on GPU the cudaFree routine may\\nblock its caller until all previously queued work on all GPUs completes. To avoid this bottleneck,\\nPyTorch implements a custom allocator which incrementally builds up a cache of CUDA memory\\nand reassigns it to later allocations without further use of CUDA APIs. The incremental allocation\\nis also crucial for better interoperability, because taking up all GPU memory ahead of time would\\nprevent the user from utilizing other GPU-enabled Python packages.\\nTo further improve its effectiveness, this allocator was tuned for the speciﬁc memory usage patterns of\\ndeep learning. For example, it rounds up allocations to multiples of 512 bytes to avoid fragmentation\\nissues. Moreover, it maintains a distinct pool of memory for every CUDA stream (work queue).\\nThe one-pool-per-stream design assumption simpliﬁes the implementation and improves the perfor-\\nmance of the allocator: because the CPU runs ahead of the GPU, memory is freed on the CPU before\\nits last use on the GPU ﬁnishes. Since streams serialize execution, if the free precedes the reallocation\\non the CPU, the same order will occur on the GPU. So the allocator can reallocate memory freed on\\nthe CPU immediately as long as the new allocation is used on the same stream as the freed region.\\nHowever, if an allocation was last used on one stream and then allocated on another, additional\\nsynchronization is needed.\\nThe one-pool-per-stream design seems limiting since the allocations end up fragmented per stream, but\\nin practice PyTorch almost never uses multiple streams. It is notoriously hard to write CUDA kernels\\nin a way that would let them cooperatively share the GPU because exact scheduling is hardware\\ncontrolled. In practice, kernel writers usually resort to monolithic kernels that combine multiple tasks.\\nData loading and distributed computing utilities are exceptions to the one stream design, and they\\ncarefully insert additional synchronization to avoid bad interactions with the allocator.\\nWhile this design is susceptible to certain corner cases, it almost never exhibits unwanted behaviors\\nin practical code. Most of our users are not aware of its existence.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 6}, page_content='5.4\\nMultiprocessing\\nDue to the global interpreter lock (GIL) Python’s default implementation does not allow concurrent\\nthreads to execute in parallel. To alleviate this problem, the Python community has established a\\nstandard multiprocessing module, containing a number of utilities that allow users to easily spawn\\nchild processes and implement basic inter-process communication primitives.\\nHowever, the implementation of the primitives uses the same form of serialization used for on-disk\\npersistence, which is inefﬁcient when dealing with large arrays. Hence, PyTorch extends the Python\\nmultiprocessing module into torch.multiprocessing, which is a drop-in replacement for the\\nbuilt in package and automatically moves the data of tensors sent to other processes to shared memory\\ninstead of sending it over the communication channel.\\nThis design greatly improves performance and makes the process isolation weaker, resulting in a\\nprogramming model which more closely resembles regular threaded programs. Users can easily\\nimplement heavily parallel programs that operate on independent GPUs but later synchronize gradients\\nusing all-reduce style primitives.\\nAnother unique feature of this system is that it transparently handles sharing of CUDA tensors,\\nmaking it easy to implement techniques like Hogwild [42].\\n5.5\\nReference counting\\nUsers often design their models to utilize all memory available during training, and increasing batch\\nsizes is a common technique of speeding up the process. Therefore, to deliver great performance,\\nPyTorch has to treat memory as a scarce resource that it needs to manage carefully.\\nLibraries with eager semantics have to manage tensor memory without knowing how it will be used\\nin the future. Garbage collection is the typical way to handle this automatically because it has good\\namortized performance. In this approach, the runtime periodically investigates the state of the system,\\nenumerates used objects and frees everything else. However, by deferring the deallocation, it causes\\nthe program to use more memory overall [43]. Given the scarcity of GPU memory, these overheads\\nare unacceptable. In fact, Torch7 utilized the garbage collector built into Lua, and a common anti-\\npattern among the users was to sprinkle the program with explicit triggers to the garbage collector,\\nhoping that the memory errors go away.\\nPyTorch takes a different approach: it relies on a reference counting scheme to track the number of\\nuses of each tensor, and frees the underlying memory immediately once this count reaches zero. Note\\nthat PyTorch tracks both references internal to the libtorch library and external references made by\\nusers in their Python code by integrating with Python’s own reference counting mechanism. This\\nensures that memory is released exactly when tensors become unneeded.\\nOne notable caveat is that we can only guarantee the desired performance characteristics in implemen-\\ntations of languages that either already utilize reference counting (CPython, Swift, but not PyPy or\\nmany scripting languages such as Lua), and those that allow for user-deﬁned behavior for assignment,\\ncopies, and moves (e.g. C++, Rust). Bindings to implementations that do not satisfy those criteria\\nwill have to implement their own specialized memory management on top of PyTorch.\\n6\\nEvaluation\\nIn this section we compare the performance of PyTorch with several other commonly-used deep\\nlearning libraries, and ﬁnd that it achieves competitive performance across a range of tasks. All\\nexperiments were performed on a workstation with two Intel Xeon E5-2698 v4 CPUs and one\\nNVIDIA Quadro GP100 GPU.\\n6.1\\nAsynchronous dataﬂow\\nWe start by quantifying the ability of PyTorch to asynchronously execute dataﬂow on GPU. We use\\nthe built-in proﬁler [44] to instrument various benchmarks and record a timeline of the execution of a\\nsingle training step.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 7}, page_content='Figure 1 shows a representative timeline of execution for the ﬁrst few operations of a ResNet-50\\nmodel. The host CPU which queues the work quickly outpaces the execution of the operators on\\nthe GPU. This allows PyTorch to achieve almost perfect device utilization. In this example, GPU\\nexecution takes around three times longer than CPU scheduling. The exact ratio depends on the\\nrelative performance of the host CPU and the GPU, as well as the number of elements in each tensor\\nand the average arithmetic complexity of the ﬂoating point computations to be performed on the\\nGPU.\\nFigure 1: A trace of the ﬁrst few operators of Resnet-50. The top row depicts the execution of the control\\nﬂow running on the host CPU. The gray areas are Python code executed by its interpreter. The colored areas\\ncorrespond to the work done on the host CPU to queue various operators (convolution, batch normalization, and\\nso on). The bottom row shows the corresponding execution of those operators on the GPU. The arrows pair the\\ntwo events in time.\\n6.2\\nMemory management\\nWe used the NVIDIA proﬁler to trace the execution of the CUDA runtime as well as the execution\\nof the CUDA kernels launched during one training iteration of the ResNet-50 model. As shown in\\nFigure 2, the behavior of the ﬁrst iteration differs signiﬁcantly from that of subsequent ones. At\\nﬁrst, calls to the CUDA memory management functions (cudaMalloc and cudaFree) slow down the\\nexecution quite dramatically by blocking the CPU thread for long periods of time, hence lowering\\nthe utilization of the GPU. This effect disappears in subsequent iterations as the PyTorch caching\\nmemory allocator starts reusing previously allocated regions.\\nFigure 2: Annotated traces of the execution of ResNet-50 on GPU.\\n6.3\\nBenchmarks\\nFinally, we can get an overall sense of single-machine eager mode performance of PyTorch by com-\\nparing it to three popular graph-based deep learning frameworks (CNTK, MXNet and TensorFlow), a\\ndeﬁne-by-run framework (Chainer), and production oriented platform (PaddlePaddle). The Appendix\\ndetails all the steps needed to reproduce our setup.\\nOur results are summarized in Table 1. On all the benchmarks, the performance of PyTorch is within\\n17% of that of of the fastest framework. We attribute this result to the fact that these tools ofﬂoad\\nmost of the computation to the same version of the cuDNN and cuBLAS libraries.\\nFramework\\nThroughput (higher is better)\\nAlexNet\\nVGG-19\\nResNet-50\\nMobileNet\\nGNMTv2\\nNCF\\nChainer\\n778 ± 15\\nN/A\\n219 ± 1\\nN/A\\nN/A\\nN/A\\nCNTK\\n845 ± 8\\n84 ± 3\\n210 ± 1\\nN/A\\nN/A\\nN/A\\nMXNet\\n1554 ± 22\\n113 ± 1\\n218 ± 2\\n444 ± 2\\nN/A\\nN/A\\nPaddlePaddle\\n933 ± 123\\n112 ± 2\\n192 ± 4\\n557 ± 24\\nN/A\\nN/A\\nTensorFlow\\n1422 ± 27\\n66 ± 2\\n200 ± 1\\n216 ± 15\\n9631 ± 1.3%\\n4.8e6 ± 2.9%\\nPyTorch\\n1547 ± 316\\n119 ± 1\\n212 ± 2\\n463 ± 17\\n15512 ± 4.8%\\n5.4e6 ± 3.4%\\nTable 1: Training speed for 6 models using 32bit ﬂoats. Throughput is measured in images per second for the\\nAlexNet, VGG-19, ResNet-50, and MobileNet models, in tokens per second for the GNMTv2 model, and in\\nsamples per second for the NCF model. The fastest speed for each model is shown in bold.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 8}, page_content='6.4\\nAdoption\\nThe validity of design decisions and their impact on ease-of-use is hard to measure. As a proxy,\\nwe tried to quantify how well the machine learning community received PyTorch by counting how\\noften various machine learning tools (including Caffe, Chainer, CNTK, Keras, MXNet, PyTorch,\\nTensorFlow, and Theano) are mentioned on arXiv e-Prints since the initial release of PyTorch in\\nJanuary 2017. In Figure 3 we report the monthly number of mentions of the word \"PyTorch\" as a\\npercentage of all mentions among these deep learning frameworks. We counted tools mentioned\\nmultiple times in a given paper only once, and made the search case insensitive to account for various\\nspellings.\\nFigure 3: Among arXiv papers each month that mention common deep learning frameworks, percentage of\\nthem that mention PyTorch.\\n7\\nConclusion and future work\\nPyTorch has become a popular tool in the deep learning research community by combining a focus\\non usability with careful performance considerations. In addition to continuing to support the latest\\ntrends and advances in deep learning, in the future we plan to continue to improve the speed and\\nscalability of PyTorch. Most notably, we are working on the PyTorch JIT: a suite of tools that\\nallow PyTorch programs to be executed outside of the Python interpreter where they can be further\\noptimized. We also intend to improve support for distributed computation by providing efﬁcient\\nprimitives for data parallelism as well as a Pythonic library for model parallelism based around\\nremote procedure calls.\\n8\\nAcknowledgements\\nWe are grateful to the PyTorch community for their feedback and contributions that greatly inﬂuenced\\nthe design and implementation of PyTorch. We thank all the PyTorch core team members, contributors\\nand package maintainers including Ailing Zhang, Alex Suhan, Alfredo Mendoza, Alican Bozkurt,\\nAndrew Tulloch, Ansha Yu, Anthony Shoumikhin, Bram Wasti, Brian Vaughan, Christian Puhrsch,\\nDavid Reiss, David Riazati, Davide Libenzi, Dmytro Dzhulgakov, Dwaraj Rajagopal, Edward Yang,\\nElias Ellison, Fritz Obermeyer, George Zhang, Hao Lu, Hong Xu, Hung Duong, Igor Fedan, Ilia\\nCherniavskii, Iurii Zdebskyi, Ivan Kobzarev, James Reed, Jeff Smith, Jerry Chen, Jerry Zhang, Jiakai\\nLiu, Johannes M. Dieterich, Karl Ostmo, Lin Qiao, Martin Yuan, Michael Suo, Mike Ruberry, Mikhail\\nZolothukhin, Mingzhe Li, Neeraj Pradhan, Nick Korovaiko, Owen Anderson, Pavel Belevich, Peter\\nJohnson, Pritam Damania, Raghuraman Krishnamoorthi, Richard Zou, Roy Li, Rui Zhu, Sebastian\\nMessmer, Shen Li, Simon Wang, Supriya Rao, Tao Xu, Thomas Viehmann, Vincent Quenneville-\\nBelair, Vishwak Srinivasan, Vitaly Fedyunin, Wanchao Liang, Wei Yang, Will Feng, Xiaomeng Yang,\\nXiaoqiang Zheng, Xintao Chen, Yangqing Jia, Yanli Zhao, Yinghai Lu and Zafar Takhirov.\\nReferences\\n[1] Yangqing \"Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,\\nSergio Guadarrama, and Trevor\" Darrell. \"caffe: Convolutional architecture for fast feature\\nembedding\". \"arXiv preprint arXiv:1408.5093\", \"2014\".\\n[2] Frank Seide and Amit Agarwal. Cntk: Microsoft’s open-source deep-learning toolkit. In\\nProceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery\\nand Data Mining, KDD ’16, pages 2135–2135, New York, NY, USA, 2016. ACM.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 9}, page_content='[3] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\\nGreg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\\nAndrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,\\nManjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek\\nMurray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\\nTalwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete\\nWarden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-\\nscale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.\\n[4] Theano Development Team. Theano: A Python framework for fast computation of mathematical\\nexpressions. arXiv e-prints, abs/1605.02688, May 2016.\\n[5] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open\\nsource framework for deep learning. In Proceedings of Workshop on Machine Learning Systems\\n(LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing\\nSystems (NIPS), 2015.\\n[6] Ronan Collobert, Samy Bengio, and Johnny Mariéthoz. Torch: a modular machine learning\\nsoftware library. Technical report, Idiap, 2002.\\n[7] G. Neubig, C. Dyer, Y. Goldberg, A. Matthews, W. Ammar, A. Anastasopoulos, M. Balles-\\nteros, D. Chiang, D. Clothiaux, T. Cohn, K. Duh, M. Faruqui, C. Gan, D. Garrette, Y. Ji,\\nL. Kong, A. Kuncoro, G. Kumar, C. Malaviya, P. Michel, Y. Oda, M. Richardson, N. Saphra,\\nS. Swayamdipta, and P. Yin. DyNet: The Dynamic Neural Network Toolkit. ArXiv e-prints,\\nJanuary 2017.\\n[8] Philip S. Abrams. An APL Machine. PhD thesis, Stanford University, 1970.\\n[9] The MathWorks, Inc., Natick, Massachusetts, United States. MATLAB and Statistics Toolbox.\\n[10] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for\\nStatistical Computing, Vienna, Austria.\\n[11] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to\\nnumerical computing. SIAM review, 59(1):65–98, 2017.\\n[12] Travis Oliphant.\\nNumPy:\\nA guide to NumPy.\\nUSA: Trelgol Publishing, 2006.\\nhttp://www.numpy.org/.\\n[13] Gaël Guennebaud, Benoît Jacob, et al. Eigen v3. http://eigen.tuxfamily.org, 2010.\\n[14] Y LeCun and L Bottou.\\nLush reference manual.\\nTechnical report, code available at\\nhttp://lush.sourceforge.net, 2002.\\n[15] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark\\nSiskind. Automatic differentiation in machine learning: A survey. J. Mach. Learn. Res.,\\n18(1):5595–5637, January 2017.\\n[16] Dougal Maclaurin. Modeling, Inference and Optimization with Composable Differentiable\\nProcedures. PhD thesis, Harvard University, April 2016.\\n[17] Matthew Johnson et. al. Jax. https://github.com/google/jax, 2018.\\n[18] Mike Innes et. al. Flux.jl. https://github.com/FluxML/Flux.jl, 2018.\\n[19] Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientiﬁc tools for\\nPython, 2001–. http://www.scipy.org/.\\n[20] Wes McKinney. Data structures for statistical computing in python. In Proceedings of the 9th\\nPython in Science Conference, 51-56, 2010.\\n[21] Pierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Eblearn: Open-source energy-based\\nlearning in c++. In 2009 21st IEEE International Conference on Tools with Artiﬁcial Intelligence,\\npages 693–697. IEEE, 2009.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 10}, page_content='[22] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan D. Cohen, John Tran, Bryan\\nCatanzaro, and Evan Shelhamer.\\ncudnn: Efﬁcient primitives for deep learning.\\nCoRR,\\nabs/1410.0759, 2014.\\n[23] Andrew Lavin. maxdnn: An efﬁcient convolution kernel for deep learning with maxwell gpus,\\nJanuary 2015.\\n[24] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. 2016 IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), pages 4013–4021, 2016.\\n[25] Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet. Torch7: A matlab-like environment\\nfor machine learning. In NIPS 2011, 2011.\\n[26] Richard Gabriel. The rise of worse is better. http://dreamsongs.com/RiseOfWorseIsBetter.html.\\n[27] Yann\\nLeCun\\nand\\nCorinna\\nCortes.\\nMNIST\\nhandwritten\\ndigit\\ndatabase.\\nhttp://yann.lecun.com/exdb/mnist/.\\n[28] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\\nMichelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, John\\nQuan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David\\nSilver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence,\\nAnders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement\\nlearning. CoRR, abs/1708.04782, 2017.\\n[29] DMLC. Dlpack: Open in memory tensor structure. https://github.com/dmlc/dlpack.\\n[30] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\\npytorch. In NIPS Workshop, 2017.\\n[31] Dan Piponi. Automatic differentiation, C++ templates, and photogrammetry. J. Graphics, GPU,\\n& Game Tools, 9(4):41–55, 2004.\\n[32] Holger Leuck and Hans-Hellmut Nagel. Automatic differentiation facilitates of-integration\\ninto steering-angle-based road vehicle tracking. In 1999 Conference on Computer Vision and\\nPattern Recognition (CVPR ’99), 23-25 June 1999, Ft. Collins, CO, USA, pages 2360–2365,\\n1999.\\n[33] The\\nPython\\nteam.\\nThe\\ncpython\\nglobal\\ninterpreter\\nlock.\\nhttps://wiki.python.org/moin/GlobalInterpreterLock.\\n[34] Giovanni Petrantoni and Jörg Wollenschläger.\\nNimtorch.\\nhttps://github.com/fragcolor-\\nxyz/nimtorch.\\n[35] Austin\\nHuang,\\nJunji\\nHashimoto,\\nand\\nSam\\nStites.\\nHasktorch.\\nhttps://github.com/hasktorch/hasktorch.\\n[36] G. Synnaeve, Z. Lin, J. Gehring, D. Gant, V. Mella, V. Khalidov, N. Carion, and N. Usunier.\\nForward modeling for partial observation strategy games - a starcraft defogger. In Advances in\\nNeural Information Processing Systems, pages 10761–10771, 2018.\\n[37] The PyTorch team. Torch Script. https://pytorch.org/docs/stable/jit.html.\\n[38] Justin Luitjens. Cuda streams. GPU technology conference, 2014.\\n[39] Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson. Hoard:\\nA scalable memory allocator for multithreaded applications. In Proceedings of the Ninth\\nInternational Conference on Architectural Support for Programming Languages and Operating\\nSystems, ASPLOS IX, pages 117–128, New York, NY, USA, 2000. ACM.\\n[40] J. Evans. A scalable concurrent malloc(3) implementation for freebsd. In In BSDCan — The\\nTechnical BSD Conference, May 2006.\\n[41] S. Ghemawat and P. Menage. Tcmalloc: Thread-caching malloc.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 11}, page_content='[42] Benjamin Recht, Christopher Ré, Stephen J. Wright, and Feng Niu. Hogwild: A lock-free\\napproach to parallelizing stochastic gradient descent. In Advances in Neural Information\\nProcessing Systems 24: 25th Annual Conference on Neural Information Processing Systems\\n2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain., pages 693–701,\\n2011.\\n[43] Matthew Hertz and Emery D. Berger. Quantifying the performance of garbage collection vs.\\nexplicit memory management. In Proceedings of the 20th Annual ACM SIGPLAN Conference\\non Object-oriented Programming, Systems, Languages, and Applications, OOPSLA ’05, pages\\n313–326, New York, NY, USA, 2005. ACM.\\n[44] The PyTorch team. Pytorch Autograd Proﬁler. https://pytorch.org/docs/1.0.1/autograd.html#proﬁler.\\n12'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 0}, page_content=\"Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n132 \\nA Research of Challenges and Solutions in Retrieval \\nAugmented Generation (RAG) Systems \\nJiafeng Gu * \\nSchool of CS and Math, University of Puget Sound, WA, United States \\n* Corresponding Author Email: jgu@pugetsound.edu \\nAbstract. Retrieval-Augmented Generation (RAG) systems represent a significant innovation in the \\nfield of Natural Language Processing (NLP), ingeniously integrating Large Language Models (LLMs) \\nwith dynamic external knowledge retrieval. This amalgamation not only enhances the models' \\nresponsiveness to real-world knowledge but also addresses the limitations of conventional \\ngenerative models in terms of knowledge update velocity and factual accuracy. This review \\nexamines the challenges faced by RAG systems and their solutions. It delves into the central \\narchitecture of RAG systems, encompassing retrieval components, generative components, and \\nknowledge bases, with a particular focus on recent advancements that have expanded the \\nboundaries of performance and functionality. The study critically analyzes major challenges such as \\nretrieval efficiency and dynamic knowledge management. This paper evaluates various advanced \\nsolutions proposed in recent literature, comparing their efficacy and discussing the trade-offs \\ninvolved. Ultimately, this paper aims to provide researchers, developers, and users of RAG systems \\nwith a comprehensive perspective, fostering ongoing innovation and the expansion of applications \\nin this domain. \\nKeywords: Retrieval augmented generation, natural language processing, information retrieval, \\nknowledge base. \\n1. Introduction \\nRetrieval-Augmented Generation (RAG) systems have emerged as a groundbreaking approach in \\nnatural language processing, tackling the fundamental limitations of traditional Large Language \\nModels (LLMs). By leveraging the power of LLMs with dynamic access to external knowledge, RAG \\nsystems represent a significant advancement in AI and Natural Language Processing (NLP) [1]. This \\ninnovative approach allows for the generation of more accurate, relevant, and up-to-date responses \\nacross a wide range of applications. The significance of RAG research lies in its potential to transform \\nAI applications fundamentally. These systems enhance text accuracy and relevance, improve AI's \\ncapability to handle complex, knowledge-intensive tasks, and enable continuous knowledge updating \\nwithout the need for constant model retraining. This dynamic integration of retrieval and generation \\nmechanisms addresses the longstanding challenge of knowledge staleness in pre-trained language \\nmodels, opening new avenues for more adaptive and context-aware AI systems. \\nCurrent challenges in RAG systems span various aspects of their architecture and functionality. \\nWhile recent advancements have made significant strides, they often come with their own limitations. \\nFor instance, the Retrieval-Enhanced Transformer (RETRO) has shown impressive scalability, \\ncapable of retrieving from databases with trillions of tokens and demonstrating competitive \\nperformance with models 25 times its size [2]. However, RETRO faces challenges in computational \\nefficiency and the possibility of mistakes spreading in its iterative retrieval process. Another cutting-\\nedge approach, the atlas model, employs few-shot learning with retrieval augmented language models, \\ngetting great performance on various knowledge-intensive tasks [3]. Despite its impressive \\nperformance, Atlas still faces challenges in efficiently updating its knowledge base and may struggle \\nwith queries that require real-time information retrieval and integration. \\nThis paper aims to provide a comprehensive review of these challenges and the proposed cutting-\\nedge solutions. It analyzes the architecture and components of RAG systems in depth, identifying key \\nchallenges and their impact on system performance. By critically reviewing and comparing existing \\nsolutions, this paper highlights both their achievements and limitations. Furthermore, this work\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 1}, page_content='Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n133 \\nexplores promising future research directions that could address current limitations but may also \\nintroduce new challenges in data processing and model design. \\n2. Architecture of RAG System \\nRAG systems consist of three primary components: the retrieval component, the generation \\ncomponent, and the knowledge base. Each plays a crucial role in producing accurate, relevant, and \\nup-to-date responses (Fig.1). The system retrieves relevant content based on user queries using this \\nembedded knowledge base. The retrieved chunks are then combined with the original query to form \\na prompt, which is processed by a LLM to generate the final response.  \\n \\nFig 1. The workflow of a RAG system (Photo/Picture credit: Original).  \\n2.1. Retrieval Component \\nThe retrieval component serves as the critical bridge between user input and the vast repository of \\ninformation stored in the knowledge base. Its primary function is to identify and extract relevant \\ninformation based on the input query. This process involves several complex steps, including query \\nunderstanding, eﬀicient searching, and relevance ranking. \\nRecent advancements in dense retrieval methods, such as those proposed by Karpukhin et al., have \\nsignificantly improved the effectiveness of this component [4]. These methods leverage dense vector \\nrepresentations of both queries and documents, enabling more nuanced semantic matching compared \\nto traditional lexical retrieval approaches. The main advantage of this component lies in its ability to \\naccess and utilize vast amounts of external knowledge, potentially overcoming the limitations of static \\nknowledge inherent in traditional language models. Hybrid retrieval approaches have also emerged \\nas a promising direction. For instance, GAO proposed a method combining sparse and dense retrieval \\ntechniques [5]. This approach aims to leverage the strengths of both lexical and semantic matching, \\npotentially offering more robust performance across diverse query types. \\nHowever, challenges persist in achieving optimal performance, particularly in terms of query \\ninterpretation and balancing semantic relevance with diversity in the retrieved information. The \\nretrieval component must not only consider the semantic similarity between the query and potential \\nmatches but also ensure a diverse set of relevant information to provide comprehensive context for \\nthe generation task.  \\n2.2. Generation Component \\nThe generation component, typically based on a large language model, is responsible for producing \\nthe final output in RAG systems. This crucial element integrates the retrieved information with the \\noriginal input to generate coherent, contextually appropriate, and informative responses. The'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 2}, page_content='Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n134 \\ngeneration process involves several key steps: context integration, text generation, and output \\nrefinement. \\nRecent work has shown promising results in improving the generation component\\'s ability to \\neffectively utilize retrieved information. A significant breakthrough in this area is the Fusion-in-\\nDecoder model proposed by lzacard [6]. This model processes all retrieved passages jointly in the \\ndecoder, allowing for more effective integration of information from multiple sources. This approach \\ndemonstrates the potential for RAG systems to adapt quickly to new tasks and domains with minimal \\nfine-tuning. Another notable advancement is the development of iterative retrieval-generation models, \\nas demonstrated by Shuster [7]. These models involve multiple rounds of retrieval and generation, \\nenabling the system to handle complex queries that may require multi-step reasoning or information \\ngathering. Researchers have also explored integrating external knowledge graphs and structured data \\nwithin the generation process. Xu Yichong proposed an approach that leverages both retrieved textual \\ninformation and structured knowledge, potentially improving the factual accuracy and logical \\ncoherence of generated outputs [8]. \\nThe strength of this component lies in its ability to generate fluent, coherent, and contextually \\nrelevant responses. By leveraging the power of large language models and augmenting them with \\nretrieved information, RAG systems can produce outputs that are both linguistically sophisticated and \\nfactually grounded. \\nHowever, significant challenges remain. Ensuring factual consistency between the generated \\ncontent and the retrieved information is a critical issue. The model must accurately incorporate the \\nretrieved facts while maintaining the overall coherence and fluency of the generated text. Additionally, \\nmaintaining consistency and coherence across longer outputs poses another significant challenge, \\nrequiring sophisticated mechanisms for long-range dependency modeling and content planning. \\n2.3. Knowledge Base \\nThe knowledge base serves as the external memory of the RAG system. Recent research has \\nexplored various approaches to knowledge base design, including the integration of diverse data \\nformats. \\nOne significant innovation is the creation of dynamic knowledge bases that can be efficiently \\nupdated. The Generative Pseudo-Labeling (GPL) method proposed by Wang Kexin, allows for \\ncontinuous learning and updating of the knowledge base [9]. This approach enables RAG systems to \\nincorporate new information without the need for full retraining, which is particularly crucial in \\ndomains with rapidly evolving knowledge. \\nResearchers have also explored multi-modal knowledge bases. For instance, Gao introduced a \\nmulti-modal retrieval-augmented framework that can process and integrate information from both \\ntextual and visual sources [10]. This advancement allows RAG systems to leverage a broader range \\nof information types, potentially enhancing their ability to handle complex, multi-modal queries. \\n3. Challenges in RAG Systems \\nDynamic knowledge management presents complex challenges in keeping the knowledge base up-\\nto-date while maintaining system performance. This is particularly critical in domains with rapidly \\nevolving information, such as news, scientific research, or social media trends. \\n3.1. Scalability \\nThe fundamental challenge of scalability lies in the curse of dimensionality. As the volume of data \\nincreases, the search space grows exponentially, making it computationally intractable to perform \\nexact nearest neighbor search in high-dimensional spaces. This is particularly problematic for dense \\nvector representations used in modern retrieval systems. \\nIn high-dimensional spaces, the concept of \"nearest\" neighbor becomes less meaningful due to the \\nphenomenon known as \"distance concentration\". As dimensionality increases, the ratio of the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 3}, page_content='Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n135 \\ndistances of the nearest and farthest neighbors to a given point approaches 1, making it difficult to \\ndistinguish between close and far points [11]. This phenomenon significantly impacts the \\neffectiveness of traditional similarity search algorithms. \\nWhile approximate methods like Locality-Sensitive Hashing (LSH) or Hierarchical Navigable \\nSmall World (HNSW) graphs offer potential solutions, they introduce a complex trade-off between \\naccuracy and speed. For instance, LSH may miss some nearest neighbors, while HNSW requires \\ncareful tuning of its graph structure to balance between search speed and index build time. Optimizing \\nthese trade-offs remains a significant challenge, especially as the scale of data continues to grow. \\nRecent research has explored hybrid approaches to address these scalability issues. For example, \\nthe ScaNN method combines quantization for fast in-memory search with anisotropic vector \\nquantization for reduced search space [12]. However, such methods still struggle with dynamic \\nupdates to the index, which is crucial for real-time RAG systems. The challenge of developing \\nscalable methods that can adapt to varying conditions while maintaining retrieval quality remains an \\nopen problem in the field. \\n3.2. Query Reformulation \\nQuery reformulation in RAG systems faces significant challenges stemming from the semantic \\ngap between user queries and knowledge base content. This process involves complex natural \\nlanguage understanding and generation tasks, requiring sophisticate models to capture nuanced \\nsemantic relationships. \\nA key challenge is handling biased or loaded queries while maintaining objectivity. For instance, \\na query like \"Why are vaccines harmful?\" contains a biased premise that the system must recognize \\nand neutralize to ensure balanced information retrieval. Developing methods to detect and mitigate \\nsuch biases without completely disregarding user intent remains an open problem. Another significant \\nchallenge lies in adapting queries to specific domains or temporal contexts. User queries often contain \\ndomain-specific jargon or references to current events that require specialized knowledge to interpret \\ncorrectly. Balancing the need for domain expertise with general language understanding is a complex \\ntask that current systems struggle to achieve consistently. \\nThe temporal aspect of queries presents its own set of challenges. Queries implicitly referencing \\ncurrent events or time-sensitive information require the reformulation process to incorporate temporal \\ncontext. Striking the right balance between current relevance and historical context is particularly \\ndifficult and requires sophisticated temporal reasoning capabilities [13]. \\n3.3. Latency \\nThe core challenge of latency in RAG systems stems from the fundamental trade-off between \\nresponse time and result quality. In interactive applications, the system must carefully balance the \\ndepth of retrieval against the user\\'s patience threshold. This balancing act is particularly critical as the \\nretrieval depth significantly impacts both latency and result quality; deeper retrieval can provide more \\ncomprehensive results but at the cost of increased response time. \\nOne of the primary factors contributing to latency is the complexity of multi-step retrieval \\nprocesses, often necessary for handling sophisticated queries or performing multi-hop reasoning. As \\nthe number of retrieval steps increases, managing cumulative latency becomes increasingly \\nchallenging. Each additional step not only adds to the overall response time but also introduces \\npotential points of failure or inconsistency in the retrieval process. \\nThe latency challenge is further exacerbated in scenarios requiring real-time knowledge base \\nupdates. Ensuring that newly added information is immediately available for retrieval, without \\ncompromising query response times, presents a significant technical hurdle. This is particularly \\nproblematic in domains with rapidly changing information, where the relevance of retrieval results \\ncan degrade quickly if the knowledge base is not continuously updated [14].'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 4}, page_content=\"Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n136 \\n4. Solutions and Advancements in RAG Systems \\nRecent years have witnessed significant advancements in RAG systems, addressing key challenges \\nin retrieval efficiency, scalability, and knowledge integration. This section explores two innovative \\napproaches that represent the cutting edge of RAG technology. Self-RAG, introduced by Asai et al., \\nand represents a significant shift in RAG system design. It incorporates retrieval, generation, and \\nevaluation into a single framework, allowing the model to iteratively improve its own performance. \\nThis self-improving capability addresses challenges in query reformulation and result quality \\nassessment, potentially leading to more accurate and relevant responses over time [15]. \\nGraphRAG, developed by Microsoft, takes a different approach by leveraging graph structures for \\nknowledge representation. This framework uses large language models to extract structured data from \\nunstructured text, building labeled knowledge graphs to support various applications. GraphRAG's \\nuse of graph machine learning algorithms for semantic aggregation and hierarchical analysis enables \\nit to answer high-level abstract or summary questions, showcasing the potential of structured \\nknowledge in RAG systems [16]. \\nTable 1. Comparison of advanced RAG Solutions. \\nModel \\nKey feature \\nSelf-Improvement Reasoning Capability \\nSelf-RAG \\nIterative Refinement \\nHigh \\nAdaptive \\nGraphRAG Graph-based Representation \\nModerate \\nHigh \\n \\nAs illustrated in Table 1, these two approaches offer unique strengths and address different aspects \\nof RAG system design. Self-RAG focuses on iterative self-improvement and adaptive reasoning, \\nwhile GraphRAG introduces structured knowledge representation for enhanced reasoning capabilities. \\nThese advancements collectively represent significant progress in addressing the core challenges \\nof RAG systems. However, each approach also introduces new complexities and trade-offs. For \\ninstance, while GraphRAG's structured knowledge representation offers powerful reasoning \\ncapabilities, it may face challenges in domains where information is inherently unstructured or rapidly \\nchanging. Similarly, the iterative processes in Self-RAG, while powerful, may introduce additional \\ncomputational overhead. \\n5. Future Directions for RAG Systems \\nAs the explore promising future research directions for RAG systems, several key areas emerge \\nthat could significantly advance the field while remaining grounded in current technological \\ntrajectories. \\nOne promising direction is the integration of RAG systems with LLMs that have multimodal \\ncapabilities. This combination could enable RAG systems to not only retrieve and process textual \\ninformation but also understand and generate content across various modalities such as images, audio, \\nand video. For instance, a multimodal RAG system could retrieve relevant images or video clips \\nalongside textual information, providing more comprehensive and contextually rich responses. This \\nintegration could be particularly valuable in fields like medical diagnosis, where the ability to retrieve \\nand analyze both textual reports and medical imaging data could enhance diagnostic accuracy. \\nAnother exciting avenue is the exploration of dynamic knowledge graphs within RAG systems. \\nBy continuously updating and refining a structured knowledge representation based on new \\ninformation retrieved, RAG systems could develop more nuanced and up-to-date understanding of \\ncomplex topics. This approach could involve real-time fact-checking and information validation \\nmechanisms, potentially mitigating the spread of misinformation and ensuring the reliability of \\ngenerated content. The dynamic nature of these knowledge graphs could also allow RAG systems to \\nadapt more quickly to emerging topics and changing information landscapes. \\nA third promising direction is the integration of RAG systems with robotics and embodied AI. \\nThis combination could lead to robots that not only interact with their environment but also leverage\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 5}, page_content=\"Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n137 \\nvast knowledge bases to make informed decisions and provide rich, contextual information. For \\nexample, a RAG-enhanced robot in a manufacturing setting could access and apply complex technical \\nknowledge in real-time, improving problem-solving capabilities and adaptability. In healthcare, robot \\nassistants equipped with RAG systems could provide personalized care by combining real-time \\npatient data with comprehensive medical knowledge. Furthermore, in educational settings, RAG-\\npowered robotic tutors could offer personalized learning experiences by dynamically retrieving and \\npresenting information tailored to each student's needs and learning style. This fusion of RAG \\ntechnology with robotics could significantly enhance the physical world interaction capabilities of AI \\nsystems, opening up new possibilities in fields ranging from space exploration to disaster response, \\nwhere quick access to vast amounts of relevant information could be crucial for mission success and \\nsafety. \\n6. Conclusion \\nThis paper conducts a comprehensive analysis of recent advancements in RAG systems, with a \\nparticular emphasis on key challenges and solutions such as scalability, query reformulation, latency, \\nand RAG system architectures. This work examines innovative approaches such as RETRO's \\ncapability in handling massive datasets, Self-RAG's adaptive query reformulation, and GraphRAG's \\nstructured knowledge representation. The analysis reveals that while significant progress has been \\nmade across each domain, trade-offs remain prevalent. Enhancements in scalability often come at the \\ncost of increased computational complexity. For instance, while RETRO demonstrates its superiority \\non large-scale datasets, the computational overhead and energy consumption cannot be overlooked. \\nMeanwhile, advanced query reformulation techniques like Self-RAG show promise in tackling \\ncomplex queries but may introduce additional latency, posing a challenge for real-time application \\nscenarios. Additionally, the structured knowledge representation methods employed by GraphRAG \\nenhance reasoning capabilities but still face hurdles when confronted with unstructured or rapidly \\nevolving information. \\nTo address these issues, future research can explore multiple directions. Firstly, integrating \\ncutting-edge hardware architectures with optimization algorithms to reduce computational \\ncomplexity can improve the overall performance of systems. Secondly, developing more efficient \\nquery reformulation algorithms to minimize latency ensures that systems maintain real-time \\nresponsiveness even when dealing with intricate problems. Moreover, building dynamic models for \\nboth structured and unstructured information will be a crucial area of research, aiming to increase the \\nflexibility and adaptability of knowledge representation. \\nReference \\n[1] Lewis, Patrick, Scott Reed, Jack Urbanek, and Nando de Freitas. Retrieval-augmented generation for \\nknowledge-intensive NLP tasks. Advances in Neural Information Processing Systems 33 2020: 9459-\\n9474. \\n[2] Borgeaud, Sebastian, Arthur Mensch, Guillaume Lample, and Marc'Aurelio Ranzato. Improving language \\nmodels by retrieving from trillions of tokens. International conference on machine learning. PMLR, 2022. \\n[3] Izacard, Gautier, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. \\nJournal of Machine Learning Research 24.251 2023: 1-43. \\n[4] Karpukhin, Vladimir, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Alexander Kolesnikov, and \\nSebastian Ruder. Dense passage retrieval for open-domain question answering. arXiv preprint \\narXiv:2004.04906 2020. \\n[5] GAO, Luyu, Wei-Sheng Chin, Yu-Chia Chen, and Cho-Jui Hsieh. Complement lexical retrieval model \\nwith semantic residual embeddings. Advances in Information Retrieval: 43rd European Conference on IR \\nResearch, ECIR 2021, Virtual Event, March 28-April 1, 2021, Part I 43.  \\n[6] Izacard, Gautier, and Edouard Grave. Leveraging passage retrieval with generative models for open \\ndomain question answering. arXiv preprint arXiv:2007.01282 2020.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 6}, page_content='Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n138 \\n[7] Shuster, Kurt, Alexander M. Rush, and Jason Weston. Retrieval augmentation reduces hallucination in \\nconversation. arXiv preprint arXiv:2104.07567 2021. \\n[8] Xu, Yichong, Xiaojun Wan, Xiaoyan Zhu, and Xipeng Qiu. Fusing context into knowledge graph for \\ncommonsense question answering. arXiv preprint arXiv:2012.04808 2020. \\n[9] Wang, Kexin, Zhenzhong Lan, and Jianfeng Gao. GPL: Generative pseudo labeling for unsupervised \\ndomain adaptation of dense retrieval. arXiv preprint arXiv:2112.07577 2021. \\n[10] Shibata, Tetsutaro. Asymptotics of solution curves of Kirchhoff type elliptic equations with logarithmic \\nKirchhoff function. Qualitative Theory of Dynamical Systems 22.2 2023: 64. \\n[11] Peng, Dehua, Zhipeng Gui, and Huayi Wu. Interpreting the curse of dimensionality from distance \\nconcentration and manifold effect. arXiv preprint arXiv:2401.00422 2023. \\n[12] Hassantabar, Shayan, Zeyu Wang, and Niraj K. Jha. SCANN: Synthesis of compact and accurate neural \\nnetworks. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 41.9 2021: \\n3012-3025. \\n[13] Dhingra, Bhuwan, and Graham Neubig. Time-aware language models as temporal knowledge bases. \\nTransactions of the Association for Computational Linguistics 10 2022: 257-273. \\n[14] GAO, Yunfan, Zhenzhong LAN, and Jianfeng GAO. Retrieval-augmented generation for large language \\nmodels: A survey. ArXiv preprint arXiv: 2312.10997 2023. \\n[15] Asai, Akari, and Masaaki Komachi. Self-rag: Learning to retrieve, generate, and critique through self-\\nreflection. arXiv preprint arXiv:2310.11511 2023. \\n[16] Edge, Darren, and Peter J. Stuckey. From local to global: A graph rag approach to query-focused \\nsummarization. arXiv preprint arXiv:2404.16130 2024.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 0}, page_content='Research on Tensor Flow with a system for large-scale machine \\nlearning \\nHafiz Nur \\nGenovasi University College, Malaysia \\nGoogle Brain \\nAbstract \\nTensorFlow is a machine learning system that operates at \\nlarge \\nscale \\nand \\nin \\nheterogeneous \\nenvironments. \\nTensorFlow \\nuses \\ndataflow \\ngraphs \\nto \\nrepresent \\ncomputation, shared state, and the operations that mutate \\nthat state. It maps the nodes of a dataflow graph across \\nmany machines in a cluster, and within a machine across \\nmultiple computational devices, including multicore CPUs, \\ngeneralpurpose GPUs, and custom designed ASICs known as \\nTensor Processing Units (TPUs). This architecture gives \\nflexibility to the application developer: whereas in previous \\n“parameter server” designs the management of shared \\nstate is built into the system, TensorFlow enables \\ndevelopers to experiment with novel optimizations and \\ntraining algorithms. TensorFlow supports a variety of \\napplications, with particularly strong support for training \\nand inference on deep neural networks. Several Google \\nservices use TensorFlow in production, we have released it \\nas an open-source project, and it has become widely used \\nfor machine learning research. In this paper, we describe \\nthe TensorFlow dataflow model in contrast to existing \\nsystems, and demonstrate the compelling performance \\nthat \\nTensorFlow \\nachieves \\nfor \\nseveral \\nreal-world \\napplications. \\n1 Introduction \\nIn recent years, machine learning has driven advances in \\nmany different fields [3, 5, 23, 24, 30, 27, 40, 45, 48, 50, 55, \\n68, 69, 73, 76]. We attribute this success to the invention of \\nmore sophisticated machine learning models [42, 51], the \\n                                                                \\n \\navailability of large datasets for tackling problems in these \\nfields [10, 65], and the development of software platforms \\nthat enable the easy use \\nof large amounts of computational resources for training \\nsuch models on these large datasets [14, 21]. \\nWe introduce the TensorFlow system\\n1\\n for \\nexperimenting with new models, training them on \\nlarge datasets, and moving them into production. We \\nhave based TensorFlow on years of experience with \\nour first-generation system, DistBelief [21], both \\nsimplifying and generalizing it to enable researchers to \\nexplore a wider variety of ideas with relative ease. \\nTensorFlow supports both large-scale training and \\ninference: it efficiently uses hundreds of powerful \\n(GPU-enabled) servers for fast training, and it runs \\ntrained models for inference in production on various \\nplatforms, ranging from large distributed clusters in a \\ndatacenter, down to performing inference locally on \\nmobile devices. At the same time, it is flexible and \\ngeneral enough to support experimentation and \\nresearch into new machine learning models and \\nsystem-level optimizations. \\nTensorFlow uses a unified dataflow graph to \\nrepresent both the computation in an algorithm and \\nthe state on which the algorithm operates. We draw \\ninspiration from the high-level programming models \\nof dataflow systems [2, 22, 75], and the low-level \\nefficiency of parameter servers [14, 21, 46]. Unlike \\ntraditional dataflow systems, in which graph vertices \\nrepresent functional computation on immutable data, \\nTensorFlow allows vertices to represent computations \\nthat own or update mutable state. Edges carry tensors \\n(multi-dimensional arrays) between nodes, and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 1}, page_content='2 \\nTensorFlow transparently inserts the appropriate \\ncommunication \\nbetween \\ndistributed \\nsubcomputations. By unifying the computation and \\nstate management in a single programming model, \\nTensorFlow allows programmers to experiment with \\ndifferent parallelization schemes that, for example, \\noffload computation onto the servers that hold the \\nshared state to reduce the amount of network traffic. \\nWe have also built various coordination protocols, and \\nachieved encouraging results with synchronous \\nreplication, echoing recent results [11, 19] that \\ncontradict \\nthe \\ncommonly \\nheld \\nbelief \\nthat \\nasynchronous replication is required for scalable \\nlearning [14, 21, 46]. \\nOver the past year, more than 60 teams at Google have \\nused TensorFlow, and we have released the system as an \\nopen-source project. Thanks to our large community of \\nusers we have gained experience with many different \\nmachine learning applications. In this paper, we focus on \\nneural network training as a challenging systems problem, \\nand select two representative applications from this space: \\nimage classification and language modeling. These \\napplications \\nstress \\ncomputational \\nthroughput \\nand \\naggregate model size respectively, and we use them both to \\ndemonstrate the extensibility of TensorFlow, and to \\nevaluate the efficiency and scalability of our present \\nimplementation. \\n2 Background & Motivation \\nTo make the case for developing TensorFlow, we start by \\noutlining the requirements for a large-scale machine \\nlearning system (§2.1), then consider how related work \\nmeets or does not meet those requirements (§2.2). \\n2.1 Requirements \\nDistributed execution A cluster of powerful computers can \\nsolve many machine learning problems more efficiently, \\nusing more data and larger models. \\nMachine learning algorithms generally perform better \\nwith more training data. For example, recent breakthroughs \\nin image classification models have benefited from the \\npublic ImageNet dataset, which contains 136 gigabytes of \\ndigital images [65]; and language modeling has benefited \\nfrom efforts like the One Billion Word Benchmark [10]. The \\nscale of these datasets motivates a dataparallel approach \\nto training: a distributed file system holds the data, and a \\nset of workers processes different subsets of data in \\nparallel. Data-parallelism eliminates the I/O bottleneck for \\ninput data, and any preprocessing operations can be \\napplied to input records independently. \\nEffective learned models for image recognition, language \\nmodeling, document clustering, and many other problems \\nhave a large number of parameters. For example, the \\ncurrent state-of-the-art image classification model, ResNet, \\nuses 2.3 million floating-point parameters to classify images \\ninto one of 1000 categories [26]. The One Billion Word \\nBenchmark has a vocabulary of 800,000 words, and it has \\nbeen used to train language models with 1.04 billion \\nparameters [39]. A distributed system can shard the model \\nacross many processes, to increase the available network \\nbandwidth when many workers are simultaneously reading \\nand updating the model. \\nA distributed system for model training must use \\nthe network efficiently. Many scalable algorithms \\ntrain a model using mini-batch gradient descent [21, \\n47], where a worker reads the current version of the \\nmodel and a small batch of input examples, calculates \\nan update to the model that reduces a loss function \\non those examples, and applies the update to the \\nmodel. Mini-batch methods are most effective when \\neach worker uses the most current model as a starting \\npoint, which requires a large amount of data to be \\ntransferred to the worker with low latency. \\nAccelerator support Machine learning algorithms \\noften perform expensive computations, such as matrix \\nmultiplication and multi-dimensional convolution, \\nwhich are highly parallelizable, but have many data \\ndependencies \\nthat \\nrequire \\na \\ntightly \\ncoupled \\nimplementation. The recent availability of general-\\npurpose GPUs has provided a large number of cores \\nthat can operate on fast local memory. For example, a \\nsingle NVIDIA Titan X GPU card has 6 TFLOPS peak \\nperformance [60]. In 2012, state-ofthe-art results for \\ndifferent image classification tasks were achieved \\nusing 16,000 CPU cores for three days [45], and using \\ntwo GPUs for six days [42]. Since then, GPU vendors \\nhave innovated in their support for machine learning: \\nNVIDIA’s cuDNN library [13] for GPU-based neural \\nnetwork training accelerates several popular image \\nmodels by 2–4× when using version R4 in place of R2 \\n[15]. \\nIn addition to general-purpose devices, many \\nspecialpurpose accelerators for deep learning have'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 2}, page_content='3 \\nachieved significant performance improvements and \\npower savings. At Google, our colleagues have built \\nthe Tensor Processing Unit (TPU) specifically for \\nmachine learning, and it achieves an order of \\nmagnitude improvement in performance-per-watt \\ncompared to alternative state-of-the-art technology \\n[38]. The Movidius Deep Learning Accelerator uses a \\nlow-power Myriad 2 processor with custom vector \\nprocessing units that accelerate many machine \\nlearning and computer vision algorithms [53]. \\nOvtcharov \\net \\nal. \\nhave \\nachieved \\nsignificant \\nperformance improvements and power savings for \\nsome convolutional models using field programmable \\ngate arrays (FPGAs) [58]. Since it is difficult to predict \\nthe next popular architecture for executing machine \\nlearning algorithms, we require that TensorFlow uses \\na portable programming model that can target a \\ngeneric device abstraction, and allows its operations \\nto be specialized for new architectures as they \\nemerge. \\nTraining & inference support In addition to training, \\nscalable and high-performance inference is a \\nrequirement for using models in production [18]. \\nDepending on the nature of the application, the \\ninference may be required to produce results with \\nvery low latency in an interactive service, or execute \\non a disconnected mobile device. If the model is large, \\nit might require multiple servers to participate in each \\ninference computation, and thus require distributed \\ncomputation support. Developers benefit when they \\ncan use the same code to define a model for both \\ntraining and inference. Training and inference demand \\nsimilar performance, so we prefer a common \\nwelloptimized system for both computations. Since \\ninference can be computationally intensive (e.g., an \\nimage classification model might perform 5 billion \\nFLOPS per image [70]), it must be possible to \\naccelerate it with GPUs. \\nExtensibility Single-machine machine learning frameworks \\n[36, 2, 17] have extensible programming models that enable \\ntheir users to advance the state of the art with new \\napproaches, such as adversarial learning [25] and deep \\nreinforcement learning [51]. We seek a system that \\nprovides the same ability to experiment, and also allows \\nusers to scale up the same code to run in production. The \\nsystem must support expressive control-flow and stateful \\nconstructs, while also satisfying our other requirements. \\n2.2 Related work \\nSingle-machine frameworks Many machine learning \\nresearchers carry out their work on a single—often \\nGPUequipped—computer [41, 42], and many flexible \\nsinglemachine frameworks have emerged to support this \\nscenario. Caffe [36] is a high-performance framework for \\ntraining declaratively specified convolutional neural \\nnetworks that runs on multicore CPUs and GPUs. Theano [2] \\nallows programmers to express a model as a dataflow \\ngraph, and generates efficient compiled code for training \\nthat model. Torch [17] has an imperative programming \\nmodel for scientific computation (including machine \\nlearning) that supports fine-grained control over the order \\nof execution and memory utilization. \\nWhile these frameworks do not satisfy our requirement \\nfor distributed execution, TensorFlow’s programming \\nmodel is close to Theano’s dataflow representation \\n(§3). \\nBatch dataflow systems Starting with MapReduce [22], \\nbatch dataflow systems have been applied to a large \\nnumber of machine learning algorithms [71], and more \\nrecent systems have focused on increasing expressivity and \\nperformance. DryadLINQ [74] adds a high-level query \\nlanguage that supports more sophisticated algorithms than \\nMapReduce. Spark [75] extends DryadLINQ with the ability \\nto cache previously computed datasets in memory, and is \\ntherefore better suited to iterative machine learning \\nalgorithms (such as k-means clustering and logistic \\nregression) when the input data fit in memory. Dandelion \\nextends DryadLINQ to support generating code for GPUs \\n[63] and FPGAs [16]. \\nThe principal limitation of a batch dataflow system \\nis that it requires the input data to be immutable, and \\nall of the subcomputations to be deterministic, so that \\nthe system can re-execute subcomputations when \\nmachines in the cluster fail. This feature—which is \\nbeneficial for many conventional workloads—makes \\nupdating a machine learning model a heavy operation. \\nFor example, the SparkNet system for training deep \\nneural networks on Spark takes 20 seconds to \\nbroadcast weights and collect updates from five \\nworkers [52]. As a result, these systems must process \\nlarger batches in each model update step, which slows'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 3}, page_content='4 \\nconvergence [9]. We show in Subsection 6.3 that \\nTensorFlow can train larger models on larger clusters \\nwith step times as short as 2 seconds. \\nWhile not a batch dataflow system, Naiad [54] \\naugments a dataflow model with streaming execution, \\nstateful vertices, and structured timestamps (“timely \\ndataflow”) that enable it to handle incremental \\nupdates and iterative algorithms in the same \\ncomputation. Naiad represents iteration using cyclic \\ndataflow graphs, which together with mutable state \\nmake it possible to implement algorithms that require \\nmillisecond-scale latencies for coordination. Naiad is \\ndesigned for computing on sparse, discrete data, and \\ndoes not support GPU (or any other form of) \\nacceleration, but we borrow aspects of timely \\ndataflow iteration in Subsection 3.4. \\nParameter servers Inspired by work on distributed \\nkey-value stores, a parameter server architecture uses \\na set of servers to manage shared state that is updated \\nby a set of data-parallel workers. Unlike a standard \\nkey-value store, the write operation in a parameter \\nserver is specialized for parameter updates: it is \\ntypically an associative and commutative combiner, \\nlike addition-assignment (+=), that is applied to the \\ncurrent parameter value and the incoming update to \\nproduce a new parameter value. \\nParameter servers emerged as an architecture for \\nscalable topic modeling [66], and our previous system \\nDistBelief [21] showed how a similar architecture \\ncould be applied to deep neural network training. \\nProject Adam [14] demonstrated an efficient \\nparameter \\nserver \\narchitecture \\nfor \\ntraining \\nconvolutional neural networks, and Li et al.’s \\n“Parameter Server” [46] added innovations in \\nconsistency models, fault tolerance, and elastic \\nrescaling. Despite earlier skepticism that parameter \\nservers would be compatible with GPU acceleration \\n[14], Cui et al. have recently shown that GeePS [19], a \\nparameter server specialized for use with GPUs, can \\nachieve speedups on modest-sized clusters. \\nMXNet [12] is a recent system that uses a parameter \\nserver to scale training, supports GPU acceleration, and \\nincludes a flexible programming model with interfaces for \\nmany languages. While MXNet partially fulfills our \\nextensibility requirements, the parameter server is \\n“privileged” code, which makes it difficult for researchers to \\ncustomize the handling of large models (§4.2). \\nThe parameter server architecture meets most of our \\nrequirements, and our DistBelief [21] uses parameter \\nservers with a Caffe-like model definition format [36] to \\ngreat effect. We found this architecture to be insufficiently \\nextensible, because adding a new optimization algorithm, \\nor \\nexperimenting \\nwith \\nan \\nunconventional \\nmodel \\narchitecture would require our users to modify the \\nparameter server implementation, which uses C++ for \\nperformance. While some of the practitioners who use that \\nsystem are comfortable with making these changes, the \\nmajority are accustomed to writing models in high-level \\nlanguages, such as Python and Lua, and the complexity of \\nthe highperformance parameter server implementation is a \\nbarrier to entry. With TensorFlow we therefore sought a \\nhighlevel programming model that allows users to \\ncustomize the code that runs in all parts of the system (§3). \\n3 TensorFlow execution model \\nTensorFlow uses a single dataflow graph to represent all \\ncomputation and state in a machine learning algorithm, \\nincluding the individual mathematical operations, the \\nparameters and their update rules, and the input \\npreprocessing \\n(Figure \\n1). \\nDataflow \\nmakes \\nthe \\ncommunication between subcomputations explicit, and \\ntherefore makes it easy to execute independent \\ncomputations in parallel, and partition the computation \\nacross multiple distributed devices. Dataflow TensorFlow \\ndiffers from batch dataflow systems (§2.2) in two respects: \\n• The model supports multiple concurrent executions on \\noverlapping subgraphs of the overall graph. \\n• Individual vertices may have mutable state that can be \\nshared between different executions of the graph. \\nThe key observation in the parameter server architecture \\n[21, 14, 46] is that mutable state is crucial when training \\nvery large models, because it becomes possible to make in-\\nplace updates to very large parameters, and propagate \\nthose updates to parallel training steps as quickly as \\npossible. Dataflow with mutable state enables TensorFlow \\nto mimic the functionality of a parameter server, but with \\nadditional flexibility, because it becomes possible to \\nexecute arbitrary dataflow subgraphs on the machines that \\nhost the shared model parameters. As a result, our users \\nhave been able to experiment with different optimization \\nalgorithms, consistency schemes, and parallelization \\nstrategies.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 4}, page_content='5 \\n3.1 Dataflow graph elements \\nIn a TensorFlow graph, each vertex represents an \\natomic unit of computation, and each edge represents \\nthe output from or input to a vertex. We refer to the \\ncomputation at vertices as operations, and the values \\nthat flow along edges as tensors, because TensorFlow \\nis designed for mathematical computation, and uses \\ntensors (or multidimensional arrays) to represent all \\ndata in those computations. \\nTensors In TensorFlow, we model all data as tensors \\n(dense n-dimensional arrays) with each element \\nhaving one of a small number of primitive types, such \\nas int32, float32, or string. Tensors naturally represent \\nthe inputs to and results of the common mathematical \\noperations in many machine learning algorithms: for \\nexample, a matrix multiplication takes two 2-D tensors \\nand produces a 2-D tensor; and a mini-batch 2-D \\nconvolution takes two 4-D tensors and produces \\nanother 4-D tensor. \\nAll tensors in TensorFlow are dense. This decision \\nensures that the lowest levels of the system can have \\nsimple implementations for memory allocation and \\nserialization, which reduces the overhead imposed by \\nthe framework. To represent sparse tensors, \\nTensorFlow offers two alternatives: either encode the \\ndata into variable-length string elements of a dense \\ntensor, or use a tuple of dense tensors (e.g., an n-D \\nsparse tensor with m non-zero elements could be \\nrepresented an m×n index matrix and a length-m \\nvalue vector). The size of a tensor can vary in one or \\nmore dimensions, making it possible to represent \\nsparse tensors with differing numbers of elements, at \\nthe cost of more sophisticated shape inference. \\nOperations An operation takes m ≥ 0 tensors as input, \\nand produces n ≥ 0 tensors as output. An operation \\nhas a named “type” (such as Const, MatMul, or Assign) \\nand may have zero or more compile-time attributes \\nthat determine its behavior. An operation can be \\ngeneric and variadic at compile-time: its attributes \\ndetermine both the expected types and arity of its \\ninputs and outputs. preprocessing, training, and \\ncheckpointing state. \\nFor example, the simplest operation Const has no inputs \\nand a single output. Const has an attribute T that \\ndetermines the type of its output, and an attribute Value \\nthat determines the value that it produces. AddN is variadic: \\nit has a type attribute T, and an integer attribute N that \\ndefines how many inputs (of type T) it accepts. \\nStateful operations: variables An operation can contain \\nmutable state that is read and/or written each time it \\nexecutes. A Variable operation owns a mutable buffer that \\nis used to store the shared parameters of a model as it is \\ntrained. A Variable has no inputs, and produces a reference \\nhandle, which acts as a typed capability for reading and \\nwriting the buffer. A Read operation takes a reference \\nhandle as input, and outputs the value of the variable as a \\ndense tensor. Several operations can modify the underlying \\nbuffer: for example, AssignAdd takes a reference handle r \\nand a tensor value x, and when executed performs the \\nupdate State0[r] ← State[r]+x. Subsequent Read(r) \\noperations produce the value State0[r]. \\nStateful operations: queues TensorFlow includes several \\nqueue implementations, which support more advanced \\nforms of coordination. The simplest queue is FIFOQueue, \\nwhich owns an internal queue of tensors, and supports \\nconcurrent access. Like a Variable, the FIFOQueue \\noperation produces a reference handle that can be \\nconsumed by one of the standard queue operations, such \\nas Enqueue and Dequeue. These operations respectively \\npush their input onto the tail of the queue, or pop the head \\nelement and output it. Enqueue will block if its given queue \\nis full, and Dequeue will block if its given queue is empty. \\nWhen queues are used in an input preprocessing pipeline, \\nthis blocking provides backpressure; it also supports \\nsynchronization (§4.4). \\n \\nFigure 1: A schematic TensorFlow dataflow graph for a training pipeline contains subgraphs for reading input data,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 5}, page_content='6 \\n3.2 Partial and concurrent execution \\nTensorFlow uses the dataflow graph to represent all \\npossible computations in a particular application, and the \\nAPI for executing a graph allows the client to specify the \\nsubgraph that should be executed. A subgraph is specified \\ndeclaratively: the client selects zero or more edges to feed \\ninput tensors into the dataflow, and one or more edges to \\nfetch output tensors from the dataflow; the runtime then \\nprunes the graph to contain the necessary set of operations. \\nEach invocation of the API is called a step, and TensorFlow \\nsupports multiple concurrent steps on the same graph, \\nwhere stateful operations enable coordination between the \\nsteps. \\nFigure 1 shows a typical training application, with \\nmultiple subgraphs that execute concurrently, and \\ninteract through shared variables and queues. The \\ncore training subgraph depends on a set of model \\nparameters, and input batches from a queue. Many \\nconcurrent steps of the training subgraph update the \\nmodel based on different input batches, to implement \\ndata-parallel training. To fill the input queue, \\nconcurrent preprocessing steps transform individual \\ninput records (e.g., decoding images and applying \\nrandom distortions), and a separate I/O subgraph \\nreads records from a distributed file system. A \\ncheckpointing subgraph runs periodically for fault \\ntolerance (§4.3). \\nPartial and concurrent execution is responsible for \\nmuch of TensorFlow’s flexibility. Adding mutable state \\nand coordination via queues makes it possible to \\nspecify a wide variety of model architectures in \\n“unprivileged” code, which enables advanced users to \\nexperiment without modifying the internals of the \\nTensorFlow runtime. \\n3.3 Distributed execution \\nDataflow simplifies distributed execution, because it \\nmakes communication between subcomputations \\nexplicit. In principle, the same TensorFlow program \\ncan be deployed to a distributed cluster of GPUs for \\ntraining, a cluster of TPUs for serving, and a cellphone \\nfor mobile inference. \\nEach operation resides on a particular device, such \\nas a CPU or GPU in a particular task. A device is \\nresponsible for executing a kernel for each operation \\nassigned to it. \\nTensorFlow allows multiple kernels to be registered for a \\nsingle operation, with specialized implementations for a \\nparticular device or data type (see §5 for details). For many \\noperations, such as element-wise operators (Add, Sub, \\netc.), we use a single kernel implementation that can be \\ncompiled for CPU and GPU using different compilers. \\nThe TensorFlow runtime places operations on devices, \\nsubject to implicit or explicit device constraints in the graph. \\nThe placement algorithm computes a feasible set of devices \\nfor each operation, calculates the sets of operations that \\nmust be colocated, and selects a satisfying device for each \\ncolocation group. Stateful operations and operations their \\nstate must be placed on the same device, which leads to \\nimplicit colocation constraints. In addition, the user may \\nspecify partial device preferences such as “any device in a \\nparticular task”, or “a GPU in any task”, and the runtime will \\nrespect these constraints. A typical training application will \\nuse client-side programming constructs to add constraints \\nsuch that, for example, parameters are distributed among a \\nset of “PS” tasks. \\nOnce the operations in a graph have been placed, and the \\npartial subgraph has been computed for a step (§3.2), \\nTensorFlow partitions the operations into per-device \\nsubgraphs. A per-device subgraph for device d contains all \\nof the operations that were assigned to d, with additional \\nSend and Recv operations that replace edges across device \\nboundaries. Send transmits its single input to a specified \\ndevice as soon as the tensor is available, using a rendezvous \\nkey to name the value. Recv has a single output, and blocks \\nuntil the value for a specified rendezvous key is available \\nlocally, before producing that value. Send and Recv have \\nspecialized implementations for several device-type pairs; \\nwe describe some of these in Section 5. \\nWe optimized TensorFlow for executing large subgraphs \\nrepeatedly with low latency. Once the graph for a step has \\nbeen pruned, placed, and partitioned, its subgraphs are \\ncached in their respective devices. A client session \\nmaintains the mapping from step definitions to cached \\nsubgraphs, so that a distributed step on a large graph can \\nbe initiated with one small message to each participating \\ntask. This model favors static, reusable graphs, but it can \\nsupport dynamic computations using dynamic control flow, \\nas the next subsection describes. \\n3.4 Dynamic control flow \\nMost evaluation in TensorFlow is strict: all inputs to an \\noperation must be computed before the operation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 6}, page_content='7 \\nexecutes. Advanced algorithms—such as efficiently training \\na recurrent neural network [37]—require dynamic control \\nflow, which for efficiency requires non-strict evaluation. \\n \\nFigure 2: A conditional graph using Switch and Merge \\nTensorFlow supports conditional control flow using \\nthe primitive Switch and Merge operations, which are \\nbased on Arvind and Culler’s original dynamic \\ndataflow architectures [4]. Switch acts like a \\ndemultiplexer: it takes a data input and a control \\ninput, and uses the control input to select which of its \\ntwo outputs should produce a value. The Switch \\noutput not taken receives a special dead value, which \\npropagates recursively through the rest of the graph \\nuntil it reaches a Merge operation. Merge acts like a \\nmultiplexer: it forwards at most one non-dead input \\nto its output, or produces a dead output if both of its \\ninputs are dead. We use these primitives to build a \\nnonstrict conditional subgraph (Figure 2) that \\nexecutes one of two branches, based on the runtime \\nvalue of a tensor. \\nSwitch and Merge also support iteration. The \\nimplementation of loops in TensorFlow is based on \\nSwitch and Merge [4], with additional structural \\nconstraints based on timely dataflow [54] to simplify \\nthe distributed execution state. Like timely dataflow, \\nTensorFlow supports multiple concurrent iterations \\nand nested loops, but simplifies memory management \\nby restricting each operation to producing a single \\nvalue per output per iteration. \\n4 Extensibility case studies \\nBy choosing a unified dataflow graph to represent all \\ncomputation in TensorFlow, we have enabled users to \\nexperiment with features that were built into the \\nruntime of our previous system [21]. In this section, \\nwe discuss four extensions to TensorFlow that we \\nhave built using simple dataflow primitives and “user-\\nlevel” code. \\n4.1 Differentiation and optimization \\nMany learning algorithms train a set of parameters \\nusing some variant of stochastic gradient descent \\n(SGD), which entails computing the gradients of a cost \\nfunction with respect to those parameters, then \\nupdating the parameters based on those gradients. \\nWe implement a user-level library for TensorFlow that \\nautomatically differentiates expressions. A user can, \\nfor example, define a neural network as a composition \\nof layers and a loss function, and the library will derive \\nthe backpropagation [64]. \\nThe differentiation algorithm performs breadth-first \\nsearch to identify all of the backwards paths from the target \\noperation (e.g., a loss function) to a set of parameters, and \\nsums the partial gradients that each path contributes. Our \\nusers frequently specialize the gradients for some \\noperations, and they have implemented optimizations like \\nbatch normalization [32] and gradient clipping [59] to \\naccelerate training and make it more robust. We have \\nextended the algorithm to differentiate conditional and \\niterative \\nsubcomputations \\n(§3.4), \\nand \\ndeveloped \\ntechniques for managing GPU memory when iterating (and \\naccumulating intermediate values) over long sequences in \\nthe input data (similar to GeePS [19]). \\nTensorFlow users can also experiment with a wide range \\nof optimization algorithms, which compute new values for \\nthe parameters in each training step. SGD is easy to \\nimplement in a parameter server: for each parameter W, \\ngradient ∂L/∂W, and learning rate α, the update rule is W0 \\n← W − α × ∂L/∂W. A parameter server can implement SGD \\nby using -= as the write operation, and writing α × ∂L/∂W \\nto each W after a training step. \\nHowever, there are many more advanced optimization \\nschemes that are difficult to express as a single write \\noperation. For example, the Momentum algorithm \\naccumulates a “velocity” for each parameter based on its \\ngradient over multiple iterations, then computes the \\nparameter update from that accumulation; and many \\nrefinements to this algorithm have been proposed [67]. To \\nimplement Momentum in DistBelief [21], we had to modify \\nthe C++ code of the parameter server to change the \\nrepresentation of parameter data, and execute arbitrary \\ncode in the write operation; such modifications are beyond \\nthe majority of our users. Optimization algorithms are the \\nT r u e   b r a n c h \\nF a l s e   b r a n c h \\nS w i t c h \\nM e r g e \\nT r u e'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 7}, page_content='8 \\ntopic of active research, and our users have implemented \\nseveral on top of TensorFlow, including Momentum, \\nAdagrad, Adadelta, RMSProp, Adam, and L-BFGS. These can \\nbe built in TensorFlow using Variable operations and \\nprimitive mathematical operations without needing to \\nmodify the underlying system, which makes it easy to \\nexperiment with new algorithms as they emerge. \\n4.2 Handling very large models \\nTo train a model on high-dimensional data, such as words in \\na corpus of text [7], it is common to use a distributed \\nrepresentation, which embeds a training example as a \\npattern of activity across several neurons, which can be \\n \\nFigure 3: Schematic dataflow graph for a sparse \\nembedding layer containing a two-way sharded \\nembedding matrix. \\nlearned by backpropagation [29]. For example, in a \\nlanguage model, a training example might be a sparse \\nvector with non-zero entries corresponding to the IDs \\nof words in a vocabulary, and the distributed \\nrepresentation for each word will be a lower-\\ndimensional vector [6]. \\nInference proceeds by multiplying a batch of b \\nsparse vectors against an n×d embedding matrix, \\nwhere n is the number of words in the vocabulary, and \\nd is the desired dimensionality, to produce a much \\nsmaller b × d dense matrix representation; for \\ntraining, most optimization algorithms modify only \\nthe rows of the embedding matrix that were read by \\nthe sparse multiplication. In many TensorFlow models \\nthat process sparse data, n×d can amount to gigabytes \\nof parameters: e.g., a large language model may use \\nover 109 parameters with a vocabulary of 800,000 \\nwords [39], and we have experience with document \\nmodels [20] where the parameters occupy several \\nterabytes. Such models are too large to copy to a \\nworker on every use, or even to store in RAM on a \\nsingle host. \\nWe implement sparse embedding layers in the \\nTensorFlow graph as a composition of primitive \\noperations. Figure 3 shows a simplified graph for an \\nembedding layer that is split across two parameter \\nserver tasks. The core operation of this subgraph is \\nGather, which extracts a sparse set of rows from a \\ntensor, and TensorFlow colocates this operation with \\nthe variable on which it operates. The dynamic \\npartition (Part) operation divides the incoming indices \\ninto variable-sized tensors that contain the indices \\ndestined for each shard, and the dynamic static \\n(Stitch) operation reassembles the partial results from \\neach shard into a single result tensor. Each of these \\noperations has a corresponding gradient, so it \\nsupports automatic differentiation (§4.1), and the \\nresult is a set of sparse update operations that act on \\njust the values that were originally gathered from each \\nof the shards. \\nWhile sparse reads and updates are possible in a \\nparameter server [46], TensorFlow adds the flexibility \\nto offload arbitrary computation onto the devices that \\nhost \\nthe \\nshared \\nparameters. \\nFor \\nexample, \\nclassification models typically use a softmax classifier \\nthat multiplies the final output by a weight matrix with \\nc columns, where c is the number of possible classes; \\nfor a language model, c is the size of the vocabulary, \\nwhich can be large. Our users have experimented with \\nseveral schemes to accelerate the softmax calculation. \\nThe first is similar to an optimization in Project Adam \\n[14], whereby the weights are sharded across several \\ntasks, and the multiplication and gradient calculation \\nare colocated with the shards. More efficient training \\nis possible using a sampled softmax [35], which \\nperforms a sparse multiplication based on the true \\nclass for an example and a set of randomly sampled \\nfalse classes. We compare the performance of these \\ntwo schemes in §6.4. \\n4.3 Fault tolerance \\nTraining a model can take several hours or days, even using \\na large number of machines [21, 14]. It is desirable to be \\nable to train a model using non-dedicated resources, for \\nexample using a cluster manager, like Mesos [28] or Borg \\n[72], that does not guarantee availability of the same \\nresources for the duration of the training process.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 8}, page_content='9 \\nTherefore, a TensorFlow job is likely to experience failure \\nduring the training process, and we require some form of \\nfault tolerance. However, failures are unlikely to be so \\ncommon that individual operations need fault tolerance, so \\na mechanism like Spark’s RDDs [75] would impose \\nsignificant overhead for little benefit. There is no need to \\nmake every write to the parameter state durable, because \\nwe can recompute any update from the input data, and \\nmany learning algorithms do not require strong consistency \\n[62]. Although we do not use strong consistency for the \\ntraining state, we rely on a system like Chubby [8] or \\nZooKeeper [31] to map task IDs to IP addresses. \\nWe implement user-level checkpointing for fault \\ntolerance in TensorFlow, using primitive operations in the \\ngraph (Figure 1): Save writes one or more tensors to a \\ncheckpoint file, and Restore reads one or more tensors from \\na checkpoint file. Our typical configuration connects each \\nVariable in a task to the same Save operation, with one Save \\nper task, to maximize the I/O bandwidth to a distributed file \\nsystem. The Restore operations read named tensors from a \\nfile, and a standard Assign stores the restored value in its \\nrespective variable. During training, a typical client runs all \\nof the Save operations periodically to produce a new \\ncheckpoint; when the client starts up, it attempts to Restore \\nthe latest checkpoint. \\nTensorFlow includes a client library for constructing the \\nappropriate graph structure, and invoking Save and Restore \\nas necessary. This behavior is customizable: the user can \\napply different policies to subsets of the variables in a \\nmodel, or customize the checkpoint retention scheme. For \\nexample, many users retain checkpoints with the highest \\nscore in a custom evaluation metric. The implementation is \\nalso reusable: it may be used for model fine-tuning and \\nunsupervised pre-training [43, 45], which are forms of \\ntransfer learning, in which the parameters of a model \\ntrained on one task (e.g. recognizing general images) are \\nused as the starting point for another task (e.g. recognizing \\nparticular breeds of dog). Having checkpoint and parameter \\nmanagement as programmable operations in the graph \\ngives users the flexibility to implement schemes like these \\nand others that we have not anticipated. \\nThe checkpointing library does not attempt to \\nproduce consistent checkpoints: if training and \\ncheckpointing execute concurrently, the checkpoint \\nmay include none, all, or some of the updates from the \\ntraining step. This is no problem for models that we \\ntrain by asynchronous gradient descent [21]. \\nConsistent \\ncheckpoints \\nrequire \\nadditional \\nsynchronization to ensure that checkpointing does not \\nrun concurrently with update operations. For \\nexample, one can use the scheme in next subsection \\nto take a checkpoint after the synchronous update \\nstep. \\n4.4 Synchronous replica coordination \\nSGD is robust to asynchrony [62], and previous \\nsystems \\ntrain \\ndeep \\nneural \\nnetworks \\nusing \\nasynchronous parameter updates [21, 14], which are \\nbelieved scalable because they maintain high \\nthroughput in the presence of stragglers. The \\nincreased throughput comes at the cost of training \\nsteps using stale data. Some have recently revisited \\nthe assumption that synchronous training does not \\nscale [11, 19]. Since GPUs enable training with \\nhundreds—rather than thousands [45]—of machines, \\nit may be possible to train a model synchronously in \\nless time than asynchronous training on the same \\nmachines. \\nThough we designed TensorFlow for asynchronous \\ntraining, we have begun experimenting with \\nsynchronous methods. The TensorFlow graph enables \\nusers to change how parameters are read and written \\nwhen training a model, and we implement three \\nalternatives. In the asynchronous case (Figure 4(a)), \\neach worker reads the current value when the step \\nbegins, and applies its gradient to the different current \\nvalue at the end: this ensures high utilization, but the \\nindividual steps use stale information, making each \\nstep less effective. The synchronous cases use queues \\n(§3.1) to coordinate execution: a blocking queue acts \\nas a barrier to ensure that all workers read the same \\nparameter version, and a second queue accumulates \\nmul-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 9}, page_content='10 \\nFigure 5: The layered TensorFlow architecture. \\ntiple gradient updates in order to apply them atomically. \\nThe simple synchronous version (Figure 4(b)) accumulates \\nupdates from all workers before applying them, but slow \\nworkers limit overall throughput. \\nTo mitigate stragglers, we implement backup workers \\n(Figure 4(c), [11]), which are similar to MapReduce backup \\ntasks [22]. Whereas MapReduce starts backup tasks \\nreactively—after \\ndetecting \\na \\nstraggler—our \\nbackup \\nworkers run proactively, and the aggregation takes the first \\nm of n updates produced. We exploit the fact that SGD \\nsamples training data randomly, so each worker processes \\na different random batch. In Subsection 6.3 we show how \\nbackup workers improve throughput by up to 15%. \\n5 Implementation \\nWe implement TensorFlow as an extensible, crossplatform \\nlibrary. Figure 5 illustrates the system architecture: a thin C \\nAPI separates user-level in various languages from the core \\nlibrary. In this section, we discuss the implementation of the \\nvarious components. \\nThe core TensorFlow library is implemented in C++ for \\nportability and performance: it runs on several operating \\nsystems including Linux, Mac OS X, Android, and iOS; the \\nx86 and various ARM-based CPU architectures; and \\nNVIDIA’s Kepler, Maxwell, and Pascal GPU \\nmicroarchitectures. The implementation is open-source, \\nand we have accepted several external contributions that \\nenable TensorFlow to run on other architectures. \\nThe distributed master translates user requests into \\nexecution across a set of tasks. Given a graph and a \\nstep definition, it prunes (§3.2) and partitions (§3.3) \\nthe graph to obtain subgraphs for each participating \\ndevice, and caches these subgraphs so that they may \\nbe re-used in subsequent steps. Since the master sees \\nthe overall computation for a step, it applies standard \\noptimizations \\nsuch \\nas \\ncommon \\nsubexpression \\nelimination and constant folding; pruning is a form of \\ndead code elimination. It then coordinates execution \\nof the optimized subgraphs across a set of tasks. \\nThe dataflow executor in each task handles \\nrequests from the master, and schedules the \\nexecution of the kernels that comprise a local \\nsubgraph. We optimize the dataflow executor for \\nrunning large, fine-grained graphs with low overhead; \\nour current implementation dispatches approximately \\n2,000,000 null operations per second. The dataflow \\nexecutor dispatches kernels to local devices and runs \\nkernels in parallel when possible: e.g., by using \\nmultiple cores in a CPU device, or multiple streams on \\na GPU. \\nThe runtime contains over 200 standard operations, \\nincluding mathematical, array manipulation, control \\nflow, and state management operations. Many of the \\noperation \\nkernels \\nare \\nimplemented \\nusing \\nEigen::Tensor [34], which uses C++ templates to \\ngenerate efficient parallel code for multicore CPUs \\nand GPUs; however, we liberally use libraries like \\ncuDNN [13] to implement kernels where a more \\nefficient specialization is possible. We have also \\nimplemented support for quantization, which enables \\nfaster inference in environments such as mobile \\ndevices and high-throughput datacenter applications, \\nand use the gemmlowp low-precision matrix \\nmultiplication library [33] to accelerate quantized \\ncomputation. \\nWe specialize Send and Recv operations for each pair of \\nsource and destination device types. Transfers between \\nlocal CPU and GPU devices use the cudaMemcpyAsync() API \\nto overlap computation and data transfer; transfers \\nbetween two local GPUs use DMA to relieve pressure on the \\nhost. For transfers between tasks, TensorFlow supports \\n \\nFigure 4: Three parameter synchronization schemes for a single parameter in data-parallel training (§4.4): (a) \\nasynchronous, (b) synchronous without backup workers, and (c) synchronous with backup workers.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 10}, page_content='11 \\nmultiple protocols, including gRPC over TCP, and RDMA \\nover Converged Ethernet. We are also investigating \\noptimizations for GPU-to-GPU communication that use \\ncollective operations [57]. \\nSection 4 describes features that we implement totally \\nabove the C API, in user-level code. Typically, users compose \\nstandard operations to build higher-level abstractions, such \\nas neural network layers, optimization algorithms (§4.1), \\nand sharded embedding computations (§4.2). TensorFlow \\nsupports multiple client languages, and we have prioritized \\nsupport for Python and C++, because our internal users are \\nmost familiar with these languages. As features become \\nmore established, we typically port them to C++, so that \\nusers can access an optimized implementation from all \\nclient languages. \\nIf it is difficult or inefficient to represent a \\nsubcomputation as a composition of operations, users can \\nregister additional kernels that provide an efficient \\nimplementation written in C++. We have found it profitable \\nto hand-implement fused kernels for some performance \\ncritical operations, such as a the ReLU and Sigmoid \\nactivation functions and their corresponding gradients. We \\nare currently investigating automatic kernel fusion using \\nHalide [61] and other compiler-based techniques. \\nIn addition to the core runtime, our colleagues have built \\nseveral tools that aid users of TensorFlow. These include \\nserving infrastructure for running inference in production, a \\nvisualization dashboard that enables users to follow the \\nprogress of a training run, a graph visualizer that helps users \\nto understand the connections in a model, and a distributed \\nprofiler that traces the execution of a computation across \\nmultiple devices and tasks. We describe these tools in an \\nextended whitepaper [1], and they can be downloaded \\nfrom the project repository. \\n6 Evaluation \\nIn this section, we evaluate the performance of TensorFlow \\non several synthetic and realistic workloads. Unless \\notherwise stated, we run all experiments on a shared \\nproduction cluster, and all figures plot median values with \\nerror bars showing the 10th and 90th percentiles. \\nHere we focus on system performance metrics, rather \\nthan learning objectives like time to accuracy. TensorFlow is \\na system that allows machine learning practitioners and \\nresearchers to experiment with new techniques, and this \\nevaluation demonstrates that the system (i) has little \\noverhead, and (ii) can employ large amounts of \\ncomputation to accelerate real-world applications. While \\ntechniques like synchronous replication can enable some \\nmodels to converge in fewer steps overall, we defer the \\nanalysis of such improvements to other papers. \\n6.1 Single-machine benchmarks \\nAlthough TensorFlow is a system for “large-scale” \\nmachine learning, it is imperative that scalability does \\nnot mask poor performance at small scales [49]. Table \\n1 contains results from Chintala’s independent \\nbenchmark of convolutional models on TensorFlow \\nand three singlemachine frameworks [15]. All \\nframeworks use a six-core Intel Core i7-5930K CPU at \\n3.5GHz and an NVIDIA Titan X GPU. \\nLibrary \\nTraining step time (ms) \\nAlexNet \\nOverfeat \\nOxfordNet GoogleNet \\nCaffe [36] \\n324 \\n823 \\n1068 \\n1935 \\nNeon [56] \\n87 \\n211 \\n320 \\n270 \\nTorch [17] \\n81 \\n268 \\n529 \\n470 \\nTensorFlow \\n81 \\n279 \\n540 \\n445 \\nTable 1: Step times for training four convolutional \\nmodels with different libraries, using one GPU. All \\nresults are for training with 32-bit floats. The fastest \\nlibrary for each model is shown in bold. \\nTable 1 shows that TensorFlow achieves shorter \\nstep times than Caffe [36], and performance within 6% \\nof the latest version of Torch [17]. We attribute the \\nsimilar performance of TensorFlow and Torch to the \\nfact that both use the same version of the cuDNN \\nlibrary [13], which implements the convolution and \\npooling operations on the critical path for training; \\nCaffe uses open-source implementations for these \\noperations that are simpler but less efficient than \\ncuDNN. \\nThe \\nNeon \\nlibrary \\n[56] \\noutperforms \\nTensorFlow on three of the models, by using \\nhandoptimized \\nconvolutional \\nkernels \\n[44] \\nimplemented in assembly language; in principle, we \\ncould implement these kernels in TensorFlow, but we \\nhave not yet done so. \\n6.2 Synchronous replica microbenchmark \\nThe performance of our coordination implementation \\n(§4.4) is the main limiting factor for scaling with \\nadditional machines. Figure 6 shows that number of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 11}, page_content='12 \\nnull training steps that TensorFlow performs per \\nsecond for varying model sizes, and increasing \\nnumbers of synchronous workers. In a null training \\nstep, a worker fetches the \\n \\nFigure 6: Baseline throughput for synchronous replication \\nwith a null model. Sparse accesses enable TensorFlow to \\nhandle larger models, such as embedding matrices (§4.2). \\nshared model parameters from 16 PS tasks, performs a \\ntrivial computation, and sends updates to the parameters. \\nThe Scalar curve in Figure 6 shows the best performance \\nthat we could expect for a synchronous training step, \\nbecause only a single 4-byte value is fetched from each PS \\ntask. The median step time is 1.8ms using a single worker, \\ngrowing to 8.8ms with 100 workers. These times measure \\nthe overhead of the synchronization mechanism, and \\ncapture some of the noise that we expect when running on \\na shared cluster. \\nThe Dense curves show the performance of a null step \\nwhen the worker fetches the entire model. We repeat the \\nexperiment with models of size 100MB and 1GB, with the \\nparameters sharded equally over 16 PS tasks. The median \\nstep time for 100MB increases from 147ms with one worker \\nto 613ms with 100 workers. For 1GB, it increases from 1.01s \\nwith one worker to 7.16s with 100 workers. \\nFor large models, it is typical that a training step accesses \\nonly a subset of the parameters, and the Sparse curves \\nshow the throughput of the embedding lookup operation \\nfrom Subsection 4.2. Each worker reads 32 randomly \\nselected entries from a large embedding matrix containing \\n1GB or 16GB of data. As expected, the step times do not \\nvary with the size of the embedding, and TensorFlow \\nachieves step times ranging from 5 to 20ms. \\n6.3 Image classification \\nDeep neural networks have achieved breakthrough \\nperformance on computer vision tasks such as recognizing \\nobjects in photographs [42], and these tasks are a key \\napplication for TensorFlow at Google. Training a network to \\nhigh accuracy requires a large amount of computation, and \\nwe use TensorFlow to scale out the computation across a \\ncluster of GPU-enabled servers. In these experiments, we \\nfocus on Google’s Inception-v3 model, which achieves \\n78.8% accuracy the ILSVRC 2012 image classification \\nchallenge [70]; the same techniques apply to other deep \\nconvolutional models—such as Microsoft’s ResNet [26]—\\nthat TensorFlow users have implemented. We investigate \\nthe scalability of training the Inception-v3 model using \\nmultiple replicas. We configure a TensorFlow job with 17 PS \\ntasks, and vary the number of worker tasks. Each worker \\ntask has one NVIDIA K40 GPU and 5 IvyBridge cores, and a \\nPS task has 8 IvyBridge cores. We investigate the effect of \\ncoordination (§4.4) on training performance, using up to \\n200 workers to validate recent promising results for \\nsynchronous training [11, 19]. In particular, if synchronous \\ntraining can be made efficient, a model such as Inception-\\nV3 will train in fewer steps, and converge to a higher \\naccuracy than with asynchronous training [11]. \\nTraining throughput improves to 2,300 images per \\nsecond as we increase the number of workers to 200, \\nbut with diminishing returns (Figure 7(a)). Figures 7(b) \\nand (c) explain the limits to scaling: as we add more \\nworkers, the step time increases, because there is \\nmore contention on the PS tasks, both at the network \\ninterface and in the aggregation of updates. As \\nexpected, for all configurations, synchronous steps \\nare longer than asynchronous steps, because all \\nworkers must wait for the slowest worker to catch up \\nbefore starting the next step. While the median \\nsynchronous step is approximately 10% longer than an \\nasynchronous step with the same workers, above the \\n90th percentile the synchronous performance \\ndegrades \\nsharply, \\nbecause \\nstragglers \\ndisproportionately impact the tail. \\nTo mitigate tail latency, we can add backup workers, \\nso that a step completes when the first m of n tasks \\nproduce gradients. Figure 8 shows the effect on step \\ntime of adding backup workers to a 50-worker \\nInception training job. Each additional backup worker \\nup to and including the fourth reduces the median \\nstep time, because the probability of a straggler'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 12}, page_content='13 \\naffecting the step decreases. Adding a fifth backup \\nworker slightly degrades performance, because the \\n51st worker (i.e., the first whose result is discarded) is \\nmore likely to be a non-straggler that generates more \\nincoming traffic for the PS tasks. Figure 8 also plots the \\nnormalized speedup for each configuration, which we \\ndefine as t(b)/t(0)×50/(50+b) (where t(b) is the \\nmedian step time with b backup workers), and which \\ndiscounts the speedup by the fraction of additional \\nresources consumed. Although adding 4 backup \\nworkers achieves the shortest overall step time \\n(1.93s), adding 3 achieves the highest normalized \\nspeedup (9.5%), and hence trains the model to the \\nsame quality using less aggregate GPU-time. \\nFigure 8: Backup workers reduce the step time for 50worker \\nInception-v3 training. 4 backup workers give the shortest \\noverall step time, but 3 backup workers are most efficient \\nwhen we normalize for the total resources used. \\n6.4 Language modeling \\nGiven a sequence of words, a language model predicts the \\nmost probable next word [6]. Therefore, language models \\nare integral to predictive text, speech recognition, and \\ntranslation applications. In this experiment, we investigate \\nhow TensorFlow can train a recurrent neural network (viz. \\nLSTM-512-512 [39]) to model the text in the One Billion \\nWord Benchmark [10]. The vocabulary size |V | limits the \\nperformance of training, because the final layer must \\ndecode the output state into probabilities for each of |V | \\nclasses [35]. The resulting parameters can be large (|V |×d \\nfor output state dimension d) so we use the techniques for \\nhandling large models from Subsection 4.2. We use a \\nrestricted vocabulary of the most common 40,000 words—\\ninstead of the full 800,000 words [10]—in order to \\nexperiment with smaller configurations. \\nFigure 9 shows the training throughput, measured in \\nFigure 9: Increasing the number of PS tasks leads to \\nincreased throughput for language model training, by \\nparallelizing the softmax computation. Sampled softmax \\nincreases throughput by performing less computation. \\nwords per second, for varying numbers of PS and \\nworker tasks, and two softmax implementations. The \\nfull softmax (dashed lines) multiplies each output by a \\n \\nFigure 7: (a) Inception-v3 training throughput increases with up to 200 workers. However, adding more workers \\ngets diminishing returns because the step time increases for both (b) asynchronous and (c) synchronous \\nreplication. \\n \\n0 \\n1 \\n2 \\n3 \\n4 \\n5 \\nNumber of backup workers \\n1.9 \\n2.0 \\n2.1 \\n2.2 \\n2.3 \\n2.4 \\n2.5 \\nStep time \\n1.00 \\n1.02 \\n1.04 \\n1.06 \\n1.08 \\nSpeedup'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 13}, page_content='14 \\n512 × 40,000 weight matrix sharded across the PS \\ntasks. Adding more PS tasks increases the throughput, \\nbecause TensorFlow can exploit distributed model \\nparallelism [21, 41] and perform the multiplication \\nand gradient calculation on the PS tasks, as in Project \\nAdam [14]. Adding a second PS task is more effective \\nthan increasing from 4 to 32, or 32 to 256 workers. \\nEventually the throughput saturates, as the LSTM \\ncalculations dominate the training step. \\nThe sampled softmax (solid lines) reduces the data \\ntransferred and the computation performed at the PS \\ntasks [35]. Instead of a dense weight matrix, it \\nmultiplies the output by a random sparse matrix \\ncontaining weights for the true class and a random \\nsample of false classes. We sample 512 classes for \\neach batch, which reduces the softmax data transfer \\nand computation by a factor of 78. \\n7 Conclusions \\nWe have described the TensorFlow system and its \\nextensible dataflow-based programming model. The core \\nidea of this paper is that TensorFlow’s dataflow \\nrepresentation subsumes existing work on parameter \\nserver systems, and offers a uniform programming model \\nthat allows users to harness large-scale heterogeneous \\nsystems, both for production tasks and for experimenting \\nwith new approaches. We have shown several examples of \\nhow the TensorFlow programming model supports \\nexperimentation (§4) and demonstrated that the resulting \\nimplementations are performant and scalable (§6). \\nOur initial experience with TensorFlow is encouraging. A \\nlarge number of groups at Google have deployed \\nTensorFlow in production, and TensorFlow is helping our \\nresearch colleagues to make new advances in machine \\nlearning. Since we released TensorFlow as open-source \\nsoftware, over 8,000 people have forked the source code \\nrepository, the binary distribution has been downloaded \\n500,000 times, and our users have published dozens of \\nmachine learning models that use TensorFlow. \\nTensorFlow is a work in progress. Its flexible dataflow \\nrepresentation enables power users to achieve excellent \\nperformance, but we have not yet determined default \\npolicies that work well for most users. Further research on \\nautomatic optimization should bridge this gap. On the \\nsystem level, we are actively developing algorithms for \\nautomatic placement, kernel fusion, memory management, \\nand scheduling. While the current implementations of \\nmutable state and fault tolerance suffice for applications \\nwith weak consistency requirements, we expect that some \\nTensorFlow applications will require stronger consistency, \\nand we are investigating how to build such policies at user-\\nlevel. Finally, our users are demanding, and some have \\nbegun to chafe at the limitations of a static dataflow graph, \\nespecially for algorithms like deep reinforcement learning \\n[51]. Therefore, we face the intriguing problem of providing \\na system that transparently and efficiently uses distributed \\nresources, even when the structure of the computation \\nunfolds dynamically. \\nBy sharing the implementation of TensorFlow and \\nengaging with the research community, we hope that this \\nwork will spur further research in distributed systems and \\nmachine learning. \\nAcknowledgments \\nWe gratefully acknowledge contributions from our \\ncolleagues within Google, and from members of the wider \\nmachine learning community. In particular, we appreciate \\nthe feedback we have received both from the rest of the \\nGoogle Brain team and the hundreds of DistBelief and \\nTensorFlow users that has helped us improve the usability \\nof functionality of the system. \\nMany individuals have contributed to TensorFlow, \\nincluding: John Giannandrea (for creating a supportive \\nresearch environment); Irina Kofman, Amy McDonald \\nSandjideh, and Phing Turner (project management); \\nAshish Agarwal, Dave Andersen, Anelia Angelova, \\nEugene Brevdo, Yaroslav Bulatov, Jerjou Cheng, \\nMaciek Chociej, Craig Citro, Greg Corrado, George \\nDahl, Andrew Dai, Lucy Gao, mig Gerard, Ian \\nGoodfellow, Stephan Gouws, Gunhan Gulsoy, Steinar \\nGunderson, Andrew Harp, Peter Hawkins, Yangqing \\nJia, Rafal Jozefowicz, Łukasz Kaiser, Naveen Kumar, \\nGeoffrey Hinton, Mrinal Kalakrishnan, Anjuli Kannan, \\nRasmus Larsen, \\nYutaka Leon-Suematsu, Frank Li, Peter Liu, Xiaobing \\nLiu, Olivia Nordquist, Chris Olah, Nishant Patil, \\nSaurabh Saxena, Mike Schuster, Andrew Selle, Pierre \\nSermanet, Noam Shazeer, Jonathon Shlens, Jascha \\nSohl-Dickstein, Ilya Sutskever, Kunal Talwar, Philip \\nTucker, Vincent Vanhoucke, Oriol Vinyals, Chad \\nWhipkey, Yonghui Wu, Ke Yang, Zongheng Yang, and \\nYao Zhang (general contributions to the project); Shan \\nCarter, Doug Fritz, Patrick Hurst, Dilip Krishnan, Dan \\nMane, Daniel Smilkov, Fer-´ nanda Viegas, Martin \\nWattenberg, James Wexler, Jimbo´ Wilson, Kanit \\nWongsuphasawat, Cassandra Xia, and the Big Picture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 14}, page_content='15 \\nteam (visualization); Chris Leary, Robert Hundt, \\nRobert Springer, Cliff Young, and the Stream Executor \\nteam (accelerator support); Norm Jouppi and the \\nteam that created the Tensor Processing Unit; Kayur \\nPatel, Michael Piatek, and the coLab team; and the \\ngrowing community of open-source contributors and \\nusers who have helped make TensorFlow better. \\nReferences \\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, \\nZ. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, \\nM. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, \\nG. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,´ \\nM. Kudlur, J. Levenberg, D. Mane, R. Monga, S. \\nMoore, D. G. Murray, C. Olah, M. Schuster, J. \\nShlens, B. Steiner, I. Sutskever, K. Talwar, P. A. \\nTucker, V. Vanhoucke, V. Vasudevan, F. B. \\nViegas, O. Vinyals, P. Warden, M. Watten-´ berg, \\nM. Wicke, Y. Yu, and X. Zheng. Tensorflow: Large-\\nscale machine learning on heterogeneous \\ndistributed systems. CoRR, abs/1603.04467, \\n2016. \\narxiv.org/abs/1603.04467. \\nSoftware \\navailable from tensorflow.org. \\n[2] R. Al-Rfou, G. Alain, A. Almahairi, C. Angermueller, D. \\nBahdanau, N. Ballas, F. Bastien, J. Bayer, A. Belikov, A. \\nBelopolsky, Y. Bengio, A. Bergeron, J. Bergstra, V. \\nBisson, J. Bleecher Snyder, N. Bouchard, N. Boulanger-\\nLewandowski, X. Bouthillier, A. de Brebisson, O. \\nBreuleux, P.-´ L. Carrier, K. Cho, J. Chorowski, P. \\nChristiano, T. Cooijmans, M.-A. Cotˆ e, M. C´ otˆ e, A. \\nCourville,´ Y. N. Dauphin, O. Delalleau, J. Demouth, G. \\nDesjardins, S. Dieleman, L. Dinh, M. Ducoffe, V. \\nDumoulin, S. Ebrahimi Kahou, D. Erhan, Z. Fan, O. \\nFirat, M. Germain, X. Glorot, I. Goodfellow, M. \\nGraham, C. Gulcehre, P. Hamel, I. Harlouchet, J.-P. \\nHeng, B. Hidasi, S. Honari, A. Jain, S. Jean, K. Jia, M. \\nKorobov, V. Kulkarni, A. Lamb, P. Lamblin, E. Larsen, C. \\nLaurent, S. Lee, S. Lefrancois, S. Lemieux, N. Leonard, \\nZ. Lin, J. A. Livezey,´ C. Lorenz, J. Lowin, Q. Ma, P.-A. \\nManzagol, O. Mastropietro, R. T. McGibbon, R. \\nMemisevic, B. van Merrienboer, V. Michalski, M. \\nMirza,¨ A. Orlandi, C. Pal, R. Pascanu, M. Pezeshki, C. \\nRaffel, D. Renshaw, M. Rocklin, A. Romero, M. Roth, P. \\nSadowski, J. Salvatier, F. Savard, J. Schluter, J. \\nSchulman, G. Schwartz, I. V. Serban,¨ D. Serdyuk, S. \\nShabanian, E. Simon, S. Spieckermann, S. R. \\nSubramanyam, J. Sygnowski, J. Tanguay, G. van Tulder, \\nJ. Turian, S. Urban, P. Vincent, F. Visin, H. de Vries, D. \\nWarde-Farley, D. J. Webb, M. Willson, K. Xu, L. Xue, L. \\nYao, S. Zhang, and Y. Zhang. Theano: A Python \\nframework for fast computation of mathematical \\nexpressions. arXiv e-prints, abs/1605.02688, May \\n2016. arxiv.org/abs/1605.02688. \\n[3] A. Angelova, A. Krizhevsky, and V. Vanhoucke. \\nPedestrian detection with a large-field-of-view deep \\nnetwork. In Robotics and Automation (ICRA), 2015 \\nIEEE International Conference on, pages 704–711. \\nIEEE, 2015. CalTech PDF. \\n[4] Arvind and D. E. Culler. Annual review of computer \\nscience vol. 1, 1986. chapter Dataflow Architectures, \\npages \\n225–253. \\n1986. \\nwww.dtic.mil/cgi-\\nbin/GetTRDoc?Location=U2& \\ndoc=GetTRDoc.pdf&AD=ADA166235. \\n[5] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object \\nrecognition with visual attention. arXiv preprint \\narXiv:1412.7755, 2014. arxiv.org/abs/1412.7755. \\n[6] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A \\nneural probabilistic language model. \\nJournal of Machine Learning Research, 3:1137– \\n1155, \\n2003. \\nwww.iro.umontreal.ca/˜lisa/pointeurs/ \\nBengioDucharmeVincentJauvin jmlr.pdf. \\n[7] T. Brants and A. Franz. Web 1T 5-gram version 1, 2006. \\ncatalog.ldc.upenn.edu/LDC2006T13. \\n[8] M. \\nBurrows. \\nThe \\nChubby \\nlock \\nservice \\nfor \\nlooselycoupled distributed systems. In Proceedings of \\nthe 7th Symposium on Operating Systems Design and \\nImplementation, OSDI ’06, pages 335–350, Berkeley, \\nCA, \\nUSA, \\n2006. \\nUSENIX \\nAssociation. \\nwww.usenix.org/legacy/event/osdi06/tech/full \\npapers/burrows/burrows.pdf. \\n[9] R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu. Sample \\nsize selection in optimization methods for machine \\nlearning. Mathematical Programming, 134(1):127–\\n155, 2012. dx.doi.org/10.1007/s10107012-0572-5. \\n[10] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, \\nand P. Koehn. One billion word benchmark for \\nmeasuring progress in statistical language modeling. \\nCoRR, abs/1312.3005, 2013. arxiv.org/abs/1312.3005.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 15}, page_content='16 \\n[11] J. Chen, R. Monga, S. Bengio, and R. Jozefowicz. \\nRevisiting \\ndistributed \\nsynchronous \\nSGD. \\nIn \\nInternational Conference on Learning Representations \\nWorkshop Track, 2016. arxiv.org/abs/1604.00981. \\n[12] T. Chen, \\nM. Li, \\nY. Li, \\nM. Lin, N. Wang, \\nM. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. \\nMXNet: A flexible and efficient machine learning \\nlibrary for heterogeneous distributed systems. In \\nProceedings of the Workshop on Machine \\nLearning \\nSystems \\nat \\nNeural \\nInformation \\nProcessing Systems (LearningSys), Dec. 2015. \\nwww.cs.cmu.edu/ muli/file/mxnet-learning-sys.pdf. \\n[13] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. \\nTran, B. Catanzaro, and E. Shelhamer. cuDNN: Efficient \\nprimitives \\nfor \\ndeep \\nlearning. \\narXiv \\npreprint \\narXiv:1410.0759, 2014. arxiv.org/abs/1410.0759. \\n[14] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. \\nProject Adam: Building an efficient and scalable deep \\nlearning training system. In 11th USENIX Symposium \\non Operating Systems Design and Implementation \\n(OSDI 14), pages 571–582, 2014. \\nwww.usenix.org/system/files/conference/osdi14/ \\nosdi14-paper-chilimbi.pdf. \\n[15] S. Chintala. convnet-benchmarks, \\n2016. \\ngithub.com/soumith/convnet-benchmarks. \\n[16] E. S. Chung, J. D. Davis, and J. Lee. LINQits: Big data on \\nlittle clients. In Proceedings of the 40th Annual \\nInternational Symposium on Computer Architecture, \\nISCA ’13, pages 261–272, New York, NY, USA, 2013. \\nACM. doi.acm.org/10.1145/2485922.2485945. \\n[17] R. Collobert, S. Bengio, and J. Mariethoz.´ Torch: A \\nmodular machine learning software library. Technical \\nreport, \\nIDIAP, \\n2002. \\ninfoscience.epfl.ch/record/82802/files/rr02-46.pdf. \\n[18] D. Crankshaw, P. Bailis, J. E. Gonzalez, H. Li, Z. Zhang, \\nM. J. Franklin, A. Ghodsi, and M. I. Jordan. The missing \\npiece in complex analytics: Low latency, scalable \\nmodel management and serving with Velox. In CIDR \\n2015, Seventh Biennial Conference on Innovative Data \\nSystems Research, Asilomar, CA, USA, January 4-7, \\n2015, \\nOnline \\nProceedings, \\n2015. \\narxiv.org/abs/1409.3809. \\n[19] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and E. P. \\nXing. GeePS: Scalable deep learning on distributed \\nGPUs with a GPUspecialized parameter server. In \\nProceedings of the Eleventh European Conference on \\nComputer \\nSystems, \\nEuroSys \\n’16, \\n2016. \\nwww.pdl.cmu.edu/PDLFTP/CloudComputing/GeePS-\\ncui-eurosys16.pdf. \\n[20] A. Dai, C. Olah, and Q. V. Le. Document embedding \\nwith \\nparagraph \\nvectors. \\narXiv \\npreprint \\narXiv:1507.07998, 2015. arxiv.org/abs/1507.07998. \\n[21] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. \\nV. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. \\nYang, and A. Y. Ng. Large scale distributed deep \\nnetworks. In NIPS, 2012. Google Research PDF. \\n[22] J. Dean and S. Ghemawat. Mapreduce: Simplified data \\nprocessing on large clusters. In Proceedings of the 6th \\nConference on Symposium on Opearting Systems \\nDesign & Implementation - Volume 6, OSDI’04, \\nBerkeley, CA, USA, 2004. USENIX Association. \\nresearch.google.com/archive/mapreduceosdi04.pdf. \\n[23] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, \\nT. Mikolov, et al. DeVISE: A deep visualsemantic \\nembedding model. In Advances in Neural Information \\nProcessing \\nSystems, \\npages \\n2121–2129, \\n2013. \\nresearch.google.com/pubs/archive/41473.pdf. \\n[24] J. Gonzalez-Dominguez, I. Lopez-Moreno, P. J. \\nMoreno, and J. Gonzalez-Rodriguez. Frame-by-frame \\nlanguage identification in short utterances using deep \\nneural networks. Neural Networks, 64:49–58, 2015. \\nresearch.google.com/en//pubs/archive/42929.pdf. \\n[25] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. \\nWarde-Farley, S. Ozair, A. C. Courville, and Y. Bengio. \\nGenerative adversarial nets. In Advances in Neural \\nInformation \\nProcessing \\nSystems \\n27: \\nAnnual \\nConference on Neural Information Processing Systems \\n2014, December 8-13 2014, Montreal, Quebec, \\nCanada, \\npages \\n2672– \\n2680, \\n2014. \\npapers.nips.cc/paper/5423-generativeadversarial-\\nnets. \\n[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual \\nlearning \\nfor \\nimage \\nrecognition. \\nCoRR, \\nabs/1512.03385, 2015. arxiv.org/abs/1512.03385. \\n[27] G. Heigold, V. Vanhoucke, A. Senior, P. Nguyen, M. \\nRanzato, M. Devin, and J. Dean. Multilingual acoustic'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 16}, page_content='17 \\nmodels using distributed deep neural networks. In \\nAcoustics, Speech and Signal Processing (ICASSP), \\n2013 IEEE International Conference on, pages 8619–\\n8623. \\nIEEE, \\n2013. \\nresearch.google.com/pubs/archive/40807.pdf. \\n[28] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. \\nJoseph, R. Katz, S. Shenker, and I. Stoica. Mesos: A \\nplatform for fine-grained resource sharing in the data \\ncenter. In Proceedings of the 8th USENIX Conference \\non Networked Systems Design and Implementation, \\nNSDI’11, pages 295–308, Berkeley, CA, USA, 2011. \\nUSENIX \\nAssociation. \\nwww.cs.berkeley.edu/˜alig/papers/mesos.pdf. \\n[29] G. E. Hinton. Learning distributed representations of \\nconcepts. In Proceedings of the Eighth Annual \\nConference of the Cognitive Science Society, pages 1–\\n12. \\nHillsdale, \\nNJ: \\nErlbaum, \\n1986. \\nwww.cogsci.ucsd.edu/˜ajyu/Teaching/Cogs202 \\nsp13/Readings/hinton86.pdf. \\n[30] G. E. Hinton, \\nL. Deng, D. Yu, \\nG. E. Dahl, \\nA. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, \\nP. Nguyen, T. N. Sainath, and B. Kingsbury. Deep \\nneural networks for acoustic modeling in speech \\nrecognition: The shared views of four research \\ngroups. IEEE Signal Process. Mag., 29(6):82– 97, \\n2012. \\nwww.cs.toronto.edu/˜gdahl/papers/ \\ndeepSpeechReviewSPM2012.pdf. \\n[31] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed. \\nZooKeeper: Wait-free coordination for internetscale \\nsystems. In Proceedings of the 2010 USENIX \\nConference on USENIX Annual Technical Conference, \\nUSENIXATC’10, pages 11–11, Berkeley, CA, USA, 2010. \\nUSENIX \\nAssociation. \\nwww.usenix.org/legacy/event/atc10/tech/full \\npapers/Hunt.pdf. \\n[32] S. Ioffe and C. Szegedy. Batch normalization: \\nAccelerating deep network training by reducing \\ninternal covariate shift. CoRR, abs/1502.03167, 2015. \\narxiv.org/abs/1502.03167. \\n[33] B. Jacob et al. gemmlowp: a small selfcontained low-\\nprecision \\nGEMM \\nlibrary, \\n2015. \\ngithub.com/google/gemmlowp. \\n[34] B. Jacob, G. Guennebaud, et al. Eigen library for linear \\nalgebra. eigen.tuxfamily.org. \\n[35] S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using \\nvery large target vocabulary for neural machine \\ntranslation. In Proceedings of the 53rd Annual Meeting \\nof the Association for Computational Linguistics and \\nthe 7th International Joint Conference on Natural \\nLanguage Processing (Volume 1: Long Papers), pages \\n1–10, Beijing, China, July 2015. Association for \\nComputational \\nLinguistics. \\nwww.aclweb.org/anthology/P15-1001. \\n[36] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. \\nGirshick, S. Guadarrama, and T. Darrell. Caffe: \\nConvolutional \\narchitecture \\nfor \\nfast \\nfeature \\nembedding. In Proceedings of the ACM International \\nConference on Multimedia, pages 675–678. ACM, \\n2014. arxiv.org/pdf/1408.5093. \\n[37] M. I. Jordan. Serial order: A parallel distributed \\nprocessing approach. ICS report 8608, Institute for \\nCognitive \\nScience, \\nUCSD, \\nLa \\nJolla, \\n1986. \\ncseweb.ucsd.edu/˜gary/PAPERSUGGESTIONS/Jordan-\\nTR-8604.pdf. \\n[38] N. Jouppi. Google supercharges machine learning \\ntasks \\nwith \\nTPU \\ncustom \\nchip, \\n2016. \\ncloudplatform.googleblog.com/2016/05/Googlesupe\\nrcharges-machine-learning-tasks-with-\\ncustomchip.html. \\n[39] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer,´ and \\nY. Wu. Exploring the limits of language modeling. \\nCoRR, \\nabs/1602.02410, \\n2016. \\narxiv.org/abs/1602.02410. \\n[40] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. \\nSukthankar, and L. Fei-Fei. Large-scale video \\nclassification with convolutional neural networks. In \\nComputer Vision and Pattern Recognition (CVPR), \\n2014 IEEE Conference on, pages 1725–1732. IEEE, \\n2014. research.google.com/pubs/archive/42455.pdf. \\n[41] A. Krizhevsky. One weird trick for parallelizing \\nconvolutional \\nneural \\nnetworks. \\narXiv \\npreprint \\narXiv:1404.5997, 2014. arxiv.org/abs/1404.5997. \\n[42] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet \\nclassification \\nwith \\ndeep \\nconvolutional \\nneural \\nnetworks. In Advances in Neural Information \\nProcessing \\nSystems, \\n2012. \\npapers.nips.cc/paper/4824imagenet-classification-\\nwith-deep-convolutionalneural-networks.pdf.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 17}, page_content='18 \\n[43] H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin. \\nExploring strategies for training deep neural networks. \\nJournal of Machine Learning Research, 10:1–40, Jan. \\n2009. \\ndeeplearning.cs.cmu.edu/pdfs/1111/jmlr10 \\nlarochelle.pdf. \\n[44] A. Lavin and S. Gray. Fast algorithms for convolutional \\nneural networks. CoRR, abs/1509.09308, 2015. \\narxiv.org/abs/1509.09308. \\n[45] Q. Le, M. Ranzato, R. Monga, M. Devin, G. Corrado, K. \\nChen, J. Dean, and A. Ng. Building highlevel features \\nusing large scale unsupervised learning. In ICML’2012, \\n2012. Google Research PDF. \\n[46] M. Li, D. G. Andersen, J. Park, A. J. Smola, A. Ahmed, \\nV. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su. Scaling \\ndistributed machine learning with the Parameter \\nServer. In 11th USENIX Symposium on Operating \\nSystems Design and Implementation (OSDI 14), pages \\n583–598, \\n2014. \\nwww.usenix.org/system/files/conference/osdi14/osd\\ni14paper-chilimbi.pdf. \\n[47] M. Li, T. Zhang, Y. Chen, and A. J. Smola. \\nEfficient mini-batch training for stochastic \\noptimization. In Proceedings of the 20th ACM \\nSIGKDD International Conference on Knowledge \\nDiscovery and Data Mining, KDD ’14, pages 661–\\n670, New York, NY, USA, 2014. ACM. \\nwww.cs.cmu.edu/˜muli/file/minibatch sgd.pdf. \\n[48] C. J. Maddison, A. Huang, I. Sutskever, and D. Silver. \\nMove evaluation in Go using deep convolutional \\nneural networks. arXiv preprint arXiv:1412.6564, \\n2014. arxiv.org/abs/1412.6564. \\n[49] F. McSherry, M. Isard, and D. G. Murray. Scalability! \\nBut at what COST? In Proceedings of the 15th USENIX \\nConference on Hot Topics in Operating Systems, \\nHOTOS’15, Berkeley, CA, USA, 2015. USENIX \\nAssociation. \\nwww.usenix.org/system/files/conference/hotos15/ \\nhotos15-paper-mcsherry.pdf. \\n[50] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient \\nestimation of word representations in vector space. In \\nInternational \\nConference \\non \\nLearning \\nRepresentations: \\nWorkshops \\nTrack, \\n2013. \\narxiv.org/abs/1301.3781. \\n[51] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. \\nVeness, M. G. Bellemare, A. Graves, M. Riedmiller, A. \\nK. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. \\nSadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, \\nS. Legg, and D. Hassabis. Human-level control through \\ndeep reinforcement learning. Nature, 518(7540):529–\\n533, 02 2015. dx.doi.org/10.1038/nature14236. \\n[52] P. Moritz, R. Nishihara, I. Stoica, and M. I. Jordan. \\nSparkNet: Training deep networks in Spark. In \\nInternational \\nConference \\non \\nLearning \\nRepresentations, 2016. arxiv.org/abs/1511.06051. \\n[53] Movidius Ltd. Movidius announces Deep Learning \\nAccelerator and Fathom software framework, 2016. \\nwww.movidius.com/news/movidius-announcesdeep-\\nlearning-accelerator-and-fathom-\\nsoftwareframework. \\n[54] D. G. Murray, F. McSherry, R. Isaacs, M. Isard, P. \\nBarham, and M. Abadi. Naiad: a timely dataflow \\nsystem. In Proceedings of the Twenty-Fourth ACM \\nSymposium on Operating Systems Principles, pages \\n439–455. ACM, 2013. Microsoft Research PDF. \\n[55] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. \\nFearon, A. De Maria, V. Panneershelvam, M. \\nSuleyman, C. Beattie, S. Petersen, et al. Massively \\nparallel methods for deep reinforcement learning. \\narXiv preprint arXiv:1507.04296, 2015. \\narxiv.org/abs/1507.04296. \\n[56] Nervana \\nSystems. neon, \\n2016. \\ngithub.com/NervanaSystems/neon. \\n[57] NVIDIA Corporation. NCCL: Optimized primitives for \\ncollective \\nmulti-gpu \\ncommunication, \\n2016. \\ngithub.com/NVIDIA/nccl. \\n[58] K. Ovtcharov, O. Ruwase, J.-Y. Kim, J. Fowers, K. \\nStrauss, and E. Chung. Toward accelerating deep \\nlearning at scale using specialized logic. In Hot Chips: \\nA Symposium on High Performance Chips. HOTCHIPS, \\nAugust 2015. re- \\nsearch.microsoft.com/apps/pubs/default.aspx?id=246506. \\n[59] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty \\nof training recurrent neural networks. In ICML (3), \\nvolume 28 of JMLR'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 18}, page_content='19 \\nProceedings, pages 1310–1318. JMLR.org, 2013. \\nwww.jmlr.org/proceedings/papers/v28/pascanu13.p\\ndf. \\n[60] K. Powell. Nvidia \\ndevtech blog \\npost. \\nblogs.nvidia.com/blog/2015/03/17/digits-devbox/. \\n[61] J. Ragan-Kelley, C. Barnes, A. Adams, S. Paris, F. \\nDurand, and S. Amarasinghe. Halide: A language and \\ncompiler for optimizing parallelism, locality, and \\nrecomputation in image processing pipelines. ACM \\nSIGPLAN \\nNotices, \\n48(6):519– \\n530, \\n2013. \\npeople.csail.mit.edu/fredo/tmp/Halide5min.pdf. \\n[62] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-\\nfree approach to parallelizing stochastic gradient \\ndescent. In Advances in Neural Information Processing \\nSystems, \\npages \\n693–701, \\n2011. \\npapers.nips.cc/paper/4390-hogwild-a-lockfree-\\napproach-to-parallelizing-stochastic-gradientdescent. \\n[63] C. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. \\nFetterly. Dandelion: a compiler and runtime for \\nheterogeneous systems. In Proceedings of the Twenty-\\nFourth ACM Symposium on Operating Systems \\nPrinciples, pages 49–68. ACM, 2013. \\nresearch-\\nsrv.microsoft.com/pubs/201110/sosp13dandelion-\\nfinal.pdf. \\n[64] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. \\nLearning representations by backpropagating errors. \\nCognitive modeling, 5:3, 1988. \\nwww.cs.toronto.edu/ hinton/absps/naturebp.pdf. \\n[65] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, \\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, \\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual \\nRecognition Challenge. International Journal of \\nComputer Vision (IJCV), 115(3):211–252, 2015. \\narxiv.org/abs/1409.0575. \\n[66] A. Smola and S. Narayanamurthy. An architecture for \\nparallel topic models. Proc. \\nVLDB \\nEndow., 3(1-2):703–710, Sept. \\n2010. \\nvldb.org/pvldb/vldb2010/papers/R63.pdf. \\n[67] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. \\nOn the importance of initialization and momentum in \\ndeep learning. In Proceedings of the 30th International \\nConference on Machine Learning (ICML-13), pages \\n1139–1147. \\nJMLR \\nWorkshop \\nand \\nConference \\nProceedings, \\n2013. \\njmlr.org/proceedings/papers/v28/sutskever13.pdf. \\n[68] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to \\nsequence learning with neural networks. In NIPS, \\n2014. \\npapers.nips.cc/paper/5346-sequenceto-\\nsequence-learning-with-neural. \\n[69] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. \\nAnguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. \\nGoing deeper with convolutions. In CVPR’2015, 2015. \\narxiv.org/abs/1409.4842. \\n[70] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. \\nWojna. Rethinking the inception architecture for \\ncomputer vision. CoRR, abs/1512.00567, 2015. \\narxiv.org/abs/1512.00567. \\n[71] C. tao Chu, S. K. Kim, Y. an Lin, Y. Yu, G. Bradski, K. \\nOlukotun, and A. Y. Ng. Map-reduce for machine \\nlearning on multicore. In B. Scholkopf, J. C.¨ Platt, and \\nT. Hoffman, editors, Advances in Neural Information \\nProcessing Systems 19, pages 281–288. MIT Press, \\n2007. \\npapers.nips.cc/paper/3150-mapreduce-for-\\nmachine-learning-on-multicore.pdf. \\n[72] A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, \\nE. \\nTune, \\nand \\nJ. \\nWilkes. \\nLarge-scale \\ncluster \\nmanagement at Google with Borg. In Proceedings of \\nthe Tenth European Conference on Computer Systems, \\npage \\n18. \\nACM, \\n2015. \\nresearch.google.com/pubs/archive/43438.pdf. \\n[73] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and \\nG. Hinton. Grammar as a foreign language. Technical \\nreport, \\narXiv:1412.7449, \\n2014. \\narxiv.org/abs/1412.7449. \\n[74] Y. Yu, M. Isard, D. Fetterly, M. Budiu, U. Erlingsson, P. \\nK. Gunda, and J. Currey. DryadLINQ: A system for \\ngeneral-purpose distributed dataparallel computing \\nusing a high-level language. In Proceedings of the 8th \\nUSENIX Conference on Operating Systems Design and \\nImplementation, OSDI’08, pages 1–14, Berkeley, CA, \\nUSA, \\n2008. \\nUSENIX \\nAssociation. \\nwww.usenix.org/legacy/event/osdi08/tech/full \\npapers/yu y/yu y.pdf. \\n[75] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. \\nMcCauley, M. J. Franklin, S. Shenker, and I. Stoica.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 19}, page_content='20 \\nResilient \\ndistributed \\ndatasets: \\nA \\nfault-tolerant \\nabstraction for in-memory cluster computing. In \\nProceedings of the 9th USENIX conference on \\nNetworked Systems Design and Implementation. \\nUSENIX Association, 2012. \\nwww.usenix.org/system/files/conference/nsdi12/nsd\\ni12final138.pdf. \\n[76] M. D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, \\nQ. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean, and \\nG. E. Hinton. On rectified linear units for speech \\nprocessing. \\nIn \\nICASSP, \\n2013. \\nresearch.google.com/pubs/archive/40811.pdf. \\n[77] Dhaval Sahija, “Critical review of machine learning \\nintegration with augmented reality fordiscrete \\nmanufacturing” \\n2021, \\n10.2015/IJIRMF.2455.0620/202112017'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 0}, page_content='Scikit-Learn Made Easy: API Fast Guide\\nMohamad Yamen AL Mohamad\\nyamenmohamad@tabrizu.ac.ir\\nMay 29, 2025\\nAbstract\\nThis document provides a comprehensive reference guide to the core modules of scikit-learn,\\nthe premier machine learning library for Python. Each section introduces a module, explains\\nits purpose, and details key APIs with itemizes, use cases, and features. Visual diagrams\\nillustrate data flow and relationships between components, while practical code examples\\ndemonstrate real-world applications. The guide covers major functionalities including super-\\nvised learning (classification, regression), unsupervised learning (clustering, dimensionality\\nreduction), model selection, preprocessing, and pipeline construction. It is intended for data\\nscientists, machine learning practitioners, and developers who need a structured overview\\nof scikit-learn’s capabilities. By combining clear explanations with practical examples, this\\ndocument helps users understand and implement effective machine learning workflows using\\nscikit-learn’s consistent API design. Keywords: Scikit-learn, Machine Learning, Python,\\nClassification, Regression, Clustering, Dimensionality Reduction, Model Selection, Prepro-\\ncessing, Pipelines, Feature Extraction, Cross-Validation, Hyperparameter Tuning, Super-\\nvised Learning, Unsupervised Learning\\n1'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 1}, page_content='1\\nOverview\\nScikit-learn [1, 2] is a comprehensive Python library for machine learning that provides simple\\nand eﬀicient tools for data mining and data analysis. Built on NumPy, SciPy, and matplotlib, it\\nfeatures various classification, regression, clustering algorithms, and includes utilities for model\\nevaluation, data preprocessing, and pipeline construction.\\nThis document serves as a structured reference to scikit-learn’s core modules:\\n• Clear definitions of each module’s purpose\\n• Detailed explanations of key APIs and functions\\n• Visual diagrams illustrating concepts and workflows\\n• Practical code examples demonstrating real-world usage\\nScikit-learn’s consistent API design enables practitioners to:\\n– Quickly implement and compare different algorithms\\n– Build end-to-end machine learning pipelines\\n– Evaluate models using robust validation techniques\\n– Preprocess data eﬀiciently\\n2\\nSupervised Learning\\nDefinition 2.1. The sklearn package provides numerous algorithms for supervised learning,\\nwhere the goal is to predict target variables based on input features. These include both classi-\\nfication (predicting discrete labels) and regression (predicting continuous values) tasks.\\nLabeled Data\\nTrain/Test Split\\nClassifier/Regressor\\nEvaluation Metrics\\nFigure 1: Supervised Learning Workflow\\n2'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 2}, page_content='2.1\\nKey APIs\\n2.1.1\\nClassification\\n- LogisticRegression:\\n• Purpose: Linear model for binary and multiclass classification\\n• Use Case: When probabilities of class membership are needed\\n• Features: L1/L2 regularization, multiclass support\\n- SVC (Support Vector Classification):\\n• Purpose: Kernel-based classification\\n• Use Case: Complex decision boundaries in high-dimensional spaces\\n• Features: Multiple kernel options (linear, poly, rbf, sigmoid)\\n- RandomForestClassifier:\\n• Purpose: Ensemble of decision trees\\n• Use Case: Robust classification with feature importance\\n• Features: Handles missing values, parallel training\\nExample 2.1.\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import\\ntrain_test_split\\nfrom sklearn.metrics import accuracy_score\\n# Load dataset\\niris = load_iris()\\nX, y = iris.data, iris.target\\n# Split into train/test sets\\nX_train , X_test, y_train , y_test = train_test_split\\n(X, y, test_size=0.2)\\n# Train model\\nclf = RandomForestClassifier(n_estimators=100)\\nclf.fit(X_train , y_train)\\n# Evaluate\\ny_pred = clf.predict(X_test)\\nprint(f\"Accuracy: {accuracy_score(y_test , y_pred)\\n:.2f}\")\\n2.1.2\\nRegression\\n- LinearRegression:\\n• Purpose: Ordinary least squares regression\\n• Use Case: Linear relationships between features and target\\n• Features: Fast training, interpretable coeﬀicients\\n3'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 3}, page_content='- Ridge:\\n• Purpose: L2-regularized linear regression\\n• Use Case: When features are correlated\\n• Features: Reduces overfitting through regularization\\n- SVR (Support Vector Regression):\\n• Purpose: Kernel-based regression\\n• Use Case: Non-linear relationships\\n• Features: Robust to outliers with proper kernel choice\\nExample 2.2.\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.datasets import make_regression\\n# Generate synthetic regression data\\nX, y = make_regression(n_features=10, noise=0.1)\\n# Train model\\nridge = Ridge(alpha=1.0)\\nridge.fit(X, y)\\n# Predict and evaluate\\nscore = ridge.score(X, y)\\nprint(f\"R^2 Score: {score:.2f}\")\\n3\\nUnsupervised Learning\\nDefinition 3.1. The sklearn package provides algorithms for unsupervised learning tasks where\\nthe data has no labels. These include clustering (grouping similar data points) and dimension-\\nality reduction (reducing the number of features while preserving structure).\\nUnlabeled Data\\nPreprocessing\\nClustering/Dimensionality\\nReduction\\nLabels/Reduced Features\\nFigure 2: Unsupervised Learning Workflow\\n4'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 4}, page_content='3.1\\nKey APIs\\n3.1.1\\nClustering\\n- KMeans:\\n• Purpose: Partition data into k clusters\\n• Use Case: When number of clusters is known\\n• Features: Scalable, simple interpretation\\n- DBSCAN:\\n• Purpose: Density-based clustering\\n• Use Case: Clusters of arbitrary shape\\n• Features: Handles noise, no need to specify cluster count\\n- AgglomerativeClustering:\\n• Purpose: Hierarchical clustering\\n• Use Case: When hierarchy is important\\n• Features: Dendrogram visualization possible\\nExample 3.1.\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.datasets import make_blobs\\nimport matplotlib.pyplot as plt\\n# Generate synthetic data\\nX, _ = make_blobs(n_samples=300, centers=4,\\nrandom_state=42)\\n# Cluster data\\nkmeans = KMeans(n_clusters=4)\\nkmeans.fit(X)\\nlabels = kmeans.labels_\\n# Visualize\\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\\'\\nviridis\\')\\nplt.title(\"KMeans Clustering\")\\nplt.show()\\n5'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 5}, page_content='3.1.2\\nDimensionality Reduction\\n- PCA (Principal Component Analysis):\\n• Purpose: Linear dimensionality reduction\\n• Use Case: Feature extraction, visualization\\n• Features: Preserves variance, orthogonal components\\n- t-SNE:\\n• Purpose: Non-linear dimensionality reduction\\n• Use Case: Visualization of high-dimensional data\\n• Features: Preserves local structure\\n- TruncatedSVD:\\n• Purpose: Dimensionality reduction for sparse matrices\\n• Use Case: Text data, recommendation systems\\n• Features: Works with sparse input\\nExample 3.2.\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.datasets import load_digits\\n# Load digit images\\ndigits = load_digits()\\nX = digits.data\\n# Apply PCA\\npca = PCA(n_components=2)\\nX_pca = pca.fit_transform(X)\\n# Plot\\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.\\ntarget, cmap=\\'tab10\\', alpha=0.6)\\nplt.title(\"Digits Projected onto First 2 Principal\\nComponents\")\\nplt.xlabel(\"PC1\")\\nplt.ylabel(\"PC2\")\\nplt.colorbar(label=\"Digit Class\")\\nplt.show()\\n6'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 6}, page_content='4\\nModel Selection\\nDefinition 4.1. The sklearn.model_selection module provides tools for evaluating models\\nand selecting hyperparameters, including cross-validation strategies and hyperparameter search\\nmethods.\\nData\\nCV Splitter\\nParameter Search\\nBest Model\\nFigure 3: Model Selection Workflow\\n4.1\\nKey APIs\\n- train_test_split:\\n• Purpose: Split data into train and test sets\\n• Use Case: Simple model evaluation\\n• Features: Stratification option for classification\\n- KFold:\\n• Purpose: K-fold cross-validation\\n• Use Case: Robust model evaluation\\n• Features: Shuffle option, stratification\\n- GridSearchCV:\\n• Purpose: Exhaustive hyperparameter search\\n• Use Case: When parameter space is small\\n• Features: Parallel computation, refitting\\n- RandomizedSearchCV:\\n• Purpose: Randomized hyperparameter search\\n• Use Case: Large parameter spaces\\n• Features: More eﬀicient than grid search\\n7'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 7}, page_content='Example 4.1.\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import SVC\\nfrom sklearn.datasets import load_iris\\n# Load data\\niris = load_iris()\\nX, y = iris.data, iris.target\\n# Define parameter grid\\nparam_grid = {\\'C\\': [0.1, 1, 10], \\'kernel\\': [\\'linear\\n\\', \\'rbf\\']}\\n# Perform grid search\\ngrid_search = GridSearchCV(SVC(), param_grid , cv=5)\\ngrid_search.fit(X, y)\\n# Output results\\nprint(f\"Best parameters: {grid_search.best_params_}\\n\")\\nprint(f\"Best score: {grid_search.best_score_:.2f}\")\\n8'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 8}, page_content='5\\nPreprocessing\\nDefinition 5.1. The sklearn.preprocessing module provides tools for transforming raw data\\ninto formats suitable for machine learning, including scaling, normalization, encoding categorical\\nvariables, and feature extraction.\\nRaw Data\\nScaler\\nEncoder\\nProcessed Data\\nFigure 4: Preprocessing Workflow\\n5.1\\nKey APIs\\n- StandardScaler:\\n• Purpose: Standardize features by removing mean and scaling to unit variance\\n• Use Case: When features have different scales\\n• Features: Preserves outliers\\n- MinMaxScaler:\\n• Purpose: Scale features to a given range (default [0, 1])\\n• Use Case: When bounded features are required\\n• Features: Sensitive to outliers\\n- OneHotEncoder:\\n• Purpose: Convert categorical features to binary indicators\\n• Use Case: When categories have no ordinal relationship\\n• Features: Handles unknown categories\\n- LabelEncoder:\\n• Purpose: Encode target labels with value between 0 and n_classes-1\\n• Use Case: Preparing classification targets\\n• Features: Simple transformation\\n9'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 9}, page_content=\"Example 5.1.\\nfrom sklearn.preprocessing import StandardScaler ,\\nOneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\nimport pandas as pd\\n# Create sample data\\ndata = pd.DataFrame({\\n'age': [25, 30, 35],\\n'income': [50000, 60000, 70000],\\n'gender': ['M', 'F', 'M']\\n})\\n# Define preprocessing\\npreprocessor = ColumnTransformer(\\ntransformers=[\\n('num', StandardScaler(), ['age', 'income']),\\n('cat', OneHotEncoder(), ['gender'])\\n])\\n# Apply transformations\\nprocessed = preprocessor.fit_transform(data)\\nprint(processed)\\n10\"),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 10}, page_content='6\\nPipelines\\nDefinition 6.1. The sklearn.pipeline module provides utilities to chain multiple processing\\nsteps together, ensuring proper data flow and preventing data leakage during cross-validation.\\nRaw Data\\nPreprocessing\\nModel\\nPredictions\\nFigure 5: Pipeline Workflow\\n6.1\\nKey APIs\\n- Pipeline:\\n• Purpose: Chain multiple estimators into one\\n• Use Case: Ensuring proper data flow\\n• Features: Single interface for all steps\\nExample 6.1.\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import\\ntrain_test_split\\nfrom sklearn.datasets import load_breast_cancer\\n# Load data\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train , X_test, y_train , y_test = train_test_split\\n(X, y, test_size=0.2)\\n# Create pipeline\\npipe = Pipeline([\\n(\\'scaler\\', StandardScaler()),\\n(\\'clf\\', LogisticRegression())\\n])\\n# Fit and evaluate\\npipe.fit(X_train , y_train)\\nprint(f\"Test Accuracy: {pipe.score(X_test , y_test)\\n:.2f}\")\\n11'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 11}, page_content='References\\n[1] Scikit-learn Development Team. Scikit-learn Documentation. https://scikit-learn.org/\\nstable/, 2025.\\n[2] Scikit-learn\\nContributors.\\nScikit-learn\\nGitHub\\nRepository.\\nhttps://github.com/\\nscikit-learn/scikit-learn, 2025.\\n12'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 0}, page_content='seaborn: statistical data visualization\\nMichael L. Waskom1\\n1 Center for Neural Science, New York University\\nDOI: 10.21105/joss.03021\\nSoftware\\n• Review\\n• Repository\\n• Archive\\nEditor: Lorena Pantano\\nReviewers:\\n• @dangeles\\n• @Sara-ShiHo\\nSubmitted: 29 January 2021\\nPublished: 06 April 2021\\nLicense\\nAuthors of papers retain\\ncopyright and release the work\\nunder a Creative Commons\\nAttribution 4.0 International\\nLicense (CC BY 4.0).\\nSummary\\nseaborn is a library for making statistical graphics in Python. It provides a high-level interface\\nto matplotlib and integrates closely with pandas data structures. Functions in the seaborn\\nlibrary expose a declarative, dataset-oriented API that makes it easy to translate questions\\nabout data into graphics that can answer them. When given a dataset and a specification\\nof the plot to make, seaborn automatically maps the data values to visual attributes such\\nas color, size, or style, internally computes statistical transformations, and decorates the plot\\nwith informative axis labels and a legend. Many seaborn functions can generate figures with\\nmultiple panels that elicit comparisons between conditional subsets of data or across different\\npairings of variables in a dataset. seaborn is designed to be useful throughout the lifecycle of\\na scientific project. By producing complete graphics from a single function call with minimal\\narguments, seaborn facilitates rapid prototyping and exploratory data analysis.\\nAnd by\\noffering extensive options for customization, along with exposing the underlying matplotlib\\nobjects, it can be used to create polished, publication-quality figures.\\nStatement of need\\nData visualization is an indispensable part of the scientific process. Effective visualizations\\nwill allow a scientist both to understand their own data and to communicate their insights to\\nothers (Tukey, 1977). These goals can be furthered by tools for specifying a graph that provide\\na good balance between efficiency and flexibility. Within the scientific Python ecosystem, the\\nmatplotlib (Hunter, 2007) project is very well established, having been under continuous\\ndevelopment for nearly two decades. It is highly flexible, offering fine-grained control over the\\nplacement and visual appearance of objects in a plot. It can be used interactively through GUI\\napplications, and it can output graphics to a wide range of static formats. Yet its relatively\\nlow-level API can make some common tasks cumbersome to perform. For example, creating\\na scatter plot where the marker size represents a numeric variable and the marker shape\\nrepresents a categorical variable requires one to transform the size values to graphical units\\nand to loop over the categorical levels, separately invoking a plotting function for each marker\\ntype.\\nThe seaborn library offers an interface to matplotlib that permits rapid data exploration\\nand prototyping of visualizations while retaining much of the flexibility and stability that are\\nnecessary to produce publication-quality graphics. It is domain-general and can be used to\\nvisualize a wide range of datasets that are well-represented within a tabular format.\\nExample\\nThe following example demonstrates the creation of a figure with seaborn. The example\\nmakes use of one of the built-in datasets that are provided for documentation and generation of\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n1'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 1}, page_content='reproducible bug reports. It illustrates several of the features described in the Overview section,\\nincluding the declarative API, semantic mappings, faceting across subplots, aggregation with\\nerror bars, and visual theme control.\\nimport seaborn as sns\\nsns.set_theme(context=\"paper\")\\nfmri = sns.load_dataset(\"fmri\")\\ng = sns.relplot(\\ndata=fmri, kind=\"line\",\\nx=\"timepoint\", y=\"signal\",\\nhue=\"event\", style=\"event\", col=\"region\",\\nheight=3.5, aspect=.8,\\n)\\ng.savefig(\"paper_demo.pdf\")\\n0\\n5\\n10\\n15\\ntimepoint\\n0.1\\n0.0\\n0.1\\n0.2\\n0.3\\nsignal\\nregion = parietal\\n0\\n5\\n10\\n15\\ntimepoint\\nregion = frontal\\nevent\\nstim\\ncue\\nFigure 1: An example seaborn figure demonstrating some of its key features.\\nThe image was\\ngenerated using seaborn v0.11.1.\\nOverview\\nUsers interface with seaborn through a collection of plotting functions that share a common\\nAPI for plot specification and offer many more specific options for customization.\\nThese\\nfunctions range from basic plot types such as scatter and line plots to functions that apply\\nvarious transformations and abstractions, such as histogram binning, kernel density estimation,\\nand regression model fitting. Functions in seaborn are classified as either “axes-level” or\\n“figure-level.” Axes-level functions behave like most plotting functions in the matplotlib.\\npyplot namespace. By default, they hook into the state machine that tracks a “current”\\nfigure and add a layer to it, but they can also accept a matplotlib axes object to control\\nwhere the plot is drawn, similar to using the matplotlib “object-oriented” interface. Figure-\\nlevel functions create their own figure when invoked, allowing them to “facet” the dataset\\nby creating multiple conditional subplots, along with adding conveniences such as putting\\nthe legend outside the space of the plot by default. Each figure-level function corresponds\\nto several axes-level functions that serve similar purposes, with a single parameter selecting\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n2'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 2}, page_content='the kind of plot to make. For example, the displot function can produce several different\\nrepresentations of a distribution, including a histogram, kernel density estimate, or empirical\\ncumulative distribution function. The figure-level functions make use of a seaborn class that\\ncontrols the layout of the figure, mediating between the axes-level functions and matplotlib.\\nThese classes are part of the public API and can be used directly for advanced applications.\\nOne of the key features in seaborn is that variables in a dataset can be automatically\\n“mapped” to visual attributes of the graph. These transformations are referred to as “se-\\nmantic” mappings because they endow the attributes with meaning vis a vis the dataset. By\\nfreeing the user from manually specifying the transformations – which often requires looping\\nand multiple function invocations when using matplotlib directly – seaborn allows rapid\\nexploration of multidimensional relationships. To further aid efficiency, the default parameters\\nof the mappings are opinionated. For example, when mapping the color of the elements in a\\nplot, seaborn infers whether to use a qualitative or quantitative mapping based on whether\\nthe input data are categorical or numeric. This behavior can be further configured or even\\noverridden by setting additional parameters of each plotting function.\\nSeveral seaborn functions also apply statistical transformations to the input data before\\nplotting, ranging from estimating the mean or median to fitting a general linear model. When\\ndata are transformed in this way, seaborn automatically computes and shows error bars to\\nprovide a visual cue about the uncertainty of the estimate. Unlike many graphical libraries,\\nseaborn shows 95% confidence interval error bars by default, rather than standard errors. The\\nconfidence intervals are computed with a bootstrap algorithm, allowing them to generalize over\\nmany different statistics, and the default level allows the user to perform “inference by eye”\\n(Cumming & Finch, 2005). Historically, error bar specification has been relatively limited, but\\na forthcoming release (v0.12) will introduce a new configuration system that makes it possible\\nto show nonparametric percentile intervals and scaled analytic estimates of standard error or\\nstandard deviation statistics.\\nseaborn aims to be flexible about the format of its input data. The most convenient usage\\npattern provides a pandas (McKinney, 2010) dataframe with variables encoded in a long-\\nform or “tidy” (Wickham, 2014) format. With this format, columns in the dataframe can\\nbe explicitly assigned to roles in the plot, such as specifying the x and y positions of a\\nscatterplot along with size and shape semantics. Long-form data supports efficient exploration\\nand prototyping because variables can be assigned different roles in the plot without modifying\\nanything about the original dataset.\\nBut most seaborn functions can also consume and\\nvisualize “wide-form” data, typically producing similar output to how the analogous matplot\\nlib function would interpret a 2D array (e.g., producing a boxplot where each box represents a\\ncolumn in the dataframe) while making use of the index and column names to label the graph.\\nUsing the label information in a pandas object can help make plots that are interpretable\\nwithout further tweaking – reducing the chance of interpretive errors – but seaborn also\\naccepts data from a variety of more basic formats, including numpy (Harris et al., 2020) arrays\\nand simple Python collection types.\\nseaborn also offers multiple built-in themes that users can select to modify the visual appear-\\nance of their graphs. The themes make use of the matplotlib rcParams system, meaning\\nthat they will take effect for any figure created using matplotlib, not just those made by\\nseaborn. The themes are defined by two disjoint sets of parameters that separately control\\nthe style of the figure and the scaling of its elements (such as line widths and font sizes). This\\nseparation makes it easy to generate multiple versions of a figure that are scaled for different\\ncontexts, such as written reports and slide presentations. The theming system can also be\\nused to set a default color palette. As color is particularly important in data visualization and\\nno single set of defaults is universally appropriate, every plotting function makes it easy to\\nchoose an alternate categorical palette or continuous gradient mapping that is well-suited for\\nthe particular dataset and plot type. The seaborn documentation contains a tutorial on the\\nuse of color in data visualization to help users make this important decision.\\nseaborn does not aim to completely encapsulate or replace matplotlib.\\nMany useful\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n3'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 3}, page_content='graphs can be created through the seaborn interface, but more advanced applications –\\nsuch as defining composite figures with multiple arbitrary plot types – will require importing\\nand using matplotlib as well.\\nEven when calling only seaborn functions, deeper cus-\\ntomization of the plot appearance is achieved by specifying parameters that are passed-\\nthrough to the underlying matplotlib functions, and tweaks to the default axis limits,\\nticks, and labels are made by calling methods on the matplotlib object that axes-level\\nseaborn functions return.\\nThis approach is distinct from other statistical graphing sys-\\ntems, such as ggplot2 (Wickham, 2016).\\nWhile seaborn offers some similar features\\nand, in some cases, uses similar terminology to ggplot2, it does not implement the for-\\nmal Grammar of Graphics and cannot be used to produce arbitrary visualizations. Rather, its\\naim is to facilitate rapid exploration and prototyping through named functions and opinion-\\nated defaults while allowing the user to leverage the considerable flexibility of matplotlib\\nto create more domain-specific graphics and to polish figures for publication.\\nAn exam-\\nple of a successful use of this approach to produce reproducible figures can be found at\\nhttps://github.com/WagnerLabPapers/Waskom_PNAS_2017 (Waskom & Wagner, 2017).\\nAcknowledgements\\nM.L.W. has been supported by the National Science Foundation IGERT program (0801700)\\nand by the Simons Foundation as a Junior Fellow in the Simons Society of Fellows (527794).\\nMany others have helped improve seaborn by asking questions, reporting bugs, and con-\\ntributing code; thank you to this community.\\nReferences\\nCumming, G., & Finch, S. (2005). Inference by eye: confidence intervals and how to read\\npictures of data. The American Psychologist, 60(2), 170–180. https://doi.org/10.1037/\\n0003-066X.60.2.170\\nHarris, C. R., Millman, K. J., Walt, S. J. van der, Gommers, R., Virtanen, P., Cournapeau,\\nD., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., Kerkwijk,\\nM. H. van, Brett, M., Haldane, A., R’ıo, J. F. del, Wiebe, M., Peterson, P., … Oliphant,\\nT. E. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https:\\n//doi.org/10.1038/s41586-020-2649-2\\nHunter, J. D. (2007). Matplotlib: A 2D graphics environment. Computing in Science &\\nEngineering, 9(3), 90–95. https://doi.org/10.1109/MCSE.2007.55\\nMcKinney, W. (2010). Data structures for statistical computing in python. In S. van der Walt\\n& J. Millman (Eds.), Proceedings of the 9th Python in Science Conference (pp. 51–56).\\nhttps://doi.org/10.25080/Majora-92bf1922-00a\\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley. ISBN: 978-0201076165\\nWaskom, M. L., & Wagner, A. D. (2017). Distributed representation of context by intrinsic\\nsubnetworks in prefrontal cortex. Proceedings of the National Academy of Sciences, 2030–\\n2035. https://doi.org/10.1073/pnas.1615269114\\nWickham, H. (2014). Tidy data.\\nJournal of Statistical Software, Articles, 59(10), 1–23.\\nhttps://doi.org/10.18637/jss.v059.i10\\nWickham, H. (2016).\\nggplot2:\\nElegant graphics for data analysis.\\nSpringer-Verlag.\\nISBN: 978-3-319-24277-4\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n4'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Jian Tao\\njtao@tamu.edu\\nSpring 2020 HPRC Short Course  \\n03/27/2020\\nIntroduction to Deep Learning \\nwith TensorFlow'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Schedule\\n●Part I. Deep Learning (70 mins)\\n●Break (10 mins)\\n●Part II. Intro to TensorFlow (70 mins)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='GitHub Repository for the Webinars\\nhttps://github.com/jtao/dswebinar'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Jupyter Notebook and JupyterLab\\nJupyter Notebook\\nJupyterLab'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Google Colaboratory'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Google Colaboratory\\nSearch GitHub user: jtao/dswebinar'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Part I. Deep Learning\\nDeep Learning\\nby Ian Goodfellow, Yoshua Bengio, and Aaron Courville\\nhttp://www.deeplearningbook.org/\\nAnimation of Neutron Networks\\nby Grant Sanderson\\nhttps://www.3blue1brown.com/'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Relationship of AI, ML, and DL\\nArtificial Intelligence\\n \\nMachine Learning\\n \\nDeep Learning\\n●Artificial Intelligence (AI)  \\nis anything about \\nman-made intelligence \\nexhibited by machines.\\n●Machine Learning (ML) is \\nan approach to achieve AI.\\n●Deep Learning (DL) is one \\ntechnique to implement \\nML.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Machine Learning\\nTraditional Modeling\\nMachine Learning (Supervised Learning)\\nSample \\nData\\nExpected \\nOutput\\nComputer\\nModel\\nData\\nScientific \\nModel\\nComputer\\nPrediction\\nModel\\nData\\nComputer\\nPrediction'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Types of ML Algorithms\\n●\\nSupervised Learning\\n○\\ntrained with labeled data; \\nincluding regression and \\nclassification problems\\n●\\nUnsupervised Learning\\n○\\ntrained with unlabeled data; \\nclustering and association rule \\nlearning problems.\\n●\\nReinforcement Learning\\n○\\nno training data; stochastic \\nMarkov decision process; robotics \\nand self-driving cars.\\nSupervised Learning\\nReinforcement Learning\\nUnsupervised Learning\\nMachine Learning'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Supervised Learning\\nWhen both input variables - X and output variables - Y are known, one can \\napproximate the mapping function from  X to Y.\\nTraining Data\\nML Algorithm\\nModel\\nTest Data\\nStep 1: Training\\nStep 2: Testing'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Unsupervised Learning\\nWhen only input variables - X are known and the training data is neither \\nclassified nor labeled. It is usually used for clustering problems.\\nData\\nClass 1\\nClass 2\\nClass 3'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Reinforcement Learning\\nWhen the input variables are only available via interacting with the \\nenvironment, reinforcement learning can be used to train an \"agent\".\\n(Image Credit: Wikipedia.org)\\n(Image Credit: deeplearning4j.org)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Why Deep Learning?\\n●Limitations of traditional machine learning algorithms\\n○not good at handling high dimensional data.\\n○difficult to do feature extraction and object recognition.\\n●Advantages of deep learning\\n○DL is computationally expensive, but it is capable of \\nhandling high dimensional data.\\n○feature extraction is done automatically.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='What is Deep Learning?\\nDeep learning is a class of machine learning algorithms that:\\n●use a cascade of multiple layers of nonlinear processing units \\nfor feature extraction and transformation. Each successive \\nlayer uses the output from the previous layer as input.\\n●learn in supervised (e.g., classification) and/or unsupervised \\n(e.g., pattern analysis) manners.\\n●learn multiple levels of representations that correspond to \\ndifferent levels of abstraction; the levels form a hierarchy of \\nconcepts.\\n(Source: Wikipedia)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Artificial Neural Network\\n(Image Credit: Wikipedia)\\nInput\\nOutput\\nHidden Layers'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Inputs and Outputs \\n256 X 256 \\nMatrix\\n4-Element Vector \\nDL model\\n1\\n2\\n3\\n4\\n5\\n6\\nA\\nC\\nT\\nG\\nM\\nF\\nWith deep learning, we are searching for a surjective \\n(or onto) function f from a set X to a set Y. \\nX\\nY'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='18 \\nLearning Principle\\nx\\n1\\nx\\n2\\nx\\nn\\n…..\\n-\\nError:\\nOutput/Prediction\\nTarget Output\\nDataset\\n= 5'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='19 \\nLearning Principle\\nx\\n1\\nx\\n2\\nx\\nn\\n…..\\n-\\nError:\\nOutput/Prediction\\nTarget Output\\n= 15'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='20 \\nLearning Principle\\nx\\n1\\nx\\n2\\nx\\nn\\n…..\\n-\\nError:\\nOutput/Prediction\\nTarget Output\\n= 2.5'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='Supervised Deep Learning with Neural Networks\\nX3\\nX2\\nX1\\nY3\\nInput\\nOutput\\nHidden Layers\\nW1\\nW2\\nW3\\nFrom one layer to the next\\nf is the activation function,\\nWi is the weight, and bi is \\nthe bias.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Training - Minimizing the Loss \\nX3\\nX2\\nX1\\nY2\\nInput\\nOutput\\nW3, b3\\nThe loss function with regard to weights \\nand biases can be defined as\\nW2, b2\\nW1, b1\\nL\\nThe weight update is computed by moving \\na step to the opposite direction of the cost \\ngradient. \\nIterate until L stops decreasing.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Convolution in 2D\\n(Image Credit: Applied Deep Learning | Arden Dertat)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='Convolution Kernel \\n(Image Credit: Applied Deep Learning | Arden Dertat)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='Convolution on Image\\nImage Credit: Deep Learning Methods for Vision | CVPR 2012 Tutorial'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Activation Functions\\nImage Credit: towardsdatascience.com'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 26}, page_content='Introducing Non Linearity (ReLU)\\nImage Credit: Deep Learning Methods for Vision | CVPR 2012 Tutorial'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 27}, page_content='Max Pooling \\n(Image Credit: Applied Deep Learning | Arden Dertat)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 28}, page_content='Pooling - Max-Pooling and Sum-Pooling\\nImage Credit: Deep Learning Methods for Vision | CVPR 2012 Tutorial'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 29}, page_content='CNN Implementation - Drop Out\\n(Image Credit: Applied Deep Learning | Arden Dertat)\\nDropout is used to prevent overfitting. A neuron is temporarily \\n“dropped” or disabled with probability P during training.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 30}, page_content='CNN Implementation - Data Augmentation (DA)\\n(Image Credit: Applied Deep Learning | Arden Dertat)\\nDA helps to popular  \\nartificial training \\ninstances from the \\nexisting train data sets.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 31}, page_content='Convolutional Neural Networks\\nA convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward \\nartificial neural networks that explicitly assumes that the inputs are images, which allows \\nus to encode certain properties into the architecture.\\n(Image Credit: https://becominghuman.ai)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 32}, page_content='Deep Learning for Facial Recognition \\n(Image Credit: www.edureka.co)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 33}, page_content='MNIST - Introduction\\n●\\nMNIST (Mixed National \\nInstitute of Standards and \\nTechnology) is a database for \\nhandwritten digits, distributed \\nby Yann Lecun.\\n●\\n60,000 examples, and a test \\nset of 10,000 examples.\\n●\\n28x28 pixels each.\\n●\\nWidely used for research and \\neducational purposes.\\n(Image Credit: Wikipedia)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 34}, page_content='MNIST - CNN Visualization\\n(Image Credit: http://scs.ryerson.ca/~aharley/vis/)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 35}, page_content='Part II. Introduction to TensorFlow\\n36\\nTensorFlow Official Website\\nhttp://www.tensorflow.org'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 36}, page_content='A Brief History of TensorFlow\\nTensorFlow is an end-to-end FOSS (free and open source software) \\nlibrary for dataflow, differentiable programming. TensorFlow is one of \\nthe most popular program frameworks for building machine learning \\napplications.\\n●\\nGoogle Brain built DistBelief in 2011 for internal usage.\\n●\\nTensorFlow 1.0.0 was released on Feb 11, 2017\\n●\\nTensorFlow 2.0 was released in Jan 2018.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 37}, page_content='TensorFlow, Keras, and PyTorch\\nKeras is a high-level \\nneural networks API, \\nwritten in Python and \\ncapable of running on \\ntop of TensorFlow, \\nCNTK, or Theano. It \\nwas developed with a \\nfocus on enabling fast \\nexperimentation.\\nTensorFlow is an \\nend-to-end open \\nsource platform for \\nmachine learning. It \\nhas a comprehensive, \\nflexible ecosystem to \\nbuild and deploy ML \\npowered applications.\\nPyTorch is an open \\nsource machine \\nlearning framework \\nthat accelerates the \\npath from research \\nprototyping to \\nproduction \\ndeployment.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 38}, page_content='Google Trends for Popular ML Frameworks\\n(Image Credit: https://trends.google.com/)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 39}, page_content='Programming Environment\\n(Image Credit: tensorflow.org)\\nIn TF 2.0, tf.keras is the \\nrecommended \\nhigh-level API.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 40}, page_content='(Image Credit: Plumber Game by Mobiloids)\\nA Connected Pipeline for the Flow of Tensors'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 41}, page_content='What is a Tensor in TensorFlow?\\n●\\nTensorFlow uses a tensor \\ndata structure to represent all \\ndata. A TensorFlow tensor as \\nan n-dimensional array or list. \\nA tensor has a static type, a \\nrank, and a shape.\\nName\\nRank\\nTensor\\nScalar\\n0\\n[5]\\nVector\\n1\\n[1 2 3]\\nMatrix\\n2\\n[[1 2 3 4],\\n[5 6 7 8]]\\nTensor\\n3\\n...'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 42}, page_content='TensorFlow Data Types \\nBasic TensorFlow data types include:\\n●\\nint[8|16|32|64], float[16|32|64], double\\n●\\nbool \\n●\\nstring\\nwith tf.cast(), the data types of variables \\ncould be converted.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 43}, page_content='Hello World with TensorFlow\\nimport tensorflow as tf\\nv = tf.constant(\"Hello World!\")\\ntf.print(v)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 44}, page_content='TensorFlow Constants\\nimport tensorflow as tf\\nx = tf.constant(1, tf.int32)\\nzeros = tf.zeros([2, 3], tf.int32)\\nones = tf.ones([2, 3], tf.int32)\\ny = x *(zeros + ones + ones)\\ntf.print(y)\\nTensorFlow provides several operations to generate constant tensor.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 45}, page_content='TensorFlow Variables\\nTensorFlow variables can represent shared, persistent state manipulated by \\nyour program. Weights and biases are usually stored in variables.\\nimport tensorflow as tf\\nW = tf.Variable(tf.random.normal([2,2], stddev=0.1), \\nname = \"W\")\\nb = tf.Variable(tf.zeros(shape=(2)), name=\"b\")'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 46}, page_content='Machine Learning Workflow with tf.keras\\nStep 1\\nPrepare Train Data\\nThe preprocessed data set needs \\nto be shuﬄed and splitted into \\ntraining and testing data.\\n  \\nStep 2\\nDeﬁne Model\\nA model could be deﬁned with \\ntf.keras Sequential model for a \\nlinear stack of layers or tf.keras \\nfunctional API for complex \\nnetwork.\\n  \\nStep 3\\nTraining Conﬁguration\\nThe conﬁguration of the training \\nprocess requires the \\nspeciﬁcation of an optimizer, a \\nloss function, and a list of \\nmetrics.\\n  \\nStep 4\\nTrain Model\\nThe training begins by calling the \\nﬁt function. The number of \\nepochs and batch size need to be \\nset. The measurement metrics \\nneed to be evaluated.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 47}, page_content='tf.keras Built-in Datasets\\n●\\ntf.keras provides many popular reference datasets that could be used \\nfor demonstrating and testing deep neural network models. To name a \\nfew,\\n○\\nBoston Housing (regression)\\n○\\nCIFAR100 (classification of 100 image labels)\\n○\\nMNIST (classification of 10 digits)\\n○\\nFashion-MNIST (classification of 10 fashion categories)\\n○\\nReuters News (multiclass text classification)\\n●\\nThe built-in datasets could be easily read in for training purpose. E.g.,\\nfrom tensorflow.keras.datasets import boston_housing\\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 48}, page_content='Prepare Datasets for tf.keras\\nIn order to train a deep neural network model with \\nKeras, the input data sets needs to be cleaned, \\nbalanced, transformed, scaled, and splitted.\\n●\\nBalance the classes. Unbalanced classes will \\ninterfere with training.\\n●\\nTransform the categorical variables into \\none-hot encoded variables. \\n●\\nExtract the X (variables) and y (targets) values \\nfor the training and testing datasets.\\n●\\nScale/normalize the variables.\\n●\\nShuffle and split the dataset into training and \\ntesting datasets\\nDog\\nCat\\nHorse\\n1\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\nDog\\nCat\\nHorse\\n1\\n2\\n3\\nOne-hot encoding\\nNumerical encoding'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 49}, page_content=\"Create a tf.keras Model\\n●\\nLayers are the fundamental \\nbuilding blocks of tf.keras \\nmodels. \\n●\\nThe Sequential model is a \\nlinear stack of layers.\\n●\\nA Sequential model can be \\ncreated with a list of layer \\ninstances to the constructor or \\nadded with the .add() method.\\n●\\nThe input shape/dimension of \\nthe first layer need to be set.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, \\nActivation\\nmodel = Sequential([\\n    Dense(64, activation='relu', input_dim=20),\\n    Dense(10, activation='softmax')\\n])\\nInput\\nOutput\\nHidden Layers\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 50}, page_content='Compile a tf.keras Model\\nThe compile method of a Keras model configures the learning \\nprocess before the model is trained. The following 3 arguments need \\nto be set (the optimizer and loss function are required).\\n●\\nAn optimizer: Adam, AdaGrad, SGD, RMSprop, etc.\\n●\\nA loss function: mean_squared_error, mean_absolute_error, \\nmean_squared_logarithmic_error, categorical_crossentropy, \\nkullback_leibler_divergence, etc.\\n●\\nA list of measurement metrics: accuracy, binary_accuracy, \\ncategorical_accuracy, etc.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 51}, page_content='Train and Evaluate a tf.keras Model\\nModel: \"sequential_1\"\\n_______________________________________________\\nLayer (type)                 Output Shape              Param #   \\n=============================================\\ndense_11 (Dense)             (None, 64)                1344      \\n_______________________________________________\\ndense_12 (Dense)             (None, 10)                650       \\n=============================================\\nTotal params: 1,994\\nTrainable params: 1,994\\nNon-trainable params: 0\\n_______________________________________________\\nNone\\ntf.keras is trained on NumPy arrays of input \\ndata and labels. The training is done with the \\n●\\nfit() function of the model class. In the fit \\nfunction, the following two \\nhyperparameters can be set:\\n○\\nnumber of epochs\\n○\\nbatch size\\n●\\nevaluate() function returns the loss value \\n& metrics values for the model in test \\nmode.\\n●\\nsummary() function prints out the \\nnetwork architecture.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 52}, page_content='Make Predictions and More\\nAfter the model is trained, \\n●\\npredict() function of the model class could be used to \\ngenerate output predictions for the input samples.\\n●\\nget_weights() function returns a list of all weight tensors in \\nthe model, as Numpy arrays.\\n●\\nto_json() returns a representation of the model as a JSON \\nstring. Note that the representation does not include the \\nweights, only the architecture. \\n●\\nsave_weights(filepath) saves the weights of the model as a \\nHDF5 file.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 53}, page_content='Hands-on Session #1\\nGetting Started with TensorFlow'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 54}, page_content='Hands-on Session #2\\nClassify Handwritten Digits with \\nTensorFlow'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 0}, page_content=\"Natural Language to Python Source Code using \\nTransformers  \\n \\nMeet Shah \\nInformation Technology \\nSardar Patel Institute of Technology \\nMumbai, India \\nmeet8june@gmail.com \\n \\nRajat Shenoy \\nInformation Technology \\nSardar Patel Institute of Technology \\nMumbai, India \\nrajatshenoy@gmail.com \\n \\nRadha Shankarmani \\nInformation Technology \\nSardar Patel Institute of Technology \\nMumbai, India \\nradha_shankarmani@spit.ac.in\\nAbstract—Writing code using natural language is a very exciting \\napplication of Neural Machine Translation. To achieve a small \\npart of such an application, in this paper, we try to generate python \\nsource code snippets from natural English language descriptions \\nusing the Django dataset. We trained the self-attention based \\ntransformer architecture on the snippets from the dataset. We \\nachieved a BLEU score of 64.29 \\nKeywords—Transformers, tokenizers, Django, English, Python \\n \\nI. INTRODUCTION \\nProgramming is a vast field of engineering. More and more \\npeople are joining this profession and the skill is in high \\ndemand. But most of the programmers face this problem that \\nthe field is too fast and there is too much adaptation required \\nif we want to stay in this field for a long time. Writing source \\ncode in different languages is very tough for programmers. \\nThey need to first refer to the documentation of those \\nprogramming languages, understand the syntax and then \\nwrite the source code for their program. \\nWe strongly believe that programming should be based on \\nthe application of logic and problem-solving skills more as \\ncompared to having the knowledge of a particular \\nprogramming language. That is why we propose this solution \\nwhere we translate natural language (English) to a \\nprogramming \\nlanguage \\n(Python). \\nThis \\nwill \\nallow \\nprogrammers to write programs in natural language and not \\nworry about the syntax in various languages and the silly \\nsyntax errors that keep on occurring all the time.  \\nSemantic parsing is the problem of converting natural \\nlanguage constructs into logical constructs. One of the \\napplications of semantic parsing is machine translation. \\nMachine Translation uses software to convert one language \\nto another language. We are working on a small part of this \\nproblem by converting natural English language constructs \\ninto python programming language syntax. We will be using \\nthe Django dataset for this task. \\nIn the future, maybe when this proposed system is developed \\nenough, even non-programmers will be able to write \\nprograms with the help of just natural language if they \\nunderstand the logic to solve the problem. This paper is in the \\nprimitive stages of the research topic and is subject to further \\nimprovement and research. \\n \\nII. LITERATURE REVIEW \\nMany papers have been published that follows a deep \\nlearning approach for natural language to source code \\nconversion. [1] and [2] use attention-based encoder-decoder \\narchitectures. They use one or more than one LSTM layers \\nwith appropriate attention mechanisms. \\nEncoder/decoder based transformers introduced in [3] are \\nnow replacing LSTM based architectures. Transformer based \\narchitectures \\nare \\nnow \\noutperforming \\nLSTM \\nbased \\narchitectures. State-of-the-art performance has been achieved \\nin machine translation tasks[4], language modelling tasks, \\nclassification tasks [5],  grammar correction tasks [6], \\nsummarization \\ntasks[7], \\nentity \\nrecognition, \\ndialogue \\ngeneration tasks [8], and other NLP tasks as well using the \\nTransformer architecture. \\n \\nProgramming languages are of course not natural but many \\nNLP approaches are being applied to them. The programming \\nlanguages have grammar and syntax but with relatively lesser \\nvocabulary than natural languages. There also are \\nrelationships between the vocabulary. Therefore there are \\nmany opportunities to model these languages using NLP \\nmodels and tasks.  \\n \\nSuch NLP tasks applied to programming languages definitely \\nhave some uses in software development. Examples of which \\ninclude summarization/ translation of source code to generate \\ndocumentation or summaries[9][10], using natural language \\nquery for code search [11][12], patching  and detecting bugs \\nusing translation and error correction[13], code completion \\nusing language modelling or generation[14][15] \\n \\nWe will be using the Django Dataset [16] in this paper. The \\nDjango Dataset provides a dataset for annotated code \\nsnippets. For example, the dataset will contain the natural \\nlanguage intent ‘call the params.get method with string ' \\nKEY_PREFIX ' and an empty string as arguments, substitute \\nthe result for self._key_prefix.’ mapped to the python code \\n‘self . key_prefix = params . get ( 'KEY_PREFIX' , '' )’. The \\nDjango dataset contains 18805 such pairs of natural language \\nintents mapped to python code. We explore how \\nencoder/decoder based transformer architecture performs on \\nthese datasets.\"),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 1}, page_content='III. METHODOLOGY \\n \\nThe main methodology behind translating natural language to \\ncode is machine translation. The basic idea is that a \\nprogramming language is just like a normal natural language \\nwith much less vocabulary. \\n \\nThere exist mature solutions to translate one natural language \\nto another. Some products like Google Translate do it really \\nwell. If programming languages are treated like natural \\nlanguages and the machine translation solutions are applied \\nto translate from natural language to programming language, \\nwe will reach a good solution at some point in the future. \\n \\nSo for machine translation, the solutions are recurrent neural \\nnetworks (LSTMs) and Transformers. Most of the literature \\nwe reviewed used RNNs as a solution. But transformers are \\nfaster and more robust as compared to LSTMs and hence we \\nchose to use transformers as a solution. \\n \\nThe transformers cannot process English or python directly. \\nThey need to be converted to numerical form to be able to use \\nthem. That is where tokenizers come in handy and we have \\nbuilt our custom tokenizer based on BertTokenizer and we \\nhave built our custom transformer which inherits from the \\nTensorFlow model. The vocabulary generated from the \\ntokenizer is used to train the transformer. \\n \\nIV. IMPLEMENTATION \\n \\nA. User Interface \\n \\nThe implementation consists of 3 parts. The first one is the \\nuser interface. It is built in React. React is a pretty popular \\njavascript library used to build AJAX based frontend of a web \\napp. The frontend consists of 3 user input types. The user can \\ngive the input via a file in which they can type in multiple \\nlines of natural language intents. They can also type single \\nline text intents. They can give audio inputs by speaking in \\nEnglish .They will get a corresponding python snippet as an \\noutput. We have also included a Code-editor for the user, \\nwhere a person can edit the predicted code as well and send \\nthe corrected code to us that we can see in Fig. 1 \\nFig. 1 – UI of our implementation \\nB. Server \\n \\nThe second part is the server side part. This part is made in \\nDjango. Django is a versatile server side python framework \\nthat has many tools for the convenience of web developers. \\nDjango consists of the basic routing of the web app and that \\nis where the saved ML model is used to translate the English \\ninput received from the client side to python and send it back \\nto the client. \\n \\nC. Model \\n \\n1) The Dataset \\n \\nThe third and the main part of the project is the model itself \\nthat does all the work behind the scenes. The dataset used is \\nthe ase15-django-dataset [16]. This dataset consists of 18805 \\nrecords of English and corresponding python snippets. The \\ndata is distributed across code snippets of various libraries \\nlike Django, celery, JSON, etc. show in Fig. 2 \\n \\nFig. 2 – The Django Dataset [16] \\n \\n2)  Pre-processing \\n \\nThis dataset is split into train and test with train dataset \\nconsisting of 15000 records and test dataset consisting of the \\nremaining 3805 records. Some preprocessing is done on the \\ndata first and then it is converted into the TensorFlow dataset \\nformat for better processing with the TensorFlow library. \\nTensorflow is an open-source library in python by Google \\nand it is used for ML and DL operations. It has a vast number \\nof libraries built into it and is popular due to its versatility. \\nThe model training is done in a Google Colab GPU \\nenvironment. The TensorFlow version was 2.4.1.  \\n \\n3)    Tokenizer \\n \\nThen the bert_vocab_from_dataset library was loaded from \\ntensorflow_text for the purpose of vocabulary generation \\nfrom English and python. The vocabulary size for the bert \\nvocab was set to 4000. The vocabulary for python and \\nEnglish was created and stored in a text file. Two Bert \\ntokenizer models were trained using these txt files. One \\ncorresponds to English and the other corresponds to python.  \\n \\nA tokenizer is a tool that converts string input from the \\nvocabulary into a numerical format. Tokenization is \\nperformed on the input before feeding it to the transformer \\nmodel so that the model can understand the input. Also, the \\nnumerical output received from the transformer model is \\nconverted to a string or a human-readable format by'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 2}, page_content='detokenizing using the tokenizer. Then a custom tokenizer is \\nbuilt and saved using TensorFlow’s save model function.  \\n \\n4)   Transformer \\nFig. 3: Transformer Architecture [3] \\n \\nThe tokenized pairs of English and python are divided into \\nbatches with a batch size of 64 and buffer size of 20000. Then \\npositional encoding is done. RNNs inherently know the \\nposition of a word in a sentence because it is a sequential \\nmodel. A single word input is accepted sequentially by RNN. \\nWhereas in transformers, the model itself does not have any \\nidea about the position of the word in the sentence because of \\nthe inherent nature of taking and processing all the words \\nsimultaneously. Hence, we need to add an extra piece of \\ninformation along with the word which contains details about \\nthe position of the word. This extra piece of information is \\nknown as positional encoding. Then we create a custom \\ntransformer model. \\n \\nThis transformer [3] consists of an encoder, a decoder and a \\nfinal linear layer. The encoder first performs input \\nembedding, then does positional encoding and then sends the \\ndata through multiple encoding layers. An encoding layer \\nconsists of a multi-head attention sublayer and a pointwise \\nfeed-forward network sublayer. The multi-head attention \\noperation splits a linear layer into heads, then performs scaled \\ndot-product attention, and then concatenates the heads into a \\nfinal linear layer. The decoder performs an output \\nembedding, then it performs positional encoding and then the \\ndata goes through multiple decoder layers. A decoder layer \\nfirst performs masked multi-head attention, then performs \\nmulti-head attention with padding mask and then the data \\ngoes through  pointwise feed-forward networks. We can see \\nthis architecture in Fig. 3. \\n \\nWe then set our hyper parameters. We have used the Adam \\nOptimizer with a customized learning rate scheduler in \\naccordance with the formula described in [3].  \\n \\nFig.4: Learning Rate Schedule \\n \\nWe then defined our loss function and our accuracy metrics. \\nWe used checkpointing while training our model. We had got \\naccess to a single Testa T4 GPU for training. We then train \\nthe model, running 50 epochs on the training dataset. After \\nthis, we evaluate the model, save the weights to use them in \\nthe server. \\n \\nV. RESULT \\nFig.5: Accuracy and Loss Metrics \\n \\nWe trained the transformer on 15000 pairs of intents and \\nsnippets. We obtained accuracy of 0.973 and a loss of about \\n0.0867 on the 50th epoch with each epoch taking on average  \\n \\nWe also calculated the BLEU Score of our model. To \\nevaluate the quality of machine translated natural language \\nfrom another language BLEU (bilingual evaluation \\nunderstudy) is used. It used to determine how close is the \\nmachine translated text compared to the professional human \\ntranslation. It is one of the most popular and inexpensive \\nmetrics. We have shown some of our BLEU scores and our \\naverage BLEU score in Fig. 6'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 3}, page_content='Fig.6: Our Average BLEU Score \\n \\nWe also tried giving new test cases to the model, some were \\nsuccessfully translated, some were not. The model struggles \\nwith variable names but fairly accurately predicts the syntax \\nof Python programming language. We can see in figure 7 \\nthat the model gave the correct prediction for the English \\nintent. But in figure 8 we can see that the model understood \\nthe syntax of classes but struggles with the variables a bit. \\n \\nFig.7: Correct prediction \\n \\nFig.8: Incorrect prediction \\nVI. CONCLUSION \\n \\nIn this paper, we presented a way to translate natural language \\n(English) to a programming language (python). We aimed to \\nsolve the “programming language barrier” in programming. \\nWe used the transformer model on the Django Dataset and \\nachieved a BLEU Score of 64.29. Our model fairly \\nunderstands the syntax of the Python Programming Language \\nbut struggles with variable names. \\n \\nThis topic is in primitive stages of research and needs more \\nimprovement. In future when this technology is mature \\nenough, any person who does not know a programming \\nlanguage, will be able to write programs in that programming \\nlanguage.  \\n \\nVII. FUTURE SCOPE \\n \\nThe prototype we presented in this paper can be converted \\ninto a full system by adding functionalities like login, voice \\nauthentication, history of programs written, etc. Also, \\ncreating an extension for all major IDEs and text editors \\nwould be a great way for programmers to adapt to this \\ntechnology. By this, programmers will be able to easily write \\ncode and edit it if required in their favourite editor. The model \\ncan be improved by training the model on a dataset with more \\nnumber of records. Also experimentation can be done by \\nusing different tokenizers to improve the model even further. \\n \\n REFERENCES \\n \\n[1] Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model \\nfor general-purpose code generation. arXiv preprint arXiv:1704.01696  \\n[2] Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract \\nsyntax networks for code generation and semantic parsing. arXiv \\npreprint arXiv:1704.07535 .  \\n[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion \\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. \\nAttention is all you need. In Advances in Neural Information \\nProcessing Systems. pages 5998–6008. \\n[4] \\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan \\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. \\nExploring the limits of transfer learning with a unified text-to-text \\ntransformer. arXiv preprint arXiv:1910.10683 \\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. \\n2018. Bert: Pre-training of deep bidirectional transformers for language \\nunderstanding. arXiv preprint arXiv:1810.04805 \\n[6] \\nChristopher Bryant, Mariano Felice, and Edward Briscoe. 2017. \\nAutomatic annotation and evaluation of error types for grammatical \\nerror correction. Association for Computational Linguistics. \\n[7] Yang Liu and Mirella Lapata. 2019. Text summarization with pre \\ntrained encoders. arXiv preprint arXiv:1908.08345 \\n[8] Paweł Budzianowski and Ivan Vulic. 2019. Hello, ´ it’s gpt-2–how can \\ni help you? towards the use of pre trained language models for task \\noriented dialogue systems. arXiv preprint arXiv:1907.05774.  \\n[9] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: \\nGenerating sequences from structured representations of code. arXiv \\npreprint arXiv:1808.01400.  \\n[10] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian \\nWu, and Philip S Yu. 2018. Improving automatic source code \\nsummarization via deep reinforcement learning. In Proceedings of the \\n33rd ACM/IEEE International Conference on Automated Software \\nEngineering, pages 397–407.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 4}, page_content='[11] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code \\nsearch. In Proceedings of the 40th International Conference on \\nSoftware Engineering, ICSE ’18, page 933–944, New York, NY, USA. \\nAssociation for Computing Machinery. \\n[12] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and \\nMarc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the \\nstate of semantic code search. arXiv preprint arXiv:1909.09436.  \\n[13] Juan Zhai, Xiangzhe Xu, Yu Shi, Minxue Pan, Shiqing Ma, Lei Xu, \\nWeifeng Zhang, Lin Tan, and Xiangyu Zhang. 2019. Cpc: \\nautomatically classifying and propagating natural language comments \\nvia program analysis. \\n[14] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. \\n2019. Pythia: Ai-assisted code completion system. In Proceedings of \\nthe 25th ACM SIGKDD International Conference on Knowledge \\nDiscovery & Data Mining, pages 2727–2735.  \\n[15] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel \\nSundaresan. 2020. Intellicode compose: Code generation using \\ntransformers. arXiv preprint arXiv:2005.08025.  \\n[16] Y. Oda et al., \"Learning to Generate Pseudo-Code from Source Code \\nUsing Statistical Machine Translation,\" 2015 30th IEEE/ACM \\nInternational Conference on Automated Software Engineering (ASE), \\nLincoln, NE, USA, 2015, pp. 574-584, doi: 10.1/109/ASE.2015.36. \\n.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DirectoryLoader(\n",
    "    path=\"../data/pdf\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=True\n",
    "    )\n",
    "datas = data.load()\n",
    "datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ab1b5",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0acae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitting(document, chunk_size:int=1000, chunk_overlap:int=200):\n",
    "    \n",
    "    text_splits = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splits.split_documents(documents=document)\n",
    "    print(f\"{len(document)} document split into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01eec340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155 document split into 497 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 0}, page_content='LangChain\\nVasilios Mavroudis\\nAlan Turing Institute\\nvmavroudis@turing.ac.uk\\nAbstract. LangChain is a rapidly emerging framework that offers a ver-\\nsatile and modular approach to developing applications powered by large\\nlanguage models (LLMs). By leveraging LangChain, developers can sim-\\nplify complex stages of the application lifecycle—such as development,\\nproductionization, and deployment—making it easier to build scalable,\\nstateful, and contextually aware applications. It provides tools for han-\\ndling chat models, integrating retrieval-augmented generation (RAG),\\nand offering secure API interactions. With LangChain, rapid deployment\\nof sophisticated LLM solutions across diverse domains becomes feasible.\\nHowever, despite its strengths, LangChain’s emphasis on modularity and\\nintegration introduces complexities and potential security concerns that\\nwarrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 0}, page_content='warrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,\\nLangServe, and LangSmith. We explore how the framework facilitates the\\ndevelopment of LLM applications, discuss its applications across multi-\\nple domains, and critically evaluate its limitations in terms of usability,\\nsecurity, and scalability. By offering valuable insights into both the capa-\\nbilities and challenges of LangChain, this paper serves as a key resource\\nfor developers and researchers interested in leveraging LangChain for\\ninnovative and secure LLM-powered applications.\\nKeywords: LangChain · Large Language Models · LLM Applications ·\\nModular Framework\\nThe emergence of large language models (LLMs) such as OpenAI’s o1 [13],\\nGPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 0}, page_content='GPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-\\nlocked unprecedented capabilities in understanding and generating human-like\\ntext, enabling applications that range from intelligent conversational agents to\\nsophisticated data analysis tools. However, harnessing the full potential of LLMs\\nin real-world applications presents significant challenges. Developers must nav-\\nigate complexities related to model integration, state management, scalability,\\ncontextual awareness, and security.\\nLangChain has rapidly gained prominence as a powerful framework designed\\nto address these challenges in developing LLM-powered applications [2]. By\\nproviding a modular and flexible architecture, LangChain simplifies the com-\\nplexities inherent in working with LLMs, enabling developers to build scalable,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 1}, page_content='2\\nVasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nable API deployment, and LangSmith for monitoring and evaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 1}, page_content='Despite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 1}, page_content='LangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 1}, page_content='functionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1\\nArchitecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 2}, page_content='LangChain\\n3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1. LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 2}, page_content='the top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPrompt Templates: Structured templates that standardize and format queries,\\nensuring consistency and precision in interactions with AI models. These tem-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 2}, page_content='Prompt Templates: Structured templates that standardize and format queries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 3}, page_content='4\\nVasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 3}, page_content='Output Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks: Functions that log, monitor, and stream specific events within LangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1\\nChat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 3}, page_content='conversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2\\nRetrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 4}, page_content='LangChain\\n5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwith vector storage solutions like Chroma and Milvus for optimized searches [3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3\\nSecurity and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 4}, page_content='access to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4\\nIntegrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– Integration Packages: LangChain provides dedicated packages (e.g., langchain-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 4}, page_content='multi-modal data processing and AI tool integration [3]:\\n– Integration Packages: LangChain provides dedicated packages (e.g., langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– Custom Component Development: Developers can build custom plugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 5}, page_content='6\\nVasilios Mavroudis\\n1.5\\nAdvanced Components\\nBeyond these core elements, LangChain offers advanced modules that support\\ncomplex workflows, API deployments, and performance monitoring. These com-\\nponents are elaborated in the following sections:\\n– LangGraph for Stateful Process Modeling: Explored in Section 3,\\nLangGraph enables developers to structure applications with nodes and\\nedges, allowing for complex branching and multi-agent workflows.\\n– LangServe for API Deployment: Detailed in Section 4, LangServe facil-\\nitates the deployment of LangChain applications as REST APIs, supporting\\nscalability in production [5].\\n– LangSmith for Monitoring and Evaluation: Discussed in Section 2,\\nLangSmith offers tools for real-time performance monitoring, error tracking,\\nand version control to optimize applications iteratively [4].\\n2\\nLangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 5}, page_content='2\\nLangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-\\ning essential tools for building production-grade systems. Integrated with the\\nLangChain ecosystem, it enables users to trace, evaluate, and refine applications,\\nenhancing precision in complex environments. The platform addresses key chal-\\nlenges in observability, testing, and optimization, allowing developers to monitor\\nperformance, troubleshoot issues, and maintain high standards over time [9].\\n2.1\\nTracing\\nTracing is a central feature of LangSmith, providing detailed visibility into how\\napplications interact with LLMs and external data sources. For developers using\\nLangChain, LangSmith offers tracing without the need for direct SDK inte-\\ngration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 5}, page_content='gration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-\\nserve and analyze component behavior in real time. This capability is especially\\nuseful for debugging and identifying bottlenecks within complex workflows.\\nLangSmith supports multiple ways to log traces, including languages like\\nPython and TypeScript. Each trace logs every call made to an LLM, along with\\ninput parameters, function annotations, and generated outputs, making it easier\\nto identify where adjustments may be necessary. By using traceable wrappers\\nand decorators, developers can annotate functions or data pipelines, enabling\\nautomatic trace logging with minimal code alterations [9].\\n2.2\\nPerformance Testing\\nLangSmith’s evaluation tools enable developers to test and validate applica-\\ntions under real-world conditions. Evaluations require a defined dataset of test'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 6}, page_content='LangChain\\n7\\ncases, including inputs and expected outputs. Using these datasets, developers\\ncan conduct performance tests and assess how well their models meet expected\\noutcomes—an essential step for applications where accuracy and reliability are\\ncrucial. LangSmith supports custom evaluators, allowing developers to specify\\nscoring functions based on specific needs. For instance, an evaluator may mea-\\nsure the exact match between outputs and expected answers, or use metrics\\nlike cosine similarity for open-ended tasks. By supporting both built-in and cus-\\ntom evaluators, LangSmith provides flexibility in performance measurement for\\ndeterministic outputs or nuanced language generation tasks [9].\\n2.3\\nDataset Management\\nDatasets are foundational to LangSmith’s evaluation system. Organizing test\\ncases into structured datasets enables methodical testing and validation. De-\\nvelopers can create datasets manually or import them from existing sources,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 6}, page_content='cases into structured datasets enables methodical testing and validation. De-\\nvelopers can create datasets manually or import them from existing sources,\\nallowing diverse testing scenarios that reflect real-world use cases. Datasets can\\ncontain structured or unstructured data evaluations, accommodating a variety of\\ntesting needs. LangSmith’s dataset version control allows developers to maintain\\nmultiple dataset versions as applications evolve. This feature is critical for ensur-\\ning consistency in evaluation, especially as application logic changes or models\\nare retrained, providing a robust foundation for testing and validation [9].\\n2.4\\nLangSmith Workflow\\nLangSmith integrates tracing, evaluation, and dataset management into a cohe-\\nsive framework, enabling developers to progress from debugging to optimization\\nin a structured manner. A typical LangSmith workflow includes the following\\nstages:\\n– Trace Logging: Developers activate tracing on application functions, pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 6}, page_content='in a structured manner. A typical LangSmith workflow includes the following\\nstages:\\n– Trace Logging: Developers activate tracing on application functions, pro-\\nviding insights into model-component interactions.\\n– Dataset Creation and Evaluation: Developers create datasets represent-\\ning different scenarios to conduct comprehensive testing.\\n– Evaluation and Iterative Optimization: Evaluation results indicate per-\\nformance areas for refinement, guiding iterative application improvements.\\n– Version Control and Historical Tracking: LangSmith logs all interac-\\ntions, dataset versions, and evaluation scores, allowing developers to assess\\nimprovements over time.\\n2.5\\nIntegration with LangChain and LangServe\\nLangSmith integrates seamlessly with LangChain and LangServe (Section 4) to\\nenhance the end-to-end LLM application development experience. For LangChain\\nusers, LangSmith can automatically log traces and integrate with existing work-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 6}, page_content='enhance the end-to-end LLM application development experience. For LangChain\\nusers, LangSmith can automatically log traces and integrate with existing work-\\nflows. Combined with LangServe, LangSmith provides robust observability for\\nAPI deployments, monitoring real-time usage patterns, tracking request laten-\\ncies, and identifying bottlenecks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 7}, page_content='8\\nVasilios Mavroudis\\n3\\nLangGraph\\nLangGraph is a low-level framework for building stateful, multi-actor appli-\\ncations with large language models (LLMs). It provides developers with fine-\\ngrained control over application flows, incorporating cycles, branching, and per-\\nsistence to support complex agent workflows. Inspired by frameworks such as\\nPregel [11] and Apache Beam [15], LangGraph enables advanced human-in-the-\\nloop applications and persistent state management, allowing for more reliable\\nand adaptable LLM-powered systems [7].\\n3.1\\nCore Features of LangGraph\\nCycles and Branching LangGraph distinguishes itself by supporting cycles\\nand branching in application workflows. This feature is particularly beneficial\\nfor agentic architectures that require iterative or conditional logic. By enabling\\ncycles within workflows, LangGraph provides a flexible structure that allows\\nnodes to execute repeatedly until a specified condition is met. This contrasts'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 7}, page_content='cycles within workflows, LangGraph provides a flexible structure that allows\\nnodes to execute repeatedly until a specified condition is met. This contrasts\\nwith typical directed acyclic graph (DAG)-based architectures, which are limited\\nto single-pass execution without feedback loops [7].\\nPersistence and State Management One of LangGraph’s key innovations is\\nits built-in support for persistence, which enables state to be saved and accessed\\nthroughout the application’s lifecycle. This persistent state management is cru-\\ncial for applications that require continuity across sessions, such as customer ser-\\nvice agents or educational tools that need to recall previous interactions. Lang-\\nGraph’s persistence feature also facilitates advanced human-in-the-loop work-\\nflows, allowing agents to pause, receive human input, and resume operations\\nseamlessly.\\nLangGraph utilizes a stateful execution model where each node in the graph'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 7}, page_content='flows, allowing agents to pause, receive human input, and resume operations\\nseamlessly.\\nLangGraph utilizes a stateful execution model where each node in the graph\\nupdates the application state as it processes input. For instance, in a multi-turn\\nconversation, the graph maintains a memory of all previous messages, which can\\nbe accessed by subsequent nodes to ensure coherent responses. This persistent\\nstate can also be saved externally using the LangGraph Platform [8], ensuring\\nrobust memory management across long-running sessions [7].\\nHuman-in-the-Loop and Streaming Support LangGraph offers built-in\\nsupport for human-in-the-loop interactions, which is essential for applications\\nthat require manual intervention or approval at certain stages. For example, a\\nhuman operator can review an agent’s planned actions and approve, reject, or\\nmodify them before the agent proceeds. This level of control makes LangGraph\\nsuitable for high-stakes domains like healthcare or legal advice, where accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 7}, page_content='modify them before the agent proceeds. This level of control makes LangGraph\\nsuitable for high-stakes domains like healthcare or legal advice, where accuracy\\nand oversight are critical.\\nAdditionally, LangGraph supports streaming outputs from each node as they\\nare produced. This capability is especially useful for applications like chatbots'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 8}, page_content='LangChain\\n9\\nor real-time monitoring systems, where immediate feedback improves user expe-\\nrience. Streaming can be implemented within any node in the graph, enabling\\nreal-time updates as actions are processed [7].\\n3.2\\nLangGraph Platform\\nThe LangGraph Platform [8] is an infrastructure solution that extends the\\nopen-source LangGraph framework for production deployments. It includes com-\\nponents like LangGraph Server (for API access), LangGraph SDKs (client li-\\nbraries), and LangGraph CLI (a command-line interface for deployment manage-\\nment). The platform is designed to handle complex agent workflows, supporting\\nlong-running agents, background processing, and task queuing to ensure reliable\\nperformance even under heavy loads. The LangGraph Platform also includes\\nfeatures such as:\\n– Background Execution: Allows agents to run asynchronously, handling\\nuser requests in parallel without blocking other tasks.\\n– Support for Long-Running Agents: Provides infrastructure for agents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 8}, page_content='– Background Execution: Allows agents to run asynchronously, handling\\nuser requests in parallel without blocking other tasks.\\n– Support for Long-Running Agents: Provides infrastructure for agents\\nthat need to operate over extended periods, managing resource allocation\\nand monitoring agent health.\\n– Burst Handling and Task Queues: Uses queues to manage sudden in-\\ncreases in requests, ensuring that high-priority tasks are processed efficiently.\\n3.3\\nLangGraph Workflow\\nA typical LangGraph workflow begins by defining the state schema and nodes\\nrequired for the application. Each node represents an independent function, such\\nas calling an LLM, invoking a tool, or accessing external data. The developer sets\\nan entry point for graph execution and defines the transitions (edges) between\\nnodes, which can be conditional or sequential based on application requirements.\\n– Defining Nodes and State: Developers initialize nodes, such as an LLM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 8}, page_content='nodes, which can be conditional or sequential based on application requirements.\\n– Defining Nodes and State: Developers initialize nodes, such as an LLM\\nnode for responses or a tool node for external API calls, and specify the state\\nschema to manage conversation context.\\n– Setting Entry Points and Edges: Nodes are connected by edges, with\\nconditions determining the flow based on the application’s state.\\n– Compiling and Executing the Graph: Once nodes and edges are defined,\\nthe graph is compiled into a runnable format, enabling calls to functions such\\nas invoke() for execution and stream() for real-time updates.\\nLangGraph’s workflow design allows applications to cycle between nodes\\nbased on input conditions and dynamically update state, enabling applications\\nthat require complex interaction patterns.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 9}, page_content='10\\nVasilios Mavroudis\\n3.4\\nIntegration with LangChain and LangSmith\\nLangGraph integrates seamlessly with LangChain and LangSmith, although\\nit operates independently if desired. For users within the LangChain ecosys-\\ntem, LangGraph can utilize LangSmith’s tracing capabilities to monitor each\\nnode’s performance and capture detailed logs of agent interactions. Addition-\\nally, LangChain tools and APIs can be incorporated as graph nodes, expanding\\nLangGraph’s functionality with LangChain’s extensive library of tools and con-\\nnectors [7].\\n4\\nLangServe\\nLangServe [5] is an integral component of the LangChain ecosystem, specifically\\ndesigned to facilitate the deployment of large language model (LLM) applications\\nas scalable REST APIs. With LangServe, developers can create production-\\ngrade APIs that allow external systems and users to interact with LangChain\\napplications. It enables LLM-powered applications to be served in real-time with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 9}, page_content='grade APIs that allow external systems and users to interact with LangChain\\napplications. It enables LLM-powered applications to be served in real-time with\\nrobust support for load balancing, monitoring, and scalability.\\n4.1\\nCore Features of LangServe\\nAPI Deployment and Management LangServe simplifies the process of\\nturning LangChain applications into APIs, making LLM models accessible for\\nvarious services and client applications. With LangServe, any LangChain work-\\nflow can be packaged and exposed via RESTful endpoints, enabling interactions\\nwith language models, data retrieval, and external tool integration. The API-\\ncentric design allows LangServe to support diverse use cases, from chatbots and\\nrecommendation systems to complex multi-agent interactions. It provides several\\ntools for managing API endpoints, such as configurable routing, request han-\\ndling, and response formatting, which make it easy to customize each endpoint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 9}, page_content='tools for managing API endpoints, such as configurable routing, request han-\\ndling, and response formatting, which make it easy to customize each endpoint\\nbased on application requirements. This flexibility allows developers to design\\nAPIs with specific functionalities, making LangServe suitable for applications of\\nvarying complexity [5].\\nScalability and Load Balancing LangServe includes built-in support for scal-\\nability, making it ideal for applications that experience high traffic volumes. It\\ncan handle multiple API requests simultaneously, ensuring consistent perfor-\\nmance by balancing the load across instances. This feature is critical for pro-\\nduction environments where maintaining low response times under heavy loads\\nis essential. To further enhance scalability, LangServe provides tools for setting\\nup auto-scaling, which dynamically adjusts the number of instances based on\\ndemand. This allows applications to handle traffic spikes without degradation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 9}, page_content='up auto-scaling, which dynamically adjusts the number of instances based on\\ndemand. This allows applications to handle traffic spikes without degradation\\nin performance, making LangServe suitable for real-world deployments with un-\\npredictable usage patterns [5].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 10}, page_content='LangChain\\n11\\nLatency and Error Management LangServe is designed to minimize latency,\\nensuring quick responses for each API request. It employs efficient request queu-\\ning and processing mechanisms to reduce waiting times. Additionally, LangServe\\nincludes error handling and retry logic, which helps maintain reliability by man-\\naging transient failures and minimizing downtime. This focus on latency and\\nerror resilience makes LangServe suitable for mission-critical applications that\\nrequire high availability. For instance, LangServe can automatically retry failed\\nrequests and log errors for further investigation, allowing developers to identify\\nissues promptly and maintain a stable API response rate [5].\\n4.2\\nLangServe Workflow\\nThe LangServe deployment workflow is straightforward, enabling developers to\\ngo from a LangChain application to a deployed API in a few steps. Here’s an\\noutline of a typical LangServe workflow:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 10}, page_content='The LangServe deployment workflow is straightforward, enabling developers to\\ngo from a LangChain application to a deployed API in a few steps. Here’s an\\noutline of a typical LangServe workflow:\\n1. Defining Endpoints: Developers define endpoints based on application re-\\nquirements, specifying which functions or models are exposed via API routes.\\nEach endpoint can have customized parameters, allowing for flexibility in\\nhow the API interacts with different components.\\n2. Configuring Request Handling and Routing: LangServe allows for fine-\\ngrained control over how requests are processed. Developers can set up rout-\\ning rules, parameter validation, and request parsing to tailor the API expe-\\nrience.\\n3. Setting Up Load Balancing and Scaling: For applications with high\\ntraffic, LangServe’s load balancing can be configured to distribute requests\\nacross multiple instances, ensuring consistent response times. Auto-scaling\\ncan also be set up to dynamically adjust resources based on demand.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 10}, page_content='across multiple instances, ensuring consistent response times. Auto-scaling\\ncan also be set up to dynamically adjust resources based on demand.\\n4. Monitoring and Error Tracking: LangServe integrates with monitoring\\ntools, including LangSmith, to provide real-time insights into API perfor-\\nmance, usage metrics, and error rates. This monitoring helps developers\\nmaintain optimal performance and quickly resolve issues as they arise.\\nLangServe’s streamlined workflow ensures that developers can deploy robust\\nAPIs with minimal overhead, making it a practical choice for scaling LLM ap-\\nplications in production environments.\\n4.3\\nIntegration with LangSmith and LangChain\\nLangServe integrates seamlessly with LangSmith, which offers observability fea-\\ntures like tracing, logging, and performance monitoring. Through LangSmith,\\nLangServe users can track metrics such as request frequency, latency, and er-\\nror rates. This integration provides a comprehensive view of API performance,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 10}, page_content='LangServe users can track metrics such as request frequency, latency, and er-\\nror rates. This integration provides a comprehensive view of API performance,\\nenabling developers to optimize applications based on real-time data.\\nAdditionally, LangServe’s integration with LangChain allows developers to\\nleverage LangChain’s extensive library of tools, models, and connectors as part'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 11}, page_content='12\\nVasilios Mavroudis\\nof the API. LangChain workflows, tools, and chains can be directly exposed\\nvia LangServe endpoints, providing flexible API interactions and enhancing the\\nfunctionality of LLM applications [5].\\n5\\nLimitations and Criticisms\\nLangChain provides a versatile framework for the development of applications\\npowered by large language models (LLMs). However, several limitations warrant\\nattention, especially in the domains of complexity and security.\\n5.1\\nComplexity\\nLangChain’s modular architecture, while designed to simplify LLM-based ap-\\nplication development, can paradoxically increase complexity. Effective use of\\nLangChain often requires a nuanced understanding of its distinct components,\\nsuch as LangGraph and LangSmith, as well as familiarity with its API ecosys-\\ntem. Consequently, developers may face a steep learning curve, particularly those\\naiming for rapid prototyping or deployment. The need for comprehensive knowl-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 11}, page_content='tem. Consequently, developers may face a steep learning curve, particularly those\\naiming for rapid prototyping or deployment. The need for comprehensive knowl-\\nedge of each module may present a barrier to new users, complicating onboarding\\nand initial implementation phases.\\n5.2\\nSecurity Concerns\\nGiven LangChain’s modular design and extensive reliance on external integra-\\ntions, security presents a notable challenge. Although LangChain incorporates a\\nrange of security measures, including fine-grained permission control and sand-\\nboxing, the complexity of securing LLM-based applications remains high, espe-\\ncially for applications managing sensitive data. Below, we outline several critical\\nsecurity risks associated with LangChain and explore strategies for risk mitiga-\\ntion.\\nRisks Associated with External Providers To enhance functionality, LangChain\\nintegrates with numerous external services, such as vector databases, API providers,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 11}, page_content='tion.\\nRisks Associated with External Providers To enhance functionality, LangChain\\nintegrates with numerous external services, such as vector databases, API providers,\\nand cloud storage platforms. However, these integrations expose applications to\\nsecurity vulnerabilities:\\n– Data Exposure: Accessing external resources can inadvertently expose sen-\\nsitive data to third-party providers, a risk particularly relevant for applica-\\ntions handling personal or confidential information. Without stringent data\\nencryption and access control mechanisms, the potential for data leaks or\\nunauthorized access increases.\\n– Third-Party Dependency: Reliance on third-party services introduces de-\\npendencies on their security protocols. Any compromise within a provider’s\\ninfrastructure could affect LangChain applications, resulting in data breaches\\nor service interruptions. This underscores the importance of thoroughly vet-\\nting providers and monitoring them for potential security issues.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 12}, page_content='LangChain\\n13\\nLangChain’s security model addresses many of these concerns, yet challenges\\npersist, particularly in sectors with rigorous compliance standards, such as fi-\\nnance and healthcare. Key areas for ongoing improvement include:\\n– Dynamic Permission Adjustment: Current permission settings in LangChain\\nare defined at deployment, but in dynamic applications, permissions may\\nneed to adapt based on user interactions. Implementing adaptive permis-\\nsions responsive to application state or user roles could enhance security.\\n– Advanced Encryption Standards: For applications processing highly\\nsensitive data, adopting advanced encryption practices—such as end-to-end\\nor field-level encryption—could bolster data security within even trusted\\nenvironments.\\n– Proactive Security Analytics: Integrating predictive analytics to pre-\\nemptively identify risks could further secure applications. Machine learning\\nmodels analyzing application logs could flag anomalous patterns indicative'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 12}, page_content='emptively identify risks could further secure applications. Machine learning\\nmodels analyzing application logs could flag anomalous patterns indicative\\nof potential breaches or misuse.\\nIn summary, LangChain’s security framework includes robust features such\\nas granular permissions, sandboxing, and real-time monitoring. While these mea-\\nsures provide a solid foundation, the ongoing challenge of securing LLM-driven\\napplications—particularly those relying on external providers—demands contin-\\nued advancements in security practices.\\n6\\nConclusion\\nLangChain significantly advances the development of applications powered by\\nlarge language models (LLMs). Its modular framework—including components\\nlike LangGraph for stateful process modeling, LangServe for scalable API de-\\nployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 12}, page_content='ployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-\\nverse domains, including NLP, cybersecurity, healthcare, finance, and customer\\nservice.\\nWhile its versatility extends beyond NLP, allowing for applications in fields\\nlike cybersecurity (e.g., threat detection and automated incident response), the\\nframework’s emphasis on flexibility introduces complexities that may present a\\nlearning curve for developers new to LangChain. Additionally, reliance on exter-\\nnal integrations raises important security considerations, such as data exposure\\nand dependency vulnerabilities, which are critical in sensitive areas where data\\nintegrity and privacy are paramount.\\nIn summary, LangChain’s transformative potential lies in bridging the gap\\nbetween the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 12}, page_content='between the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-\\nments in usability and security, LangChain can continue to serve as a valuable\\ntool for developers seeking to leverage LLMs in building innovative and secure\\napplications. As industries increasingly adopt AI technologies, frameworks like\\nLangChain are poised to play a pivotal role in shaping the next generation of\\nintelligent, scalable, and secure solutions across various sectors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 13}, page_content='14\\nVasilios Mavroudis\\nReferences\\n1. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-\\nrencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\\nAnadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.\\n2. Harrison Chase.\\nLangChain, Oct 2022.\\nAvailable at https://github.com/\\nlangchain-ai/langchain.\\n3. LangChain, Inc. LangChain Documentation: Integration Providers. LangChain,\\nInc., San Francisco, CA, 2024. Available at https://python.langchain.com/docs/\\nintegrations/providers/.\\n4. LangChain, Inc.\\nLangChain Documentation: Key Concepts.\\nLangChain, Inc.,\\nSan Francisco, CA, 2024.\\nAvailable at https://python.langchain.com/docs/\\nconcepts/.\\n5. LangChain, Inc.\\nLangChain Documentation: LangServe.\\nLangChain, Inc.,\\nSan Francisco, CA, 2024.\\nAvailable at https://python.langchain.com/docs/\\nlangserve/.\\n6. LangChain, Inc. LangChain Documentation: Security Best Practices. LangChain,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 13}, page_content='LangChain, Inc.,\\nSan Francisco, CA, 2024.\\nAvailable at https://python.langchain.com/docs/\\nlangserve/.\\n6. LangChain, Inc. LangChain Documentation: Security Best Practices. LangChain,\\nInc., San Francisco, CA, 2024. Available at https://python.langchain.com/docs/\\nsecurity/.\\n7. LangChain, Inc. LangGraph: Building Language Agents as Graphs, 2024. Accessed:\\n2024-11-04.\\n8. LangChain, Inc. LangGraph Platform Documentation, 2024. Accessed: 2024-11-04.\\n9. LangChain, Inc. LangSmith: A Developer Platform for LLM Applications, 2024.\\nAccessed: 2024-11-04.\\n10. Patrick\\nLewis,\\nEthan\\nPerez,\\nAleksandra\\nPiktus,\\nFabio\\nPetroni,\\nVladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.\\nAdvances in Neural Information Processing Systems, 33:9459–9474, 2020.\\n11. Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 13}, page_content='Advances in Neural Information Processing Systems, 33:9459–9474, 2020.\\n11. Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan\\nHorn, Naty Leiser, and Grzegorz Czajkowski. Pregel: A System for Large-Scale\\nGraph Processing. In Proceedings of the 2010 ACM SIGMOD International Con-\\nference on Management of Data, pages 135–146, 2010.\\n12. OpenAI. Hello GPT-4O, 05 2024.\\n13. OpenAI. Introducing OpenAI O1-Preview, 09 2024.\\n14. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui\\nYu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican,\\net al. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint\\narXiv:2312.11805, 2023.\\n15. The Apache Software Foundation. Apache Beam: An Advanced Unified Program-\\nming Model, 2024. Accessed: 2024-11-04.\\n16. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-10T11:43:58+00:00', 'source': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'file_path': '..\\\\data\\\\pdf\\\\LangChain.pdf', 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-10T11:43:58+00:00', 'trapped': '', 'modDate': 'D:20241110114358Z', 'creationDate': 'D:20241110114358Z', 'page': 13}, page_content='ming Model, 2024. Accessed: 2024-11-04.\\n16. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv\\npreprint arXiv:2302.13971, 2023.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 0}, page_content='ASTRONOMICAL DATA ANALYSIS SOFTWARE AND SYSTEMS XIV\\nASP Conference Series, Vol. 347, 2005\\nP. L. Shopbell, M. C. Britton, and R. Ebert, eds.\\nmatplotlib – A Portable Python Plotting Package\\nPaul Barrett\\nSpace Telescope Science Institute\\nJohn Hunter\\nUniversity of Chicago\\nJ. Todd Miller, Jin-Chung Hsu, and Perry Greenﬁeld\\nSpace Telescope Science Institute\\nAbstract.\\nmatplotlib is a portable 2D plotting and imaging package aimed\\nprimarily at visualization of scientiﬁc, engineering, and ﬁnancial data.\\nmat-\\nplotlib can be used interactively from the Python shell, called from python\\nscripts, or embedded in a GUI application (GTK, Wx, Tk, Windows). Many\\npopular hardcopy outputs are supported including JPEG, PNG, PostScript and\\nSVG. Features include the creation of multiple axes and ﬁgures per page, inter-\\nactive navigation, many predeﬁned line styles and symbols, images, antialiasing,\\nalpha blending, date and ﬁnancial plots, W3C compliant font management and'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 0}, page_content='active navigation, many predeﬁned line styles and symbols, images, antialiasing,\\nalpha blending, date and ﬁnancial plots, W3C compliant font management and\\nFreeType2 support, legends and tables, pseudocolor plots, mathematical text\\nand more. It works with both numarray and Numeric. The goals of the pack-\\nage, basic architecture, current features (illustrated with examples), and planned\\nenhancements will be described.\\n1.\\nIntroduction\\nmatplotlib is designed with the philosophy that you should be able to create\\nsimple plots with just a few commands, or just one!\\nIf you want to see a\\nhistogram of your data, you shouldn’t need to instantiate objects, call methods,\\nset properties, etc; it should just work.\\nThe initial goals of matplotlib were:\\n• Plots should be publication quality; particularly the text (antialiased, ro-\\ntated, etc.).\\n• PostScript output for inclusion with TEX documents.\\n• Embeddable in a graphical user interface for application development.\\n• Code should be understandable.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 0}, page_content='tated, etc.).\\n• PostScript output for inclusion with TEX documents.\\n• Embeddable in a graphical user interface for application development.\\n• Code should be understandable.\\n• Making plots should be easy.\\n• The software is Open Source, so it can be downloaded, used, and dis-\\ntributed freely.\\nmatplotlib can be used in a variety of settings. Most users are familiar with\\nthe command-line for interactively creating plots and images.\\nThis interface\\nprovides a simple pop-up window for displaying and manipulating the data.\\nHowever, the true power of matplotlib is the underlying plotting library, which\\n91'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 1}, page_content='92\\nBarrett et al.\\n\\x00\\x02\\x01\\x02\\x01\\x02\\x01\\n\\x00\\x02\\x01\\x02\\x03\\x02\\x01\\n\\x00\\x02\\x01\\x05\\x04\\x06\\x01\\n\\x00\\x02\\x01\\x02\\x07\\x02\\x01\\n\\x00\\x02\\x01\\x05\\x08\\x06\\x01\\n)\\n\\t\\nA\\n('),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 1}, page_content='\\x01\\n\\x03\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x04\\x02\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x07\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x08\\x02\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x00\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x03\\n\\x00\\x02\\x10\\n\\x03\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x03\\n\\x00\\x02\\x10\\n\\x04\\x02\\x0b\\x11\\x0c\\x12\\x00\\x02\\x03\\n\\x13\\x14\\n\\x15\\n\\x16\\n\\x17\\x19\\x18\\x1b\\x1a\\x1d\\x1c\\x1f\\x1e\\x06 !\\x17#\"%$\\'&\\x11(*),+.-0/.1*243\\r56\\x1c*78$:9*;\\nFigure 1.\\nA line plot of FITS binary table data containing 10k points.\\nis operating system independent and graphical user interface (GUI) agnostic. It\\ncan be used without a GUI as part of a web server to create plots and images in\\na variety of hardcopy outputs; or can be embedded in a larger application using\\none of several GUIs (e.g. GTK, Tk, or WXwindows) running on one of several\\nOSs (e.g. Windows, OS X, Solaris, and Linux).\\n2.\\nArchitecture\\nThe matplotlib code is conceptually divided into three parts:\\n• The matlab interface is the set of functions that allow a user to create\\nplots from the command line.\\n• The frontend or matplotlib API is the set of classes that do the heavy\\nlifting by creating and managing ﬁgures, text, lines, plots, etc. This is the\\nabstract interface that knows nothing about output.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 1}, page_content='lifting by creating and managing ﬁgures, text, lines, plots, etc. This is the\\nabstract interface that knows nothing about output.\\n• The backends are device dependent drawing devices or renderers that\\ntransform the frontend representation to hardcopy (JPEG, PNG, PDF,\\nPS, SVG, Paint, GD) or a display device (Agg, GTK/GTKAgg, TkAgg,\\nWX/WXAgg). Much of the critical rendering code is written in C/C++\\nand therefore provides very good performance.\\nAgg is the Anti-Grain Graphics library that enables writing vector graphics\\nto a buﬀer, which can then be block transfered (or BLTed) to the display de-\\nvice. This means that all interactive implementations based on Agg avoid the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 2}, page_content='matplotlib – A Portable Python Plotting Package\\n93\\n<\\x0e=?>\\n@BA\\nCED\\nF0G\\n@\\nCED\\nF0GIH\\nCJD\\nF\\x19K\\nA\\nCED\\nF\\n@\\x12L\\nM\\n=?N\\nG\\nK\\nM\\n=?N\\nG\\x12O\\nM\\n=\\nN\\nK\\nP\\nM\\n=?N\\n@\\n@\\nM\\n=?N\\n@IH\\nQSR\\nN\\nG\\x12P\\nQSR\\nN\\nK\\n@\\nQSR\\nN\\nK\\nH\\nT%U\\nT%V\\nT%W\\nT%X\\nY%Z\\nY\\\\[\\nY%T\\nFigure 2.\\nA ﬁnancial plot that uses the daily high, low, and closing values\\nof a stock price.\\ngraphical limitations of the GUI and render identical graphics regardless of the\\nGUI interface.\\n3.\\nPlotting\\nThe following Python session uses the matlab interface to create a quicklook\\nspectrum of FUSE data (see Figure 1).\\n> python\\nPython 2.3.3 (#1, Jan\\n5 2004, 16:22:13)}\\n[GCC 2.96 20000731 (Red Hat Linux 7.3 2.96-113)] on linux2\\nType \"help\", \"copyright\", \"credits\" or \"license\"\\nfor more information.\\n>>> import pyfits\\n>>> fits = pyfits.open(’fuse.fits’)\\n>>> wave = fits[1].data.field(’wave’)\\n>>> flux = fits[1].data.field(’flux’)\\n>>> from matplotlib.matlab import *\\n>>> ylim(0, 1.5e-12)\\n>>> xlim(985, 1085)\\n>>> xlabel(r’$\\\\lambda\\\\ (\\\\angstrom)$’)\\n<matplotlib.text.Text instance at 0x41118f8c>'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 2}, page_content='>>> flux = fits[1].data.field(’flux’)\\n>>> from matplotlib.matlab import *\\n>>> ylim(0, 1.5e-12)\\n>>> xlim(985, 1085)\\n>>> xlabel(r’$\\\\lambda\\\\ (\\\\angstrom)$’)\\n<matplotlib.text.Text instance at 0x41118f8c>\\n>>> ylabel(r’Flux’)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 3}, page_content='94\\nBarrett et al.\\n]\\n^_]%]\\n`%].]\\na%]%]\\nb%]%]\\ncS]%]\\nd.egf.hjiEiEk\\n]\\n^_].]\\n`%].]\\na%].]\\nb%].]\\nc_].]\\nl\\nmn\\nop\\nq\\nq\\nr\\nsutwvyx\\\\twz|{~}jz\\n\\x7f\\x80z|{\\x19\\x81\\x1dtwvy\\x82jtw}\\x83\\x82\\x1b\\x84\\x85x\\x02\\x81\\nFigure 3.\\nA graphic of FITS image data. Note the labeled axes and title.\\n<matplotlib.text.Text instance at 0x4112108c>\\n>>> title(’FUSE LiF 1A spectrum of EG And’)\\n<matplotlib.text.Text instance at 0x4112420c>\\n>>> plot(wave, flux)\\nOther available plot types are: 2-D vector plots, high-low-close plots (see\\nFigure 2), histogram plots, log plots, pie charts and bar charts.\\n4.\\nImages\\nThe matlab interface has two functions for displaying image data: ﬁgimage,\\nwhich will preserve the size and shape of the image; and imshow, which will\\nresample the image to ﬁt the size of the ﬁgure (see Figure 3). Images can be\\nenhanced by annotations or graphical overlays.\\n5.\\nFeatures\\nKey features that make matplotlib easy to use are:\\n• Integrated support for numarray or Numeric – the Python multi-dimen-\\nsional array libraries.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 3}, page_content='5.\\nFeatures\\nKey features that make matplotlib easy to use are:\\n• Integrated support for numarray or Numeric – the Python multi-dimen-\\nsional array libraries.\\n• The plot window contains a simple interactive GUI with support for pan-\\nand-zoom, history recall, and saving to hardcopy.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.54', 'creator': 'xpdf/pdftops 3.01', 'creationdate': 'D:20060912114242', 'source': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'file_path': '..\\\\data\\\\pdf\\\\matplotlib.pdf', 'total_pages': 5, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': 'D:20060912114242', 'trapped': '', 'modDate': 'D:20060912114242', 'creationDate': 'D:20060912114242', 'page': 4}, page_content='matplotlib – A Portable Python Plotting Package\\n95\\n• The command line interface is modeled after the easy to use MatLab in-\\nterface.\\n• Support for multiple plots and images per page.\\n• TrueType/FreeType fonts are available in the GD, Agg, Paint, and Post-\\nScript backends. SVG support is coming soon.\\n• Mathematical text ala TEX math mode is available whenever TrueType\\nfonts are available.\\n• Images are automatically resampled to the size of the ﬁgure.\\n• A fully object-oriented design to ease programming and development.\\n6.\\nEnchancements\\nEnhancements to matplotlib that are expected in the near future are:\\n• Contour plots which can be used for image overlays.\\n• The ability to handle general 2-D transforms, which are useful for map\\nprojections and world coordinate systems.\\nTo learn more about matplotlib and to download the latest version, go to\\nhttp://matplotlib.sourceforge.net.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 0}, page_content='Copyright: © the author(s), publisher and licensee Technoscience Academy. This is an open-access article distributed under the \\nterms of the Creative Commons Attribution Non-Commercial License, which permits unrestricted non-commercial use,\\ndistribution, and reproduction in any medium, provided the original work is properly cited \\n \\nInternational Journal of Scientific Research in Computer Science, Engineering and Information Technology \\nISSN : 2456-3307 (www.ijsrcseit.com) \\ndoi : https://doi.org/10.32628/CSEIT2173105 \\n \\n \\n \\n \\n \\n67 \\nStudy On Machine Learning Algorithms \\nPraba. R1, Darshan. G2, Roshanraj. K. T2, Surya Prakash. B2 \\n1Assistant Professor, Department of Information Technology, Dr.N.G.P. Arts and Science College, Coimbatore, \\nTamil Nadu, India \\n2UG Candidate, Department of Information Technology, Dr.N.G.P. Arts and Science College, Coimbatore, \\nTamil Nadu, India \\n \\n \\n \\nArticle Info \\nVolume  7, Issue 4 \\nPage Number: 67-72 \\nPublication Issue : \\nJuly-August-2021'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 0}, page_content='Tamil Nadu, India \\n \\n \\n \\nArticle Info \\nVolume  7, Issue 4 \\nPage Number: 67-72 \\nPublication Issue : \\nJuly-August-2021 \\nArticle History \\nAccepted :  02 July 2021 \\nPublished : 08 July 2021 \\nABSTRACT \\n \\nVarious machine learning algorithms are described in this work. These \\nalgorithms are used for a variety of applications, including data mining, image \\nprocessing, predictive analytics, and so on. The fundamental benefit of \\nemploying machine learning is that once an algorithm learns what to do with \\ndata, it can complete tasks on its own. \\n \\nKeywords : Machine Learning, Algorithms \\n  \\n \\nI. INTRODUCTION \\n \\nMachine learning is used to teach machines how to \\nhandle the data more efficiently. We may be unable \\nto interpret the pattern or extract information from \\nthe data after examining it.In that case, we apply \\nmachine learning. With the abundance of datasets \\navailable, the demand for machine learning is in rise. \\nMany industries from medicine to military apply'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 0}, page_content='machine learning. With the abundance of datasets \\navailable, the demand for machine learning is in rise. \\nMany industries from medicine to military apply \\nmachine learning to extract relevant information. \\n \\nThe phrase \"Machine Learning\" was coined by \\nArthur Samuel, a pioneer in the fields of artificial \\nintelligence and computer games. Machine learning, \\nhe stated, is a “field of research that enables \\ncomputers \\nto \\nlearn \\nwithout \\nbeing \\nexplicitly \\nprogrammed.” \\n \\nMachine Learning (ML) can be defined as the process \\nof automating and refining the learning process of \\ncomputers based on their experiences without the \\nneed for programming, i.e. without the use of \\nhumans. The process begins with providing high-\\nquality data, which is then used to train our machines \\n(computers) by creating machine learning models \\nbased on the data and other methods. The algorithms \\nwe use are determined by the type of data we have \\nand the task we are attempting to automate.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 0}, page_content='(computers) by creating machine learning models \\nbased on the data and other methods. The algorithms \\nwe use are determined by the type of data we have \\nand the task we are attempting to automate. \\n \\nMachine learning (ML) is the study of computer \\nalgorithms that improve themselves over time as a \\nresult of experience and data. 1st It is consideredto be \\na component of artificial intelligence. Machine \\nlearning algorithms create a model based on sample'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 1}, page_content='Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n68 \\ndata, referred to as \"training data,\" in order to make \\npredictions or judgments without being explicitly \\nprogrammed. \\n \\nMachine learning algorithms are utilized in a wide \\nrange of applications, including medicine, email \\nfiltering, and computer vision, where developing \\ntraditional algorithms to do the required tasks is \\ndifficult or impossible. \\n \\nHowever, not all machine learning is statistical \\nlearning. A subset of machine learning is strongly \\nrelated to computational statistics, which focuses on \\nmaking predictions using computers. The discipline \\nof machine learning benefits from the study of \\nmathematical optimization since it provides tools, \\ntheory, and application domains. Data mining is a \\nsimilar \\nbranch \\nof \\nresearch \\nthat \\nfocuses \\non \\nunsupervised learning for exploratory data analysis.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 1}, page_content=\"theory, and application domains. Data mining is a \\nsimilar \\nbranch \\nof \\nresearch \\nthat \\nfocuses \\non \\nunsupervised learning for exploratory data analysis. \\nMachine learning is also known as predictive \\nanalytics when it is used to solve business challenges. \\n \\nII. LITERATURE REVIEW \\n \\nMachine learning is the process of computers figuring \\nout how to do things without being specifically \\nprogrammed to do so. It entails computers learning \\nfrom data in order to do specific jobs. It is possible to \\nbuild algorithms that teach the computer how to \\nperform all steps required to solve the problem at \\nhand for basic jobs entrusted to computers; no \\nlearning is necessary on the computer's behalf. \\nManually creating the required algorithms for more \\ncomplicated tasks can be difficult for a human. In \\npractise, assisting the computer in developing its own \\nalgorithm rather than having human programmers \\nexplain each required step can prove to be more\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 1}, page_content='practise, assisting the computer in developing its own \\nalgorithm rather than having human programmers \\nexplain each required step can prove to be more \\nproductive. Machine learning is a discipline that uses \\na variety of ways to train computers how to complete \\ntasks for which no entirely suitable solution exists. \\nWhen there are a large number of possible replies, \\none strategy is to classify some of the correct \\nresponses as valid.  The computer can then utilize \\nthis as training data to refine the algorithm(s) it uses \\nto determine right answers. The MNIST dataset of \\nhandwritten digits, for example, has frequently been \\nused to train a system for the task of digital character \\nrecognition.  \\n \\nIII. TYPES OF LEARNING \\n \\nA. Supervised Learning \\n \\nSupervised learning algorithms create a mathematical \\nmodel of a set of data that includes both the inputs \\nand the outputs that are sought a The information is \\nreferred to as training data, and it consists of a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 1}, page_content='model of a set of data that includes both the inputs \\nand the outputs that are sought a The information is \\nreferred to as training data, and it consists of a \\ncollection of training instances. Each training \\nexample has one or more inputs and a supervisory \\nsignal as the desired output. Each training sample is \\nrepresented by an array or vector, sometimes referred \\nto as a feature vector, and the training data is \\nrepresented by a matrix in the mathematical model. \\nSupervised learning techniques develop a function \\nthat may be used to predict the output associated \\nwith fresh inputs by iteratively optimizing an \\nobjective function. Active learning, classification, and \\nregression are examples of supervised learning \\nalgorithms. When the outputs are limited to a small \\nset of values, classification techniques are employed, \\nand regression methods are used when the outputs \\ncan have any numerical value within a range.  An \\nincoming email, for example, would be the input to a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 1}, page_content='and regression methods are used when the outputs \\ncan have any numerical value within a range.  An \\nincoming email, for example, would be the input to a \\nclassification algorithm that filters emails, and the \\noutput would be the name of the folder to file the \\nemail in.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 2}, page_content='Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n69 \\n \\nA supervised learning model called a support-vector \\nmachine separates data into areas separated by a \\nlinear border. The black circles are separated from \\nthe white circles by a linear barrier. \\n \\nB.Unsupervised Learning \\nUnsupervised learning methods take a collection of \\ndata with only inputs and detect structure in it, such \\nas data point grouping or clustering. As a result, the \\nalgorithms learn from unlabeled, unclassified, and \\nuncategorized test data.  Unsupervised learning \\nalgorithms discover commonalities in the data and \\nreact depending on the existence or lack of such \\ncommonalities in each new piece of data, rather than \\nresponding to feedback. The field of density \\nestimation in statistics, such as calculating the \\nprobability density function, is a key application of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 2}, page_content='responding to feedback. The field of density \\nestimation in statistics, such as calculating the \\nprobability density function, is a key application of \\nunsupervised learning. Unsupervised learning, on the \\nother hand, comprises various domains that need \\nsummarising and explaining data aspects. \\n \\nCluster analysis divides a set of observations into \\nsubsets (called clusters) so that observations within \\nthe same cluster are comparable based on one or \\nmore predetermined criteria, while observations from \\ndifferent clusters are distinct. Different clustering \\napproaches make different assumptions about the \\nstructure \\nof \\nthe \\ndata, \\nwhich \\nis \\ncommonly \\ncharacterized \\nby \\nsome \\nsimilarity \\nmetric \\nand \\nevaluated, for example, by internal compactness, or \\nthe similarity between cluster members, and \\nseparation, or the difference between clusters. \\nEstimated density and graph connectedness are used \\nin other approaches. \\n \\n \\nC. Semi-supervised Learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 2}, page_content='separation, or the difference between clusters. \\nEstimated density and graph connectedness are used \\nin other approaches. \\n \\n \\nC. Semi-supervised Learning \\nUnsupervised learning (without any labelled training \\ndata) and supervised learning (with labelled training \\ndata) are the two types of learning (with completely \\nlabelled training data).  Although some of the \\ntraining examples lack training labels, several \\nmachine-learning researchers have discovered that \\nunlabeled data, when combined with a modest \\namount of labelled data, can enhance learning \\naccuracy significantly. The training labels in weakly \\nsupervised learning are noisy, limited, or imprecise, \\nyet they are generally cheaper to obtain, resulting in \\nlarger effective training sets. \\n \\nD. Reinforcement learning \\nReinforcement learning is a branch of machine \\nlearning that studies how software agents should \\nbehave in a given environment in order to maximize \\nsome metric of cumulative reward. Game theory,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 2}, page_content='Reinforcement learning is a branch of machine \\nlearning that studies how software agents should \\nbehave in a given environment in order to maximize \\nsome metric of cumulative reward. Game theory, \\ncontrol theory, operations research, information \\ntheory, simulation-based optimization, multi-agent \\nsystems, swarm intelligence, statistics, and genetic \\nalgorithms are among the numerous disciplines that \\nstudy the field due to its generality. The environment \\nis generally represented as a Markov decision process \\nin machine learning (MDP). Dynamic programming \\ntechniques are used in many reinforcement learning \\nsystems. When exact mathematical models of the \\nMDP \\nare \\ninfeasible, \\nreinforcement \\nlearning \\nprocedures are applied. Reinforcement learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 3}, page_content=\"Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n70 \\nalgorithms are employed in autonomous vehicles and \\nin teaching humans how to play a game.  \\n \\nE. Dimensionality Reduction \\nThe technique of lowering the number of random \\nvariables under consideration by generating a set of \\nprimary variables is known as dimensionality \\nreduction. In other words, it's a method of \\nminimising the size of your feature set, also known as \\nthe \\nnumber \\nof \\nfeatures. \\nThe \\nmajority \\nof \\ndimensionality reduction approaches fall into one of \\ntwo categories: feature deletion or extraction. \\nPrincipal component analysis is one of the most \\nwidely used methods for dimensionality reduction.  \\n \\nAnalyze the Principal Components (PCA) \\nPCA entails converting higher-dimensional data \\n(such as 3D) to a smaller space (eg. 2D). This results \\nin a decreased data dimension (2D rather than 3D),\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 3}, page_content='Analyze the Principal Components (PCA) \\nPCA entails converting higher-dimensional data \\n(such as 3D) to a smaller space (eg. 2D). This results \\nin a decreased data dimension (2D rather than 3D), \\nwhile preserving all of the original variables in the \\nmodel and not modifying the data. \\n \\nA supervised learning model called a support-vector \\nmachine separates data into areas separated by a \\nlinear border. The black circles are separated from \\nthe white circles by a linear barrier.  \\n \\nIV. MODELS \\n \\n1.Artificial neural networks \\nArtificial neural networks (ANNs), also known as \\nconnectionist systems, are computing systems that \\nare based on biological neural networks found in \\nanimal brains. Such systems \"learn\" to execute tasks \\nby considering examples, usually without any task-\\nspecific rules being coded. An artificial neural \\nnetwork (ANN) is a model built on a set of connected \\nunits or nodes known as \"artificial neurons,\" which'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 3}, page_content='specific rules being coded. An artificial neural \\nnetwork (ANN) is a model built on a set of connected \\nunits or nodes known as \"artificial neurons,\" which \\nare roughly modelled after the neurons in a biological \\nbrain. Each link, like the synapses in a human brain, \\ncan send information, or a \"signal,\" from one artificial \\nneuron to the next.  \\n \\n2. Decision Tree \\nTo get from observations about an item (represented \\nin the branches) to conclusions about the item\\'s goal \\nvalue, decision tree learning employs a decision tree \\nas a predictive model (represented in the leaves). In \\nstatistics, data mining, and machine learning, it is one \\nof \\nthe \\npredictive \\nmodelling \\nmethodologies. \\nClassification trees are tree models in which the goal \\nvariable can take a discrete set of values; in these tree \\nstructures, leaves indicate class labels and branches \\nrepresent feature combinations that lead to those \\nclass labels.       \\n                  \\n \\n \\n3. Support-Vector Networks'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 3}, page_content='structures, leaves indicate class labels and branches \\nrepresent feature combinations that lead to those \\nclass labels.       \\n                  \\n \\n \\n3. Support-Vector Networks  \\nSVMs, also known as support-vector networks, are a \\ngroup of similar supervised learning algorithms for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 4}, page_content='Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n71 \\nclassification and regression. An SVM training \\nmethod creates a model that predicts whether a new \\nexample falls into one of two categories given a set of \\ntraining examples that are individually labelled as \\nbelonging to one of two categories.  Although \\nmethods such as Platt scaling exist to employ SVM in \\na probabilistic classification environment, an SVM \\ntraining algorithm is a non-probabilistic, binary, \\nlinear classifier. SVMs may perform non-linear \\nclassification as well as linear classification by \\nimplicitly \\nmapping \\ntheir \\ninputs \\ninto \\nhigh-\\ndimensional feature spaces, which is known as the \\nkernel trick. \\n \\n \\n \\n4. Regression analysis \\nRegression analysis is a broad term that refers to a \\nnumber of statistical techniques for estimating the \\nrelationship between input variables and their'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 4}, page_content='kernel trick. \\n \\n \\n \\n4. Regression analysis \\nRegression analysis is a broad term that refers to a \\nnumber of statistical techniques for estimating the \\nrelationship between input variables and their \\nassociated characteristics. Linear regression is the \\nmost frequent type, in which a single line is \\ngenerated to best match the available data using a \\nmathematical criterion such ordinary least squares.  \\n \\n \\n5. Bayesian network \\nA Bayesian network, also known as a belief network \\nor a directed acyclic graphical model, is a \\nprobabilistic graphical model that uses a directed \\nacyclic graph to describe a set of random variables \\nand their conditional independence (DAG).  A \\nBayesian network, for example, could be used to \\nillustrate \\nthe \\nprobability \\ncorrelations \\nbetween \\ndiseases and symptoms. The network may be used to \\ncalculate the chances of certain diseases being present \\nbased on symptoms. There are efficient algorithms \\nfor inference and learning. Dynamic Bayesian \\nnetworks'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 4}, page_content=\"calculate the chances of certain diseases being present \\nbased on symptoms. There are efficient algorithms \\nfor inference and learning. Dynamic Bayesian \\nnetworks \\nare \\nBayesian \\nnetworks \\nthat \\nmodel \\nsequences of variables, such as speech signals or \\nprotein sequences.  \\n \\n \\n \\nV. CONCLUSION \\n \\nThis study examines a number of different machine \\nlearning algorithms. Today, everyone, intentionally \\nor unconsciously, employs machine learning. From \\nonline shopping for a recommended product to \\nuploading images on social networking sites, there's a \\nlot to do. This document provides an overview of the \\nmost widely used machine learning algorithms. \\n \\nVI. REFERENCES \\n \\n[1]. \\nR. Praba, “A Study on Data Science Basics with \\nPython Concepts”, Volume-4, Issue-14, ISSN: \\n2582-3930.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 5}, page_content='Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n72 \\n[2]. \\nW. Richert, L. P. Coelho, “Building Machine \\nLearning \\nSystems \\nwith \\nPython”, \\nPackt \\nPublishing Ltd., ISBN 978-1-78216-140-0 \\n[3]. \\nM. \\nWelling, \\n“A \\nFirst \\nEncounter \\nwith \\nMachineLearning” \\n[4]. \\nM. Bowles, “Machine Learning in Python: \\nEssential Techniques for Predictive Analytics”, \\nJohn Wiley & Sons Inc., ISBN: 978-1-118-\\n96174-2 \\n[5]. \\nS.B. Kotsiantis, “Supervised Machine Learning: \\nA \\nReview \\nof \\nClassification \\nTechniques”, \\nInformatica 31 (2007) 249-268 \\n[6]. \\nL. Rokach, O. Maimon, “Top – Down \\nInduction of Decision Trees Classifiers – A \\nSurvey”, IEEE Transactions on Systems, \\n[7]. \\nD. Lowd, P. Domingos, “Naïve Bayes Models \\nfor Probability Estimation” \\n[8]. \\nhttps://webdocs.cs.ualberta.ca/~greiner/C- \\n651/Homework2_Fall2008.html \\n[9]. \\nD. Meyer, “Support Vector Machines – The'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 5}, page_content='D. Lowd, P. Domingos, “Naïve Bayes Models \\nfor Probability Estimation” \\n[8]. \\nhttps://webdocs.cs.ualberta.ca/~greiner/C- \\n651/Homework2_Fall2008.html \\n[9]. \\nD. Meyer, “Support Vector Machines – The \\nInterface to libsvm in package e1071”, August \\n2015 \\n[10]. S. S. Shwartz, Y. Singer, N. Srebro, “Pegasos: \\nPrimal Estimated sub - Gradient Solver for \\nSVM”, \\nProceedings \\nof \\nthe \\n24th \\nInternationalConference on Machine Learning, \\nCorvallis, OR, 2007 \\n[11]. http://www.simplilearn.com/what-is-machine-\\nlearning-and-why-itmatters- article \\n[12]. P. Harrington, “Machine Learning in action”, \\nManning Publications Co., Shelter Island, New \\nYork, 2012 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCite this article as : \\n \\nPraba. R, Darshan. G, Roshanraj. K. T, Surya Prakash. \\nB, \"Study On Machine Learning Algorithms\", \\nInternational Journal of Scientific Research in \\nComputer Science, Engineering and Information \\nTechnology (IJSRCSEIT), ISSN : 2456-3307, Volume \\n7 Issue 4, pp. 67-72, July-August 2021. Available at'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2021-07-10T19:19:00+05:30', 'source': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ML_ALGO.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'International Journal of Scientific Research in Computer Science, Engineering and Information Technology', 'author': 'TechnoScience Academy;http://technoscienceacademy.com', 'subject': 'Research Paper', 'keywords': 'IJSRCSEIT; http:/ijsrcseit.com', 'moddate': '2021-07-10T19:19:00+05:30', 'trapped': '', 'modDate': \"D:20210710191900+05'30'\", 'creationDate': \"D:20210710191900+05'30'\", 'page': 5}, page_content='International Journal of Scientific Research in \\nComputer Science, Engineering and Information \\nTechnology (IJSRCSEIT), ISSN : 2456-3307, Volume \\n7 Issue 4, pp. 67-72, July-August 2021. Available at \\ndoi : https://doi.org/10.32628/CSEIT2173105         \\n  \\nJournal URL : https://ijsrcseit.com/CSEIT2173105'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 0}, page_content='Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\u2002 |\\u2002 357\\nReview\\nArray programming with NumPy\\nCharles R. Harris1, K. Jarrod Millman2,3,4\\u2009✉, Stéfan J. van\\xa0der Walt2,4,5\\u2009✉, Ralf Gommers6\\u2009✉, \\nPauli Virtanen7,8, David Cournapeau9, Eric Wieser10, Julian Taylor11, Sebastian Berg4, \\nNathaniel J. Smith12, Robert Kern13, Matti Picus4, Stephan Hoyer14, Marten H. van Kerkwijk15, \\nMatthew Brett2,16, Allan Haldane17, Jaime Fernández del Río18, Mark Wiebe19,20,  \\nPearu Peterson6,21,22, Pierre Gérard-Marchant23,24, Kevin Sheppard25, Tyler Reddy26,  \\nWarren Weckesser4, Hameer Abbasi6, Christoph Gohlke27 & Travis E. Oliphant6\\nArray programming provides a powerful, compact and expressive syntax for \\naccessing, manipulating and operating on data in vectors, matrices and \\nhigher-dimensional arrays. NumPy is the primary array programming library for the \\nPython language. It has an essential role in research analysis pipelines in fields as'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 0}, page_content='higher-dimensional arrays. NumPy is the primary array programming library for the \\nPython language. It has an essential role in research analysis pipelines in fields as \\ndiverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials \\nscience, engineering, finance and economics. For example, in astronomy, NumPy was \\nan important part of the software stack used in the discovery of gravitational waves1 \\nand in the first imaging of a black hole2. Here we review how a few fundamental array \\nconcepts lead to a simple and powerful programming paradigm for organizing, \\nexploring and analysing scientific data. NumPy is the foundation upon which the \\nscientific Python ecosystem is constructed. It is so pervasive that several projects, \\ntargeting audiences with specialized needs, have developed their own NumPy-like \\ninterfaces and array objects. Owing to its central position in the ecosystem, NumPy'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 0}, page_content='targeting audiences with specialized needs, have developed their own NumPy-like \\ninterfaces and array objects. Owing to its central position in the ecosystem, NumPy \\nincreasingly acts as an interoperability layer between such array computation \\nlibraries and, together with its application programming interface (API), provides a \\nflexible framework to support the next decade of scientific and industrial analysis.\\nTwo Python array packages existed before NumPy. The Numeric pack-\\nage was developed in the mid-1990s and provided array objects and \\narray-aware functions in Python. It was written in C and linked to stand-\\nard fast implementations of linear algebra3,4. One of its earliest uses was \\nto steer C++ applications for inertial confinement fusion research at \\nLawrence Livermore National Laboratory5. To handle large astronomi-\\ncal images coming from the Hubble Space Telescope, a reimplementa-\\ntion of Numeric, called Numarray, added support for structured arrays,'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 0}, page_content='cal images coming from the Hubble Space Telescope, a reimplementa-\\ntion of Numeric, called Numarray, added support for structured arrays, \\nflexible indexing, memory mapping, byte-order variants, more efficient \\nmemory use, flexible IEEE 754-standard error-handling capabilities, and \\nbetter type-casting rules6. Although Numarray was highly compatible \\nwith Numeric, the two packages had enough differences that it divided \\nthe community; however, in 2005 NumPy emerged as a ‘best of both \\nworlds’ unification7—combining the features of Numarray with the \\nsmall-array performance of Numeric and its rich C API.\\nNow, 15 years later, NumPy underpins almost every Python library \\nthat does scientific or numerical computation8–11, including SciPy12, \\nMatplotlib13, pandas14, scikit-learn15 and scikit-image16. NumPy is a \\ncommunity-developed, open-source library, which provides a mul-\\ntidimensional Python array object along with array-aware functions'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 0}, page_content='community-developed, open-source library, which provides a mul-\\ntidimensional Python array object along with array-aware functions \\nthat operate on it. Because of its inherent simplicity, the NumPy array \\nis the de facto exchange format for array data in Python.\\nNumPy operates on in-memory arrays using the central processing \\nunit (CPU). To utilize modern, specialized storage and hardware, there \\nhas been a recent proliferation of Python array packages. Unlike with \\nthe Numarray–Numeric divide, it is now much harder for these new \\nlibraries to fracture the user community—given how much work is \\nalready built on top of NumPy. However, to provide the community with \\naccess to new and exploratory technologies, NumPy is transitioning \\ninto a central coordinating mechanism that specifies a well defined \\narray programming API and dispatches it, as appropriate, to special-\\nized array implementations.\\nNumPy arrays\\nThe NumPy array is a data structure that efficiently stores and accesses'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 0}, page_content='array programming API and dispatches it, as appropriate, to special-\\nized array implementations.\\nNumPy arrays\\nThe NumPy array is a data structure that efficiently stores and accesses \\nmultidimensional arrays17 (also known as tensors), and enables a wide \\nvariety of scientific computation. It consists of a pointer to memory, \\nalong with metadata used to interpret the data stored there, notably \\n‘data type’, ‘shape’ and ‘strides’ (Fig.\\xa01a).\\nhttps://doi.org/10.1038/s41586-020-2649-2\\nReceived: 21 February 2020\\nAccepted: 17 June 2020\\nPublished online: 16 September 2020\\nOpen access\\n Check for updates\\n1Independent researcher, Logan, UT, USA. 2Brain Imaging Center, University of California, Berkeley, Berkeley, CA, USA. 3Division of Biostatistics, University of California, Berkeley, Berkeley, CA, \\nUSA. 4Berkeley Institute for Data Science, University of California, Berkeley, Berkeley, CA, USA. 5Applied Mathematics, Stellenbosch University, Stellenbosch, South Africa. 6Quansight, Austin,'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 0}, page_content='USA. 4Berkeley Institute for Data Science, University of California, Berkeley, Berkeley, CA, USA. 5Applied Mathematics, Stellenbosch University, Stellenbosch, South Africa. 6Quansight, Austin, \\nTX, USA. 7Department of Physics, University of Jyväskylä, Jyväskylä, Finland. 8Nanoscience Center, University of Jyväskylä, Jyväskylä, Finland. 9Mercari JP, Tokyo, Japan. 10Department of \\nEngineering, University of Cambridge, Cambridge, UK. 11Independent researcher, Karlsruhe, Germany. 12Independent researcher, Berkeley, CA, USA. 13Enthought, Austin, TX, USA. 14Google \\nResearch, Mountain View, CA, USA. 15Department of Astronomy and Astrophysics, University of Toronto, Toronto, Ontario, Canada. 16School of Psychology, University of Birmingham, \\nEdgbaston, Birmingham, UK. 17Department of Physics, Temple University, Philadelphia, PA, USA. 18Google, Zurich, Switzerland. 19Department of Physics and Astronomy, The University of'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 0}, page_content='Edgbaston, Birmingham, UK. 17Department of Physics, Temple University, Philadelphia, PA, USA. 18Google, Zurich, Switzerland. 19Department of Physics and Astronomy, The University of \\nBritish Columbia, Vancouver, British Columbia, Canada. 20Amazon, Seattle, WA, USA. 21Independent researcher, Saue, Estonia. 22Department of Mechanics and Applied Mathematics, Institute \\nof Cybernetics at Tallinn Technical University, Tallinn, Estonia. 23Department of Biological and Agricultural Engineering, University of Georgia, Athens, GA, USA. 24France-IX Services, Paris, \\nFrance. 25Department of Economics, University of Oxford, Oxford, UK. 26CCS-7, Los Alamos National Laboratory, Los Alamos, NM, USA. 27Laboratory for Fluorescence Dynamics, Biomedical \\nEngineering Department, University of California, Irvine, Irvine, CA, USA. ✉e-mail: millman@berkeley.edu; stefanv@berkeley.edu; ralf.gommers@gmail.com'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='358\\u2002 |\\u2002 Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\nReview\\nThe data type describes the nature of elements stored in an array. \\nAn array has a single data type, and each element of an array occupies \\nthe same number of bytes in memory. Examples of data types include \\nreal and complex numbers (of lower and higher precision), strings, \\ntimestamps and pointers to Python objects.\\nThe shape of an array determines the number of elements along \\neach axis, and the number of axes is the dimensionality of the array. \\nFor example, a vector of numbers can be stored as a one-dimensional \\narray of shape N, whereas colour videos are four-dimensional arrays \\nof shape (T,\\xa0M,\\xa0N,\\xa03).\\nStrides are necessary to interpret computer memory, which stores \\nelements linearly, as multidimensional arrays. They describe the num-\\nber of bytes to move forward in memory to jump from row to row, col-\\numn to column, and so forth. Consider, for example, a two-dimensional'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='ber of bytes to move forward in memory to jump from row to row, col-\\numn to column, and so forth. Consider, for example, a two-dimensional \\narray of floating-point numbers with shape (4,\\xa03), where each element \\noccupies 8\\xa0bytes in memory. To move between consecutive columns, \\nwe need to jump forward 8\\xa0bytes in memory, and to access the next row, \\n3\\xa0×\\xa08\\xa0=\\xa024\\xa0bytes. The strides of that array are therefore (24,\\xa08). NumPy \\ncan store arrays in either C or Fortran memory order, iterating first over \\neither rows or columns. This allows external libraries written in those \\nlanguages to access NumPy array data in memory directly.\\nUsers interact with NumPy arrays using ‘indexing’ (to access sub-\\narrays or individual elements), ‘operators’ (for example, +, − and × \\nfor vectorized operations and @ for matrix multiplication), as well \\nas ‘array-aware functions’; together, these provide an easily readable, \\nexpressive, high-level API for array programming while NumPy deals'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='as ‘array-aware functions’; together, these provide an easily readable, \\nexpressive, high-level API for array programming while NumPy deals \\nwith the underlying mechanics of making operations fast.\\nIndexing an array returns single elements, subarrays or elements \\nthat satisfy a specific condition (Fig.\\xa01b). Arrays can even be indexed \\nusing other arrays (Fig.\\xa01c). Wherever possible, indexing that retrieves a \\nsubarray returns a ‘view’ on the original array such that data are shared \\nbetween the two arrays. This provides a powerful way to operate on \\nsubsets of array data while limiting memory usage.\\nTo complement the array syntax, NumPy includes functions that \\nperform vectorized calculations on arrays, including arithmetic, \\nstatistics and trigonometry (Fig.\\xa01d). Vectorization—operating on \\nentire arrays rather than their individual elements—is essential to array \\nprogramming. This means that operations that would take many tens'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='entire arrays rather than their individual elements—is essential to array \\nprogramming. This means that operations that would take many tens \\nof lines to express in languages such as C can often be implemented as \\na single, clear Python expression. This results in concise code and frees \\nusers to focus on the details of their analysis, while NumPy handles \\nlooping over array elements near-optimally—for example, taking \\nstrides into consideration to best utilize the computer’s fast cache \\nmemory.\\nWhen performing a vectorized operation (such as addition) on two \\narrays with the same shape, it is clear what should happen. Through \\n‘broadcasting’ NumPy allows the dimensions to differ, and produces \\nresults that appeal to intuition. A trivial example is the addition of a \\nscalar value to an array, but broadcasting also generalizes to more com-\\nplex examples such as scaling each column of an array or generating \\na grid of coordinates. In broadcasting, one or both arrays are virtually'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='plex examples such as scaling each column of an array or generating \\na grid of coordinates. In broadcasting, one or both arrays are virtually \\nduplicated (that is, without copying any data in memory), so that the \\nshapes of the operands match (Fig.\\xa01d). Broadcasting is also applied \\nwhen an array is indexed using arrays of indices (Fig.\\xa01c).\\nOther array-aware functions, such as sum, mean and maximum, \\nperform element-by-element ‘reductions’, aggregating results across \\none, multiple or all axes of a single array. For example, summing an \\nn-dimensional array over d axes results in an array of dimension n\\xa0−\\xa0d \\n(Fig.\\xa01f).\\nNumPy also includes array-aware functions for creating, reshaping, \\nconcatenating and padding arrays; searching, sorting and counting \\ndata; and reading and writing files. It provides extensive support for \\ngenerating pseudorandom numbers, includes an assortment of prob-\\nability distributions, and performs accelerated linear algebra, using'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='generating pseudorandom numbers, includes an assortment of prob-\\nability distributions, and performs accelerated linear algebra, using \\none of several backends such as OpenBLAS18,19 or Intel MKL optimized \\nfor the CPUs at hand (see Supplementary Methods for more details).\\nAltogether, the combination of a simple in-memory array repre-\\nsentation, a syntax that closely mimics mathematics, and a variety \\nof array-aware utility functions forms a productive and powerfully \\nexpressive array programming language.\\nIn [1]: import numpy as np\\nIn [2]: x = np.arange(12)\\nIn [3]: x = x.reshape(4, 3)\\nIn [4]: x\\nOut[4]:\\narray([[ 0,  1,  2],\\n       [ 3,  4,  5],\\n       [ 6,  7,  8],\\n       [ 9, 10, 11]])\\nIn [5]: np.mean(x, axis=0)\\nOut[5]: array([4.5, 5.5, 6.5])\\nIn [6]: x = x - np.mean(x, axis=0)\\nIn [7]: x\\nOut[7]:\\narray([[-4.5, -4.5, -4.5],\\n       [-1.5, -1.5, -1.5],\\n       [ 1.5,  1.5,  1.5],\\n       [ 4.5,  4.5,  4.5]])\\na Data structure\\ng Example\\nx =\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10 11\\ndata\\ndata type\\nshape'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='Out[7]:\\narray([[-4.5, -4.5, -4.5],\\n       [-1.5, -1.5, -1.5],\\n       [ 1.5,  1.5,  1.5],\\n       [ 4.5,  4.5,  4.5]])\\na Data structure\\ng Example\\nx =\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10 11\\ndata\\ndata type\\nshape\\nstrides\\n8-byte integer\\n(4, 3)\\n(24, 8)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n8\\n9 10 11\\n8 bytes\\nper element\\n3 × 8 = 24 bytes\\nto jump one\\nrow down\\nb Indexing (view)\\n10 11\\n9\\nx[:,1:] →\\nwith slices\\n1\\n2\\n4 5\\n7 8\\n0\\n3\\n6\\nx[:,::2]→\\nwith slices\\nwith steps\\n0\\n2\\n3\\n5\\n6\\n8\\n9\\n11\\n0 1\\n2\\n3 4\\n5\\n6\\n7 8\\n9 10\\n10 11\\nSlices are start:end:step,\\nany of which can be left blank\\nd Vectorization\\n+\\n→\\n0\\n1\\n3\\n4\\n6\\n7\\n9 10\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n4\\n5\\n7\\n8\\n10 11\\ne Broadcasting\\n×\\n3\\n6\\n0\\n9\\n1\\n2\\n→\\n0\\n0\\n3\\n6\\n6 12\\n9 18\\nf Reduction\\n0\\n1\\n3\\n4\\n6\\n7\\n9 10\\n2\\n5\\n8\\n11\\n3\\n12\\n21\\n30\\nsum\\naxis 1\\n18 22 26\\nsum\\naxis 0\\n66\\nsum\\naxis (0,1)\\nc Indexing (copy)\\n4\\n3\\n7\\n6\\nwith arrays\\nwith broadcasting\\n→\\nx\\n→\\n,\\n2\\n1\\n1\\n0\\nx\\n,\\n1 1\\n2 2\\n1 0\\n1 0\\nx \\nwith arrays\\nx[0,1],x[1,2]\\n1\\n5\\n→\\n→\\n0\\n1\\n1\\n2\\n,\\nx[x > 9]\\nwith masks\\n10 11\\n→\\n→5\\nwith scalars\\nx[1,2]'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='c Indexing (copy)\\n4\\n3\\n7\\n6\\nwith arrays\\nwith broadcasting\\n→\\nx\\n→\\n,\\n2\\n1\\n1\\n0\\nx\\n,\\n1 1\\n2 2\\n1 0\\n1 0\\nx \\nwith arrays\\nx[0,1],x[1,2]\\n1\\n5\\n→\\n→\\n0\\n1\\n1\\n2\\n,\\nx[x > 9]\\nwith masks\\n10 11\\n→\\n→5\\nwith scalars\\nx[1,2] \\nFig. 1 | The NumPy array incorporates several fundamental array concepts. \\na, The NumPy array data structure and its associated metadata fields.  \\nb, Indexing an array with slices and steps. These operations return a ‘view’ of \\nthe original data. c, Indexing an array with masks, scalar coordinates or other \\narrays, so that it returns a ‘copy’ of the original data. In the bottom example, an \\narray is indexed with other arrays; this broadcasts the indexing arguments \\nbefore performing the lookup. d, Vectorization efficiently applies operations \\nto groups of elements. e, Broadcasting in the multiplication of two-dimensional \\narrays. f, Reduction operations act along one or more axes. In this example,  \\nan array is summed along select axes to produce a vector, or along two axes'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 1}, page_content='arrays. f, Reduction operations act along one or more axes. In this example,  \\nan array is summed along select axes to produce a vector, or along two axes \\nconsecutively to produce a scalar. g, Example NumPy code, illustrating some of \\nthese concepts.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 2}, page_content='Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\u2002 |\\u2002 359\\nScientific Python ecosystem\\nPython is an open-source, general-purpose interpreted programming \\nlanguage well suited to standard programming tasks such as cleaning \\ndata, interacting with web resources and parsing text. Adding fast array \\noperations and linear algebra enables scientists to do all their work \\nwithin a single programming language—one that has the advantage of \\nbeing famously easy to learn and teach, as witnessed by its adoption \\nas a primary learning language in many universities.\\nEven though NumPy is not part of Python’s standard library, it ben-\\nefits from a good relationship with the Python developers. Over the \\nyears, the Python language has added new features and special syntax \\nso that NumPy would have a more succinct and easier-to-read array \\nnotation. However, because it is not part of the standard library, NumPy \\nis able to dictate its own release policies and development patterns.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 2}, page_content='notation. However, because it is not part of the standard library, NumPy \\nis able to dictate its own release policies and development patterns.\\nSciPy and Matplotlib are tightly coupled with NumPy in terms of his-\\ntory, development and use. SciPy provides fundamental algorithms for \\nscientific computing, including mathematical, scientific and engineer-\\ning routines. Matplotlib generates publication-ready figures and visu-\\nalizations. The combination of NumPy, SciPy and Matplotlib, together \\nwith an advanced interactive environment such as IPython20 or Jupy-\\nter21, provides a solid foundation for array programming in Python. The \\nscientific Python ecosystem (Fig.\\xa02) builds on top of this foundation to \\nprovide several, widely used technique-specific libraries15,16,22, that in \\nturn underlie numerous domain-specific projects23–28. NumPy, at the \\nbase of the ecosystem of array-aware libraries, sets documentation \\nstandards, provides array testing infrastructure and adds build sup-'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 2}, page_content='base of the ecosystem of array-aware libraries, sets documentation \\nstandards, provides array testing infrastructure and adds build sup-\\nport for Fortran and other compilers.\\nMany research groups have designed large, complex scientific librar-\\nies that add application-specific functionality to the ecosystem. For \\nexample, the eht-imaging library29, developed by the Event Horizon \\nTelescope collaboration for radio interferometry imaging, analysis \\nand simulation, relies on many lower-level components of the scientific \\nPython ecosystem. In particular, the EHT collaboration used this library \\nfor the first imaging of a black hole. Within eht-imaging, NumPy arrays \\nare used to store and manipulate numerical data at every step in the \\nprocessing chain: from raw data through calibration and image recon-\\nstruction. SciPy supplies tools for general image-processing tasks such \\nas filtering and image alignment, and scikit-image, an image-processing'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 2}, page_content='struction. SciPy supplies tools for general image-processing tasks such \\nas filtering and image alignment, and scikit-image, an image-processing \\nlibrary that extends SciPy, provides higher-level functionality such \\nas edge filters and Hough transforms. The ‘scipy.optimize’ module \\nperforms mathematical optimization. NetworkX22, a package for com-\\nplex network analysis, is used to verify image comparison consistency. \\nAstropy23,24 handles standard astronomical file formats and computes \\ntime–coordinate transformations. Matplotlib is used to visualize data \\nand to generate the final image of the black hole.\\nThe interactive environment created by the array\\xa0programming foun-\\ndation and the surrounding ecosystem of tools—inside of IPython or \\nJupyter—is ideally suited to exploratory data analysis. Users can fluidly \\ninspect, manipulate and visualize their data, and rapidly iterate to refine \\nprogramming statements. These statements are then stitched together'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 2}, page_content='inspect, manipulate and visualize their data, and rapidly iterate to refine \\nprogramming statements. These statements are then stitched together \\ninto imperative or functional programs, or notebooks containing both \\ncomputation and narrative. Scientific computing beyond exploratory \\nwork is often done in a text editor or an integrated development envi-\\nronment (IDE) such as Spyder. This rich and productive environment \\nhas made Python popular for scientific research.\\nTo complement this facility for exploratory work and rapid proto-\\ntyping, NumPy has developed a culture of using time-tested software \\nengineering practices to improve collaboration and reduce error30. This \\nculture is not only adopted by leaders in the project but also enthusi-\\nastically taught to newcomers. The NumPy team was early to adopt \\ndistributed revision control and code review to improve collaboration \\ncantera\\nChemistry\\nBiopython\\nBiology\\nAstropy\\nAstronomy\\nsimpeg\\nGeophysics\\nNLTK\\nLinguistics\\nQuantEcon\\nEconomics'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 2}, page_content='distributed revision control and code review to improve collaboration \\ncantera\\nChemistry\\nBiopython\\nBiology\\nAstropy\\nAstronomy\\nsimpeg\\nGeophysics\\nNLTK\\nLinguistics\\nQuantEcon\\nEconomics\\nSciPy\\nAlgorithms\\nMatplotlib\\nPlots\\nscikit-learn\\nMachine learning\\nNetworkX\\nNetwork analysis\\npandas, statsmodels\\nStatistics\\nscikit-image\\nImage processing\\nPsychoPy\\nkhmer\\nQiime2\\nFiPy\\ndeepchem\\nlibrosa\\nPyWavelets\\nSunPy\\nQuTiP\\nyt\\nnibabel\\nyellowbrick\\nmne-python \\nscikit-HEP\\neht-imaging\\nMDAnalysis\\niris\\ncesium\\nPyChrono\\nFoundation\\nApplication-speciﬁc\\nDomain-speciﬁc\\nTechnique-speciﬁc\\nArray Protocols\\nNumPy API\\nPython\\nLanguage\\nIPython / Jupyter\\nInteractive environments\\nNumPy\\nArrays\\nNew array implementations\\nFig. 2 | NumPy is the base of the scientific Python ecosystem. Essential libraries and projects that depend on NumPy’s API gain access to new array \\nimplementations that support NumPy’s array protocols (Fig.\\xa03).'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content='360\\u2002 |\\u2002 Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\nReview\\non code, and continuous testing that runs an extensive battery of auto-\\nmated tests for every proposed change to NumPy. The project also \\nhas comprehensive, high-quality documentation, integrated with the \\nsource code31–33.\\nThis culture of using best practices for producing reliable scientific \\nsoftware has been adopted by the ecosystem of libraries that build on \\nNumPy. For example, in a recent award given by the Royal Astronomi-\\ncal Society to Astropy, they state: “The Astropy Project has provided \\nhundreds of junior scientists with experience in professional-standard \\nsoftware development practices including use of version control, unit \\ntesting, code review and issue tracking procedures. This is a vital skill \\nset for modern researchers that is often missing from formal university \\neducation in physics or astronomy”34. Community members explicitly \\nwork to address this lack of formal education through courses and'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content='education in physics or astronomy”34. Community members explicitly \\nwork to address this lack of formal education through courses and \\nworkshops35–37.\\nThe recent rapid growth of data science, machine learning and arti-\\nficial intelligence has further and dramatically boosted the scientific \\nuse of Python. Examples of its important applications, such as the \\neht-imaging library, now exist in almost every discipline in the natu-\\nral and social sciences. These tools have become the primary software \\nenvironment in many fields. NumPy and its ecosystem are commonly \\ntaught in university courses, boot camps and summer schools, and \\nare the focus of community conferences and workshops worldwide. \\nNumPy and its API have become truly ubiquitous.\\nArray proliferation and interoperability\\nNumPy provides in-memory, multidimensional, homogeneously typed \\n(that is, single-pointer and strided) arrays on CPUs. It runs on machines \\nranging from embedded devices to the world’s largest supercomputers,'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content='(that is, single-pointer and strided) arrays on CPUs. It runs on machines \\nranging from embedded devices to the world’s largest supercomputers, \\nwith performance approaching that of compiled languages. For most \\nits existence, NumPy addressed the vast majority of array computa-\\ntion use cases.\\nHowever, scientific datasets now routinely exceed the memory capac-\\nity of a single machine and may be stored on multiple machines or in \\nthe cloud. In addition, the recent need to accelerate deep-learning and \\nartificial intelligence applications has led to the emergence of special-\\nized accelerator hardware, including graphics processing units (GPUs), \\ntensor processing units (TPUs) and field-programmable gate arrays \\n(FPGAs). Owing to its in-memory data model, NumPy is currently unable \\nto directly utilize such storage and specialized hardware. However, \\nboth distributed data and also the parallel execution of GPUs, TPUs \\nand FPGAs map well to the paradigm of array programming: therefore'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content='both distributed data and also the parallel execution of GPUs, TPUs \\nand FPGAs map well to the paradigm of array programming: therefore \\nleading to a gap between available modern hardware architectures and \\nthe tools necessary to leverage their computational power.\\nThe community’s efforts to fill this gap led to a proliferation of new \\narray implementations. For example, each deep-learning framework \\ncreated its own arrays; the PyTorch38, Tensorflow39, Apache MXNet40 \\nand JAX arrays all have the capability to run on CPUs and GPUs in a \\ndistributed fashion, using lazy evaluation to allow for additional per-\\nformance optimizations. SciPy and PyData/Sparse both provide sparse \\narrays, which typically contain few non-zero values and store only those \\nin memory for efficiency. In addition, there are projects that build on \\nNumPy arrays as data containers, and extend its capabilities. Distrib-\\nuted arrays are made possible that way by Dask, and labelled arrays—'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content=\"NumPy arrays as data containers, and extend its capabilities. Distrib-\\nuted arrays are made possible that way by Dask, and labelled arrays—\\nreferring to dimensions of an array by name rather than by index for \\nclarity, compare x[:,\\xa01] versus x.loc[:,\\xa0'time']—by xarray41.\\nSuch libraries often mimic the NumPy API, because this lowers the \\nbarrier to entry for newcomers and provides the wider community with \\na stable array\\xa0programming interface. This, in turn, prevents disruptive \\nschisms such as the divergence between\\xa0Numeric and Numarray. But \\nexploring new ways of working with arrays is experimental by nature \\nand, in fact, several promising libraries (such as Theano and Caffe) have \\nalready ceased development. And each time that a user decides to try a \\nnew technology, they must change import statements and ensure that the \\nnew library implements all the parts of the NumPy API they currently use.\\nIdeally, operating on specialized arrays using NumPy functions or\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content='new library implements all the parts of the NumPy API they currently use.\\nIdeally, operating on specialized arrays using NumPy functions or \\nsemantics would simply work, so that users could write code once, \\nand would then benefit from switching between NumPy arrays, GPU \\narrays, distributed arrays and so forth as appropriate. To support array \\noperations between external array objects, NumPy therefore added \\nthe capability to act as a central coordination mechanism with a well \\nspecified API (Fig.\\xa02).\\nTo facilitate this interoperability, NumPy provides ‘protocols’ (or \\ncontracts of operation), that allow for specialized arrays to be passed to \\nNumPy functions (Fig.\\xa03). NumPy, in turn, dispatches operations to the \\noriginating library, as required. Over four hundred of the most popular \\nNumPy functions are supported. The protocols are implemented by \\nwidely used libraries such as Dask, CuPy, xarray and PyData/Sparse.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content='NumPy functions are supported. The protocols are implemented by \\nwidely used libraries such as Dask, CuPy, xarray and PyData/Sparse. \\nThanks to these developments, users can now, for example, scale their \\ncomputation from a single machine to distributed systems using Dask. \\nThe protocols also compose well, allowing users to redeploy NumPy \\ncode at scale on distributed, multi-GPU systems via, for instance, CuPy \\narrays embedded in Dask arrays. Using NumPy’s high-level API, users \\ncan leverage highly parallel code execution on multiple systems with \\nmillions of cores, all with minimal code changes42.\\nThese array protocols are now a key feature of NumPy, and are \\nexpected to only increase in importance. The NumPy developers—\\nmany of whom are authors of this Review—iteratively refine and add \\nprotocol designs to improve utility and simplify adoption.\\nOutput\\narrays\\nInput\\narrays\\nNumPy\\nAPI\\nnp.stack\\nnp.reshape\\nnp.transpose\\nnp.argmin\\nnp.mean\\nnp.std\\nnp.max\\nnp.cos\\nnp.arctan\\nnp.log\\nnp.cumsum'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content='protocol designs to improve utility and simplify adoption.\\nOutput\\narrays\\nInput\\narrays\\nNumPy\\nAPI\\nnp.stack\\nnp.reshape\\nnp.transpose\\nnp.argmin\\nnp.mean\\nnp.std\\nnp.max\\nnp.cos\\nnp.arctan\\nnp.log\\nnp.cumsum\\nnp.diff\\n...\\nNumPy array protocols\\nIn [1]: import numpy as np\\nIn [2]: import dask.array as da\\nIn [3]: x = da.arange(12)\\nIn [4]: x = np.reshape(x, (4, 3))\\nIn [5]: x\\nOut[5]: dask.array<..., shape=(4, 3), ...>\\nIn [6]: np.mean(x, axis=0)\\nOut[6]: dask.array<..., shape=(3,), ...>\\nIn [7]: x = x - np.mean(x, axis=0)\\nIn [8]: x\\nOut[8]: dask.array<..., shape=(4, 3), ...>\\nArray\\nimplementation\\nNumPy\\nDask\\nCuPy\\nPyData/\\nSparse\\n...\\n...\\nDask\\nNumPy\\nCuPy\\nPyData\\nSparse\\n...\\nDask\\nNumPy\\nCuPy\\nPyData\\nSparse\\nFig. 3 | NumPy’s API and array protocols expose new arrays to the \\necosystem. In this example, NumPy’s ‘mean’ function is called on a Dask array. \\nThe call succeeds by dispatching to the appropriate library implementation (in \\nthis case, Dask) and results in a new Dask array. Compare this code to the'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 3}, page_content='The call succeeds by dispatching to the appropriate library implementation (in \\nthis case, Dask) and results in a new Dask array. Compare this code to the \\nexample code in Fig.\\xa01g.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\u2002 |\\u2002 361\\nDiscussion\\nNumPy combines the expressive power of array programming, the \\nperformance of C, and the readability, usability and versatility of Python \\nin a mature, well tested, well documented and community-developed \\nlibrary. Libraries in the scientific Python ecosystem provide fast imple-\\nmentations of most important algorithms. Where extreme optimiza-\\ntion is warranted, compiled languages can be used, such as Cython43, \\nNumba44 and Pythran45; these languages extend Python and trans-\\nparently accelerate bottlenecks. Owing to NumPy’s simple memory \\nmodel, it is easy to write low-level, hand-optimized code, usually in C \\nor Fortran, to manipulate NumPy arrays and pass them back to Python. \\nFurthermore, using array protocols, it is possible to utilize the full \\nspectrum of specialized hardware acceleration with minimal changes \\nto existing code.\\nNumPy was initially developed by students, faculty and researchers'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='spectrum of specialized hardware acceleration with minimal changes \\nto existing code.\\nNumPy was initially developed by students, faculty and researchers \\nto provide an advanced, open-source array programming library for \\nPython, which was free to use and unencumbered by license servers and \\nsoftware protection dongles. There was a sense of building something \\nconsequential together for the benefit of many others. Participating \\nin such an endeavour, within a welcoming community of like-minded \\nindividuals, held a powerful attraction for many early contributors.\\nThese user–developers frequently had to write code from scratch \\nto solve their own or their colleagues’ problems—often in low-level \\nlanguages that preceded Python, such as Fortran46 and C. To them, \\nthe advantages of an interactive, high-level array library were evident. \\nThe design of this new tool was informed by other powerful interactive \\nprogramming languages for scientific computing such as Basis47–50,'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='The design of this new tool was informed by other powerful interactive \\nprogramming languages for scientific computing such as Basis47–50, \\nYorick51, R52 and APL53, as well as commercial languages and environ-\\nments such as IDL (Interactive Data Language) and MATLAB.\\nWhat began as an attempt to add an array object to Python became \\nthe foundation of a vibrant ecosystem of tools. Now, a large amount of \\nscientific work depends on NumPy being correct, fast and stable. It is \\nno longer a small community project, but core scientific infrastructure.\\nThe developer culture has matured: although initial development was \\nhighly informal, NumPy now has a roadmap and a process for propos-\\ning and discussing large changes. The project has formal governance \\nstructures and is fiscally sponsored by NumFOCUS, a nonprofit that \\npromotes open practices in research, data and scientific computing. \\nOver the past few years, the project attracted its first funded develop-'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='promotes open practices in research, data and scientific computing. \\nOver the past few years, the project attracted its first funded develop-\\nment, sponsored by the Moore and Sloan Foundations, and received \\nan award as part of the Chan Zuckerberg Initiative’s Essentials of Open \\nSource Software programme. With this funding, the project was (and \\nis) able to have sustained focus over multiple months to implement \\nsubstantial new features and improvements. That said, the develop-\\nment of NumPy still depends heavily on contributions made by gradu-\\nate students and researchers in their free time (see Supplementary \\nMethods for more details).\\nNumPy is no longer merely the foundational array library underlying \\nthe scientific Python ecosystem, but it has become the standard API for \\ntensor computation and a central coordinating mechanism between \\narray types and technologies in Python. Work continues to expand on \\nand improve these interoperability features.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='tensor computation and a central coordinating mechanism between \\narray types and technologies in Python. Work continues to expand on \\nand improve these interoperability features.\\nOver the next decade, NumPy developers will face several challenges. \\nNew devices will be developed, and existing specialized hardware will \\nevolve to meet diminishing returns on Moore’s law. There will be more, \\nand a wider variety of, data science practitioners, a large proportion of \\nwhom will use NumPy. The scale of scientific data gathering will con-\\ntinue to increase, with the adoption of devices and instruments such \\nas light-sheet microscopes and the Large Synoptic Survey Telescope \\n(LSST)54. New generation languages, interpreters and compilers, such as \\nRust55, Julia56 and LLVM57, will create new concepts and data structures, \\nand determine their viability.\\nThrough the mechanisms described in this Review, NumPy is poised \\nto embrace such a changing landscape, and to continue playing a'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='and determine their viability.\\nThrough the mechanisms described in this Review, NumPy is poised \\nto embrace such a changing landscape, and to continue playing a \\nleading part in interactive scientific computation, although to do so \\nwill require sustained funding from government, academia and indus-\\ntry. But, importantly, for NumPy to meet the needs of the next decade \\nof data science, it will also need a new generation of graduate students \\nand community contributors to drive it forward.\\n1.\\t\\nAbbott, B. P. et\\xa0al. Observation of gravitational waves from a binary black hole merger. \\nPhys. Rev. Lett. 116, 061102 (2016).\\n2.\\t\\nChael, A. et\\xa0al. High-resolution linear polarimetric imaging for the Event Horizon \\nTelescope. Astrophys. J. 286, 11 (2016).\\n3.\\t\\nDubois, P. F., Hinsen, K. & Hugunin, J. Numerical Python. Comput. Phys. 10, 262–267 (1996).\\n4.\\t\\nAscher, D., Dubois, P. F., Hinsen, K., Hugunin, J. & Oliphant, T. E. An Open Source Project:'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='3.\\t\\nDubois, P. F., Hinsen, K. & Hugunin, J. Numerical Python. Comput. Phys. 10, 262–267 (1996).\\n4.\\t\\nAscher, D., Dubois, P. F., Hinsen, K., Hugunin, J. & Oliphant, T. E. An Open Source Project: \\nNumerical Python (Lawrence Livermore National Laboratory, 2001).\\n5.\\t\\nYang, T.-Y., Furnish, G. & Dubois, P. F. Steering object-oriented scientific computations. In \\nProc. TOOLS USA 97. Intl Conf. Technology of Object Oriented Systems and Languages \\n(eds Ege,\\xa0R., Singh,\\xa0M.\\xa0& Meyer,\\xa0B.) 112–119 (IEEE, 1997).\\n6.\\t\\nGreenfield, P., Miller, J. T., Hsu, J. & White, R. L. numarray: a new scientific array package \\nfor Python. In PyCon DC 2003 http://citeseerx.ist.psu.edu/viewdoc/download?d\\noi=10.1.1.112.9899 (2003).\\n7.\\t\\nOliphant, T. E. Guide to NumPy 1st edn (Trelgol Publishing, 2006).\\n8.\\t\\nDubois, P. F. Python: batteries included. Comput. Sci. Eng. 9, 7–9 (2007).\\n9.\\t\\nOliphant, T. E. Python for scientific computing. Comput. Sci. Eng. 9, 10–20 (2007).\\n10.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='8.\\t\\nDubois, P. F. Python: batteries included. Comput. Sci. Eng. 9, 7–9 (2007).\\n9.\\t\\nOliphant, T. E. Python for scientific computing. Comput. Sci. Eng. 9, 10–20 (2007).\\n10.\\t\\nMillman, K. J. & Aivazis, M. Python for scientists and engineers. Comput. Sci. Eng. 13, 9–12 \\n(2011).\\n11.\\t\\nPérez, F., Granger, B. E. & Hunter, J. D. Python: an ecosystem for scientific computing. \\nComput. Sci. Eng. 13, 13–21 (2011).  \\nExplains why the scientific Python ecosystem is a highly productive environment for \\nresearch.\\n12.\\t\\nVirtanen, P. et\\xa0al. SciPy 1.0—fundamental algorithms for scientific computing in Python. \\nNat. Methods 17, 261–272 (2020); correction 17, 352 (2020).  \\nIntroduces the SciPy library and includes a more detailed history of NumPy and SciPy.\\n13.\\t\\nHunter, J. D. Matplotlib: a 2D graphics environment. Comput. Sci. Eng. 9, 90–95 (2007).\\n14.\\t\\nMcKinney, W. Data structures for statistical computing in Python. In Proc. 9th Python in'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='13.\\t\\nHunter, J. D. Matplotlib: a 2D graphics environment. Comput. Sci. Eng. 9, 90–95 (2007).\\n14.\\t\\nMcKinney, W. Data structures for statistical computing in Python. In Proc. 9th Python in \\nScience Conf. (eds van\\xa0der\\xa0Walt,\\xa0S.\\xa0& Millman,\\xa0K.\\xa0J.) 56–61 (2010).\\n15.\\t\\nPedregosa, F. et\\xa0al. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, \\n2825–2830 (2011).\\n16.\\t\\nvan\\xa0der Walt, S. et\\xa0al. scikit-image: image processing in Python. PeerJ 2, e453 (2014).\\n17.\\t\\nvan\\xa0der Walt, S., Colbert, S. C. & Varoquaux, G. The NumPy array: a structure for efficient \\nnumerical computation. Comput. Sci. Eng. 13, 22–30 (2011).  \\nDiscusses the NumPy array data structure with a focus on how it enables efficient \\ncomputation.\\n18.\\t\\nWang, Q., Zhang, X., Zhang, Y. & Yi, Q. AUGEM: automatically generate high performance \\ndense linear algebra kernels on x86 CPUs. In SC’13: Proc. Intl Conf. High Performance \\nComputing, Networking, Storage and Analysis 25 (IEEE, 2013).\\n19.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='dense linear algebra kernels on x86 CPUs. In SC’13: Proc. Intl Conf. High Performance \\nComputing, Networking, Storage and Analysis 25 (IEEE, 2013).\\n19.\\t\\nXianyi, Z., Qian, W. & Yunquan, Z. Model-driven level 3 BLAS performance optimization \\non Loongson 3A processor. In 2012 IEEE 18th Intl Conf. Parallel and Distributed Systems \\n684–691 (IEEE, 2012).\\n20.\\t Pérez, F. & Granger, B. E. IPython: a system for interactive scientific computing. Comput. \\nSci. Eng. 9, 21–29 (2007).\\n21.\\t\\nKluyver, T. et\\xa0al. Jupyter Notebooks—a publishing format for reproducible computational \\nworkflows. In Positioning and Power in Academic Publishing: Players, Agents and Agendas \\n(eds Loizides,\\xa0F.\\xa0& Schmidt,\\xa0B.) 87–90 (IOS Press, 2016).\\n22.\\t Hagberg, A. A., Schult, D. A. & Swart, P. J. Exploring network structure, dynamics, and \\nfunction using NetworkX. In Proc. 7th Python in Science Conf. (eds Varoquaux,\\xa0G., \\nVaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 11–15 (2008).'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='function using NetworkX. In Proc. 7th Python in Science Conf. (eds Varoquaux,\\xa0G., \\nVaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 11–15 (2008).\\n23.\\t Astropy Collaboration et\\xa0al. Astropy: a community Python package for astronomy. Astron. \\nAstrophys. 558, A33 (2013).\\n24.\\t Price-Whelan, A. M. et\\xa0al. The Astropy Project: building an open-science project and \\nstatus of the v2.0 core package. Astron. J. 156, 123 (2018).\\n25.\\t Cock, P. J. et\\xa0al. Biopython: freely available Python tools for computational molecular \\nbiology and bioinformatics. Bioinformatics 25, 1422–1423 (2009).\\n26.\\t Millman, K. J. & Brett, M. Analysis of functional magnetic resonance imaging in Python. \\nComput. Sci. Eng. 9, 52–55 (2007).\\n27.\\t\\nThe SunPy Community et\\xa0al. SunPy—Python for solar physics. Comput. Sci. Discov. 8, \\n014009 (2015).\\n28.\\t Hamman, J., Rocklin, M. & Abernathy, R. Pangeo: a big-data ecosystem for scalable Earth \\nsystem science. In EGU General Assembly Conf. Abstracts 12146 (2018).'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='014009 (2015).\\n28.\\t Hamman, J., Rocklin, M. & Abernathy, R. Pangeo: a big-data ecosystem for scalable Earth \\nsystem science. In EGU General Assembly Conf. Abstracts 12146 (2018).\\n29.\\t Chael, A. A. et\\xa0al. ehtim: imaging, analysis, and simulation software for radio \\ninterferometry. Astrophysics Source Code Library https://ascl.net/1904.004 (2019).\\n30.\\t Millman, K. J. & Pérez, F. Developing open source scientific practice. In Implementing \\nReproducible Research (eds Stodden,\\xa0V., Leisch,\\xa0F.\\xa0& Peng,\\xa0R.\\xa0D.) 149–183 (CRC Press, 2014). \\nDescribes the software engineering practices embraced by the NumPy and SciPy \\ncommunities with a focus on how these practices improve research.\\n31.\\t\\nvan\\xa0der Walt, S. The SciPy Documentation Project (technical overview). In Proc. 7th Python \\nin Science Conf. (SciPy 2008) (eds Varoquaux,\\xa0G., Vaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 27–28 (2008).\\n32.\\t Harrington, J. The SciPy Documentation Project. In Proc. 7th Python in Science'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 4}, page_content='in Science Conf. (SciPy 2008) (eds Varoquaux,\\xa0G., Vaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 27–28 (2008).\\n32.\\t Harrington, J. The SciPy Documentation Project. In Proc. 7th Python in Science \\nConference (SciPy 2008) (eds Varoquaux,\\xa0G., Vaught,\\xa0T.\\xa0& Millman, K.\\xa0J.) 33–35 (2008).\\n33.\\t Harrington, J. & Goldsmith, D. Progress report: NumPy and SciPy documentation in 2009. \\nIn Proc. 8th Python in Science Conf. (SciPy 2009) (eds Varoquaux,\\xa0G., van\\xa0der\\xa0Walt,\\xa0S.\\xa0& \\nMillman,\\xa0K.\\xa0J.) 84–87 (2009).\\n34.\\t Royal Astronomical Society Report of the RAS ‘A’ Awards Committee 2020: Astropy \\nProject: 2020 Group Achievement Award (A) https://ras.ac.uk/sites/default/files/2020-01/\\nGroup%20Award%20-%20Astropy.pdf (2020).\\n35.\\t Wilson, G. Software carpentry: getting scientists to write better code by making them \\nmore productive. Comput. Sci. Eng. 8, 66–69 (2006).'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 5}, page_content='362\\u2002 |\\u2002 Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\nReview\\n36.\\t Hannay, J. E. et\\xa0al. How do scientists develop and use scientific software? In Proc. 2009 \\nICSE Workshop on Software Engineering for Computational Science and Engineering 1–8 \\n(IEEE, 2009).\\n37.\\t\\nMillman, K. J., Brett, M., Barnowski, R. & Poline, J.-B. Teaching computational \\nreproducibility for neuroimaging. Front. Neurosci. 12, 727 (2018).\\n38.\\t Paszke, A. et\\xa0al. Pytorch: an imperative style, high-performance deep learning library. In \\nAdvances in Neural Information Processing Systems 32 (eds Wallach,\\xa0H.\\xa0et al.) 8024–8035 \\n(Neural Information Processing Systems, 2019).\\n39.\\t Abadi, M. et\\xa0al. TensorFlow: a system for large-scale machine learning. In OSDI’16: Proc. \\n12th USENIX Conf. Operating Systems Design and Implementation (chairs Keeton, K. & \\nRoscoe, T.) 265–283 (USENIX Association, 2016).\\n40.\\t Chen, T. et\\xa0al. MXNet: a flexible and efficient machine learning library for heterogeneous'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 5}, page_content='Roscoe, T.) 265–283 (USENIX Association, 2016).\\n40.\\t Chen, T. et\\xa0al. MXNet: a flexible and efficient machine learning library for heterogeneous \\ndistributed systems. Preprint at http://www.arxiv.org/abs/1512.01274 (2015).\\n41.\\t\\nHoyer, S. & Hamman, J. xarray: N–D labeled arrays and datasets in Python. J. Open Res. \\nSoftw. 5, 10 (2017).\\n42.\\t Entschev, P. Distributed multi-GPU computing with Dask, CuPy and RAPIDS. In EuroPython \\n2019 https://ep2019.europython.eu/media/conference/slides/\\nfX8dJsD-distributed-multi-gpu-computing-with-dask-cupy-and-rapids.pdf (2019).\\n43.\\t Behnel, S. et\\xa0al. Cython: the best of both worlds. Comput. Sci. Eng. 13, 31–39 (2011).\\n44.\\t Lam, S. K., Pitrou, A. & Seibert, S. Numba: a LLVM-based Python JIT compiler. In Proc. \\nSecond Workshop on the LLVM Compiler Infrastructure in HPC, LLVM ’15 7:1–7:6 (ACM, 2015).\\n45.\\t Guelton, S. et\\xa0al. Pythran: enabling static optimization of scientific Python programs. \\nComput. Sci. Discov. 8, 014001 (2015).'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 5}, page_content='45.\\t Guelton, S. et\\xa0al. Pythran: enabling static optimization of scientific Python programs. \\nComput. Sci. Discov. 8, 014001 (2015).\\n46.\\t Dongarra, J., Golub, G. H., Grosse, E., Moler, C. & Moore, K. Netlib and NA-Net: building a \\nscientific computing community. IEEE Ann. Hist. Comput. 30, 30–41 (2008).\\n47.\\t\\nBarrett, K. A., Chiu, Y. H., Painter, J. F., Motteler, Z. C. & Dubois, P. F. Basis System, Part I: \\nRunning a Basis Program—A Tutorial for Beginners UCRL-MA-118543, Vol.\\xa01 (Lawrence \\nLivermore National Laboratory 1995).\\n48.\\t Dubois, P. F. & Motteler, Z. Basis System, Part II: Basis Language Reference Manual \\nUCRL-MA-118543, Vol.\\xa02 (Lawrence Livermore National Laboratory, 1995).\\n49.\\t Chiu, Y. H. & Dubois, P. F. Basis System, Part III: EZN User Manual UCRL-MA-118543, Vol.\\xa03 \\n(Lawrence Livermore National Laboratory, 1995).\\n50.\\t Chiu, Y. H. & Dubois, P. F. Basis System, Part IV: EZD User Manual UCRL-MA-118543, Vol.\\xa04 \\n(Lawrence Livermore National Laboratory, 1995).\\n51.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 5}, page_content='(Lawrence Livermore National Laboratory, 1995).\\n50.\\t Chiu, Y. H. & Dubois, P. F. Basis System, Part IV: EZD User Manual UCRL-MA-118543, Vol.\\xa04 \\n(Lawrence Livermore National Laboratory, 1995).\\n51.\\t\\nMunro, D. H. & Dubois, P. F. Using the Yorick interpreted language. Comput. Phys. 9, \\n609–615 (1995).\\n52.\\t Ihaka, R. & Gentleman, R. R: a language for data analysis and graphics. J. Comput. Graph. \\nStat. 5, 299–314 (1996).\\n53.\\t Iverson, K. E. A programming language. In Proc. 1962 Spring Joint Computer Conf. \\n345–351 (1962).\\n54.\\t Jenness, T. et\\xa0al. LSST data management software development practices and tools. In \\nProc. SPIE 10707, Software and Cyberinfrastructure for Astronomy V 1070709 (SPIE and \\nInternational Society for Optics and Photonics, 2018).\\n55.\\t Matsakis, N. D. & Klock, F. S. The Rust language. Ada Letters 34, 103–104 (2014).\\n56.\\t Bezanson, J., Edelman, A., Karpinski, S. & Shah, V. B. Julia: a fresh approach to numerical \\ncomputing. SIAM Rev. 59, 65–98 (2017).\\n57.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 5}, page_content='56.\\t Bezanson, J., Edelman, A., Karpinski, S. & Shah, V. B. Julia: a fresh approach to numerical \\ncomputing. SIAM Rev. 59, 65–98 (2017).\\n57.\\t\\nLattner, C. & Adve, V. LLVM: a compilation framework for lifelong program analysis and \\ntransformation. In Proc. 2004 Intl Symp. Code Generation and Optimization (CGO’04) \\n75–88 (IEEE, 2004).\\nAcknowledgements We thank R.\\xa0Barnowski, P.\\xa0Dubois, M.\\xa0Eickenberg, and P.\\xa0Greenfield, who \\nsuggested text and provided helpful feedback on the manuscript. K.J.M. and S.J.v.d.W. were \\nfunded in part by the Gordon and Betty Moore Foundation through grant GBMF3834 and by \\nthe Alfred P. Sloan Foundation through grant 2013-10-27 to the University of California, \\nBerkeley. S.J.v.d.W., S.B., M.P. and W.W. were funded in part by the Gordon and Betty Moore \\nFoundation through grant GBMF5447 and by the Alfred P. Sloan Foundation through grant \\nG-2017-9960 to the University of California, Berkeley.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 5}, page_content='Foundation through grant GBMF5447 and by the Alfred P. Sloan Foundation through grant \\nG-2017-9960 to the University of California, Berkeley.\\nAuthor contributions K.J.M. and S.J.v.d.W. composed the manuscript with input from \\nothers. S.B., R.G., K.S., W.W., M.B. and T.R. contributed text. All authors contributed \\nsubstantial code, documentation and/or expertise to the NumPy project. All authors \\nreviewed the manuscript.\\nCompeting interests The authors declare no competing interests.\\nAdditional information\\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\\n2649-2.\\nCorrespondence and requests for materials should be addressed to K.J.M., S.J.v.W. or R.G.\\nPeer review information Nature thanks Edouard Duchesnay,\\xa0Alan Edelman and the other, \\nanonymous, reviewer(s) for their contribution to the peer review of this work.\\nReprints and permissions information is available at http://www.nature.com/reprints.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 5}, page_content='anonymous, reviewer(s) for their contribution to the peer review of this work.\\nReprints and permissions information is available at http://www.nature.com/reprints.\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \\npublished maps and institutional affiliations.\\nOpen Access This article is licensed under a Creative Commons Attribution \\n4.0 International License, which permits use, sharing, adaptation, distribution \\nand reproduction in any medium or format, as long as you give appropriate \\ncredit to the original author(s) and the source, provide a link to the Creative Commons license, \\nand indicate if changes were made. The images or other third party material in this article are \\nincluded in the article’s Creative Commons license, unless indicated otherwise in a credit line \\nto the material. If material is not included in the article’s Creative Commons license and your'),\n",
       " Document(metadata={'producer': '', 'creator': 'Springer', 'creationdate': '2020-09-06T19:31:45+05:30', 'source': '..\\\\data\\\\pdf\\\\numpy.pdf', 'file_path': '..\\\\data\\\\pdf\\\\numpy.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Array programming with NumPy', 'author': 'Charles R. Harris', 'subject': 'Nature, doi:10.1038/s41586-020-2649-2', 'keywords': '', 'moddate': '2020-09-06T19:32:25+05:30', 'trapped': '', 'modDate': \"D:20200906193225+05'30'\", 'creationDate': \"D:20200906193145+05'30'\", 'page': 5}, page_content='included in the article’s Creative Commons license, unless indicated otherwise in a credit line \\nto the material. If material is not included in the article’s Creative Commons license and your \\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \\nneed to obtain permission directly from the copyright holder. To view a copy of this license, \\nvisit http://creativecommons.org/licenses/by/4.0/.\\n© The Author(s) 2020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 0}, page_content='1\\npandas: a Foundational Python Library for Data\\nAnalysis and Statistics\\nWes McKinney\\n!\\nAbstract—In this paper we will discuss pandas, a Python library of rich\\ndata structures and tools for working with structured data sets common to\\nstatistics, ﬁnance, social sciences, and many other ﬁelds. The library provides\\nintegrated, intuitive routines for performing common data manipulations and\\nanalysis on such data sets. It aims to be the foundational layer for the future of\\nstatistical computing in Python. It serves as a strong complement to the existing\\nscientiﬁc Python stack while implementing and improving upon the kinds of data\\nmanipulation tools found in other statistical programming languages such as\\nR. In addition to detailing its design and features of pandas, we will discuss\\nfuture avenues of work and growth opportunities for statistics and data analysis\\napplications in the Python language.\\nIntroduction\\nPython is being used increasingly in scientiﬁc applications'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 0}, page_content='future avenues of work and growth opportunities for statistics and data analysis\\napplications in the Python language.\\nIntroduction\\nPython is being used increasingly in scientiﬁc applications\\ntraditionally dominated by [R], [MATLAB], [Stata], [SAS],\\nother commercial or open-source research environments. The\\nmaturity and stability of the fundamental numerical li-\\nbraries ([NumPy], [SciPy], and others), quality of documenta-\\ntion, and availability of “kitchen-sink” distributions ([EPD],\\n[Pythonxy]) have gone a long way toward making Python\\naccessible and convenient for a broad audience. Additionally\\n[matplotlib] integrated with [IPython] provides an interactive\\nresearch and development environment with data visualization\\nsuitable for most users. However, adoption of Python for\\napplied statistical modeling has been relatively slow compared\\nwith other areas of computational science.\\nOne major issue for would-be statistical Python program-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 0}, page_content='applied statistical modeling has been relatively slow compared\\nwith other areas of computational science.\\nOne major issue for would-be statistical Python program-\\nmers in the past has been the lack of libraries implementing\\nstandard models and a cohesive framework for specifying\\nmodels. However, in recent years there have been signiﬁcant\\nnew developments in econometrics ([StaM]), Bayesian statis-\\ntics ([PyMC]), and machine learning ([SciL]), among others\\nﬁelds. However, it is still difﬁcult for many statisticians to\\nchoose Python over R given the domain-speciﬁc nature of the\\nR language and breadth of well-vetted open-source libraries\\navailable to R users ([CRAN]). In spite of this obstacle, we\\nbelieve that the Python language and the libraries and tools\\ncurrently available can be leveraged to make Python a superior\\nenvironment for data analysis and statistical computing.\\nAnother issue preventing many from using Python in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 0}, page_content='currently available can be leveraged to make Python a superior\\nenvironment for data analysis and statistical computing.\\nAnother issue preventing many from using Python in the\\npast for data analysis applications has been the lack of rich data\\nstructures with integrated handling of metadata. By metadata\\nwe mean labeling information about data points. For example,\\nCorresponding author can be contacted at: wesmckinn@gmail.com.\\nc○2011 Wes McKinney\\na table or spreadsheet of data will likely have labels for the\\ncolumns and possibly also the rows. Alternately, some columns\\nin a table might be used for grouping and aggregating data into\\na pivot or contingency table. In the case of a time series data\\nset, the row labels could be time stamps. It is often necessary\\nto have the labeling information available to allow many kinds\\nof data manipulations, such as merging data sets or performing\\nan aggregation or “group by” operation, to be expressed in an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 0}, page_content='to have the labeling information available to allow many kinds\\nof data manipulations, such as merging data sets or performing\\nan aggregation or “group by” operation, to be expressed in an\\nintuitive and concise way. Domain-speciﬁc database languages\\nlike SQL and statistical languages like R and SAS have a\\nwealth of such tools. Until relatively recently, Python had few\\ntools providing the same level of richness and expressiveness\\nfor working with labeled data sets.\\nThe pandas library, under development since 2008, is\\nintended to close the gap in the richness of available data\\nanalysis tools between Python, a general purpose systems\\nand scientiﬁc computing language, and the numerous domain-\\nspeciﬁc statistical computing platforms and database lan-\\nguages. We not only aim to provide equivalent functionality\\nbut also implement many features, such as automatic data\\nalignment and hierarchical indexing, which are not readily\\navailable in such a tightly integrated way in any other libraries'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 0}, page_content='but also implement many features, such as automatic data\\nalignment and hierarchical indexing, which are not readily\\navailable in such a tightly integrated way in any other libraries\\nor computing environments to our knowledge. While initially\\ndeveloped for ﬁnancial data analysis applications, we hope that\\npandas will enable scientiﬁc Python to be a more attractive\\nand practical statistical computing environment for academic\\nand industry practitioners alike. The library’s name derives\\nfrom panel data, a common term for multidimensional data\\nsets encountered in statistics and econometrics.\\nWhile we offer a vignette of some of the main features of\\ninterest in pandas, this paper is by no means comprehensive.\\nFor more, we refer the interested reader to the online docu-\\nmentation at http://pandas.sf.net ([pandas]).\\nStructured data sets\\nStructured data sets commonly arrive in tabular format, i.e.\\nas a two-dimensional list of observations and names for the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 0}, page_content='mentation at http://pandas.sf.net ([pandas]).\\nStructured data sets\\nStructured data sets commonly arrive in tabular format, i.e.\\nas a two-dimensional list of observations and names for the\\nﬁelds of each observation. Usually an observation can be\\nuniquely identiﬁed by one or more values or labels. We show\\nan example data set for a pair of stocks over the course of\\nseveral days. The NumPy ndarray with structured dtype can\\nbe used to hold this data:\\n>>> data\\narray([(’GOOG’, ’2009-12-28’, 622.87, 1697900.0),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 1}, page_content='2\\n(’GOOG’, ’2009-12-29’, 619.40, 1424800.0),\\n(’GOOG’, ’2009-12-30’, 622.73, 1465600.0),\\n(’GOOG’, ’2009-12-31’, 619.98, 1219800.0),\\n(’AAPL’, ’2009-12-28’, 211.61, 23003100.0),\\n(’AAPL’, ’2009-12-29’, 209.10, 15868400.0),\\n(’AAPL’, ’2009-12-30’, 211.64, 14696800.0),\\n(’AAPL’, ’2009-12-31’, 210.73, 12571000.0)],\\ndtype=[(’item’, ’|S4’), (’date’, ’|S10’),\\n(’price’, ’<f8’), (’volume’, ’<f8’)])\\n>>> data[’price’]\\narray([622.87, 619.4, 622.73, 619.98, 211.61, 209.1,\\n211.64, 210.73])\\nStructured (or record) NumPy arrays such as this can be\\neffective in many applications, but in our experience they do\\nnot provide the same level of ﬂexibility and ease of use as\\nother statistical environments. One major issue is that they do\\nnot integrate well with the rest of NumPy, which is mainly\\nintended for working with arrays of homogeneous dtype.\\nR provides the data.frame class which stores mixed-\\ntype data as a collection of independent columns. The core'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 1}, page_content='intended for working with arrays of homogeneous dtype.\\nR provides the data.frame class which stores mixed-\\ntype data as a collection of independent columns. The core\\nR language and its 3rd-party libraries were built with the\\ndata.frame object in mind, so most operations on such\\na data set are very natural. A data.frame is also ﬂexible\\nin size, an important feature when assembling a collection of\\ndata. The following code fragment loads the data stored in the\\nCSV ﬁle data into the variable df and adds a new column\\nof boolean values:\\n> df <- read.csv(’data’)\\nitem\\ndate\\nprice\\nvolume\\n1 GOOG 2009-12-28 622.87\\n1697900\\n2 GOOG 2009-12-29 619.40\\n1424800\\n3 GOOG 2009-12-30 622.73\\n1465600\\n4 GOOG 2009-12-31 619.98\\n1219800\\n5 AAPL 2009-12-28 211.61 23003100\\n6 AAPL 2009-12-29 209.10 15868400\\n7 AAPL 2009-12-30 211.64 14696800\\n8 AAPL 2009-12-31 210.73 12571000\\n> df$ind <- df$item == \"GOOG\"\\n> df\\nitem\\ndate\\nprice\\nvolume\\nind\\n1 GOOG 2009-12-28 622.87\\n1697900\\nTRUE\\n2 GOOG 2009-12-29 619.40\\n1424800\\nTRUE'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 1}, page_content='8 AAPL 2009-12-31 210.73 12571000\\n> df$ind <- df$item == \"GOOG\"\\n> df\\nitem\\ndate\\nprice\\nvolume\\nind\\n1 GOOG 2009-12-28 622.87\\n1697900\\nTRUE\\n2 GOOG 2009-12-29 619.40\\n1424800\\nTRUE\\n3 GOOG 2009-12-30 622.73\\n1465600\\nTRUE\\n4 GOOG 2009-12-31 619.98\\n1219800\\nTRUE\\n5 AAPL 2009-12-28 211.61 23003100 FALSE\\n6 AAPL 2009-12-29 209.10 15868400 FALSE\\n7 AAPL 2009-12-30 211.64 14696800 FALSE\\n8 AAPL 2009-12-31 210.73 12571000 FALSE\\npandas provides a similarly-named DataFrame class\\nwhich implements much of the functionality of its R coun-\\nterpart, though with some important enhancements which we\\nwill discuss. Here we convert the structured array above into\\na pandas DataFrame object and similarly add the same\\ncolumn:\\n>>> from pandas import DataFrame\\n>>> data = DataFrame(data)\\n>>> data\\nitem\\ndate\\nprice\\nvolume\\n0\\nGOOG\\n2009-12-28\\n622.9\\n1.698e+06\\n1\\nGOOG\\n2009-12-29\\n619.4\\n1.425e+06\\n2\\nGOOG\\n2009-12-30\\n622.7\\n1.466e+06\\n3\\nGOOG\\n2009-12-31\\n620\\n1.22e+06\\n4\\nAAPL\\n2009-12-28\\n211.6\\n2.3e+07\\n5\\nAAPL\\n2009-12-29\\n209.1\\n1.587e+07\\n6\\nAAPL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 1}, page_content='2009-12-28\\n622.9\\n1.698e+06\\n1\\nGOOG\\n2009-12-29\\n619.4\\n1.425e+06\\n2\\nGOOG\\n2009-12-30\\n622.7\\n1.466e+06\\n3\\nGOOG\\n2009-12-31\\n620\\n1.22e+06\\n4\\nAAPL\\n2009-12-28\\n211.6\\n2.3e+07\\n5\\nAAPL\\n2009-12-29\\n209.1\\n1.587e+07\\n6\\nAAPL\\n2009-12-30\\n211.6\\n1.47e+07\\n7\\nAAPL\\n2009-12-31\\n210.7\\n1.257e+07\\n>>> data[’ind’] = data[’item’] == ’GOOG’\\n>>> data\\nitem\\ndate\\nprice\\nvolume\\nind\\n0\\nGOOG\\n2009-12-28\\n622.9\\n1.698e+06\\nTrue\\n1\\nGOOG\\n2009-12-29\\n619.4\\n1.425e+06\\nTrue\\n2\\nGOOG\\n2009-12-30\\n622.7\\n1.466e+06\\nTrue\\n3\\nGOOG\\n2009-12-31\\n620\\n1.22e+06\\nTrue\\n4\\nAAPL\\n2009-12-28\\n211.6\\n2.3e+07\\nFalse\\n5\\nAAPL\\n2009-12-29\\n209.1\\n1.587e+07\\nFalse\\n6\\nAAPL\\n2009-12-30\\n211.6\\n1.47e+07\\nFalse\\n7\\nAAPL\\n2009-12-31\\n210.7\\n1.257e+07\\nFalse\\nThis data can be reshaped or “pivoted” on the date and\\nitem columns into a different form for future examples by\\nmeans of the DataFrame method pivot:\\n>>> del data[’ind’] # delete ind column\\n>>> data.pivot(’date’, ’item’)\\nprice\\nvolume\\nitem\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\ndate\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 1}, page_content='>>> del data[’ind’] # delete ind column\\n>>> data.pivot(’date’, ’item’)\\nprice\\nvolume\\nitem\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\ndate\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\nThe result of the pivot operation has a hierarchical index\\nfor the columns. As we will show in a later section, this is\\na powerful and ﬂexible way of representing and manipulat-\\ning multidimensional data. Currently the pivot method of\\nDataFrame only supports pivoting on two columns to reshape\\nthe data, but could be augmented to consider more than just\\ntwo columns. By using hierarchical indexes, we can guarantee\\nthat the result will always be two-dimensional. Later in the\\npaper we will demonstrate the pivot_table function which\\ncan produce spreadsheet-style pivot table data summaries as\\nDataFrame objects with hierarchical rows and columns.\\nBeyond observational data, one will also frequently en-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 1}, page_content='can produce spreadsheet-style pivot table data summaries as\\nDataFrame objects with hierarchical rows and columns.\\nBeyond observational data, one will also frequently en-\\ncounter categorical data, which can be used to partition identi-\\nﬁers into broader groupings. For example, stock tickers might\\nbe categorized by their industry or country of incorporation.\\nHere we have created a DataFrame object cats storing\\ncountry and industry classiﬁcations for a group of stocks:\\n>>> cats\\ncountry\\nindustry\\nAAPL\\nUS\\nTECH\\nIBM\\nUS\\nTECH\\nSAP\\nDE\\nTECH\\nGOOG\\nUS\\nTECH\\nC\\nUS\\nFIN\\nSCGLY\\nFR\\nFIN\\nBAR\\nUK\\nFIN\\nDB\\nDE\\nFIN\\nVW\\nDE\\nAUTO\\nRNO\\nFR\\nAUTO\\nF\\nUS\\nAUTO\\nTM\\nJP\\nAUTO\\npandas data model\\nEach axis of a pandas data structure has an Index object\\nwhich stores labeling information about each tick along that\\naxis. The most general Index is simply a 1-dimensional\\nvector of labels (stored in a NumPy ndarray). It’s convenient\\nto think about the Index as an implementation of an ordered'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 1}, page_content='axis. The most general Index is simply a 1-dimensional\\nvector of labels (stored in a NumPy ndarray). It’s convenient\\nto think about the Index as an implementation of an ordered\\nset. In the stock data above, the row index contains simply'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 2}, page_content='3\\nsequential observation numbers, while the column index con-\\ntains the column names. The labels are not required to be\\nsorted, though a subclass of Index could be implemented to\\nrequire sortedness and provide operations optimized for sorted\\ndata (e.g. time series data).\\nThe Index object is used for many purposes:\\n• Performing lookups to select subsets of slices of an object\\n• Providing fast data alignment routines for aligning one\\nobject with another\\n• Enabling intuitive slicing / selection to form new Index\\nobjects\\n• Forming unions and intersections of Index objects\\nHere are some examples of how the index is used internally:\\n>>> index = Index([’a’, ’b’, ’c’, ’d’, ’e’])\\n>>> ’c’ in index\\nTrue\\n>>> index.get_loc(’d’)\\n3\\n>>> index.slice_locs(’b’, ’d’)\\n(1, 4)\\n# for aligning data\\n>>> index.get_indexer([’c’, ’e’, ’f’])\\narray([ 2,\\n4, -1], dtype=int32)\\nThe basic Index uses a Python dict internally to map\\nlabels to their respective locations and implement these fea-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 2}, page_content='>>> index.get_indexer([’c’, ’e’, ’f’])\\narray([ 2,\\n4, -1], dtype=int32)\\nThe basic Index uses a Python dict internally to map\\nlabels to their respective locations and implement these fea-\\ntures, though subclasses could take a more specialized and\\npotentially higher performance approach.\\nMultidimensional objects like DataFrame are not proper\\nsubclasses of NumPy’s ndarray nor do they use arrays\\nwith structured dtype. In recent releases of pandas there is a\\nnew internal data structure known as BlockManager which\\nmanipulates a collection of n-dimensional ndarray objects\\nwe refer to as blocks. Since DataFrame needs to be able to\\nstore mixed-type data in the columns, each of these internal\\nBlock objects contains the data for a set of columns all\\nhaving the same type. In the example from above, we can\\nexamine the BlockManager, though most users would never\\nneed to do this:\\n>>> data._data\\nBlockManager\\nItems: [item date price volume ind]\\nAxis 1: [0 1 2 3 4 5 6 7]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 2}, page_content='examine the BlockManager, though most users would never\\nneed to do this:\\n>>> data._data\\nBlockManager\\nItems: [item date price volume ind]\\nAxis 1: [0 1 2 3 4 5 6 7]\\nFloatBlock: [price volume], 2 x 8, dtype float64\\nObjectBlock: [item date], 2 x 8, dtype object\\nBoolBlock: [ind], 1 x 8, dtype bool\\nThe key importance of BlockManager is that many\\noperations, e.g. anything row-oriented (as opposed to column-\\noriented), especially in homogeneous DataFrame objects,\\nare signiﬁcantly faster when the data are all stored in a\\nsingle ndarray. However, as it is common to insert and\\ndelete columns, it would be wasteful to have a reallocate-\\ncopy step on each column insertion or deletion step. As\\na result, the BlockManager effectively provides a lazy\\nevaluation scheme where-in newly inserted columns are stored\\nin new Block objects. Later, either explicitly or when certain\\nmethods are called in DataFrame, blocks having the same\\ntype will be consolidated, i.e. combined together, to form a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 2}, page_content='in new Block objects. Later, either explicitly or when certain\\nmethods are called in DataFrame, blocks having the same\\ntype will be consolidated, i.e. combined together, to form a\\nsingle homogeneously-typed Block:\\n>>> data[’newcol’] = 1.\\n>>> data._data\\nBlockManager\\nItems: [item date price volume ind newcol]\\nAxis 1: [0 1 2 3 4 5 6 7]\\nFloatBlock: [price volume], 2 x 8\\nObjectBlock: [item date], 2 x 8\\nBoolBlock: [ind], 1 x 8\\nFloatBlock: [newcol], 1 x 8\\n>>> data.consolidate()._data\\nBlockManager\\nItems: [item date price volume ind newcol]\\nAxis 1: [0 1 2 3 4 5 6 7]\\nBoolBlock: [ind], 1 x 8\\nFloatBlock: [price volume newcol], 3 x 8\\nObjectBlock: [item date], 2 x 8\\nThe separation between the internal BlockManager ob-\\nject and the external, user-facing DataFrame gives the pan-\\ndas developers a signiﬁcant amount of freedom to modify the\\ninternal structure to achieve better performance and memory\\nusage.\\nLabel-based data access\\nWhile standard []-based indexing (using __getitem__'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 2}, page_content='internal structure to achieve better performance and memory\\nusage.\\nLabel-based data access\\nWhile standard []-based indexing (using __getitem__\\nand __setitem__) is reserved for column access in\\nDataFrame, it is useful to be able to index both axes of\\na DataFrame in a matrix-like way using labels. We would\\nlike to be able to get or set data on any axis using one of the\\nfollowing:\\n• A list or array of labels or integers\\n• A slice, either with integers (e.g. 1:5) or labels (e.g.\\nlab1:lab2)\\n• A boolean vector\\n• A single label\\nTo avoid excessively overloading the []-related methods,\\nleading to ambiguous indexing semantics in some cases, we\\nhave implemented a special label-indexing attribute ix on all\\nof the pandas data structures. Thus, we can pass a tuple of\\nany of the above indexing objects to get or set values.\\n>>> df\\nA\\nB\\nC\\nD\\n2000-01-03 -0.2047\\n1.007\\n-0.5397 -0.7135\\n2000-01-04\\n0.4789 -1.296\\n0.477\\n-0.8312\\n2000-01-05 -0.5194\\n0.275\\n3.249\\n-2.37\\n2000-01-06 -0.5557\\n0.2289 -1.021\\n-1.861'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 2}, page_content='>>> df\\nA\\nB\\nC\\nD\\n2000-01-03 -0.2047\\n1.007\\n-0.5397 -0.7135\\n2000-01-04\\n0.4789 -1.296\\n0.477\\n-0.8312\\n2000-01-05 -0.5194\\n0.275\\n3.249\\n-2.37\\n2000-01-06 -0.5557\\n0.2289 -1.021\\n-1.861\\n2000-01-07\\n1.966\\n1.353\\n-0.5771 -0.8608\\n>>> df.ix[:2, [’D’, ’C’, ’A’]]\\nD\\nC\\nA\\n2000-01-03 -0.7135 -0.5397 -0.2047\\n2000-01-04 -0.8312\\n0.477\\n0.4789\\n>>> df.ix[-2:, ’B’:]\\nB\\nC\\nD\\n2000-01-06\\n0.2289 -1.021\\n-1.861\\n2000-01-07\\n1.353\\n-0.5771 -0.8608\\nSetting values also works as expected.\\n>>> date1, date2 = df.index[[1, 3]]\\n>>> df.ix[date1:date2, [’A’, ’C’]] = 0\\n>>> df\\nA\\nB\\nC\\nD\\n2000-01-03 -0.6856\\n0.1362\\n0.3996\\n1.585\\n2000-01-04\\n0\\n0.8863\\n0\\n1.907\\n2000-01-05\\n0\\n-1.351\\n0\\n0.104\\n2000-01-06\\n0\\n-0.8863\\n0\\n0.1741\\n2000-01-07 -0.05927 -1.013\\n0.9923 -0.4395'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 3}, page_content='4\\nData alignment\\nOperations between related, but differently-sized data sets can\\npose a problem as the user must ﬁrst ensure that the data points\\nare properly aligned. As an example, consider time series over\\ndifferent date ranges or economic data series over varying sets\\nof entities:\\n>>> s1\\n>>> s2\\nAAPL\\n0.044\\nAAPL\\n0.025\\nIBM\\n0.050\\nBAR\\n0.158\\nSAP\\n0.101\\nC\\n0.028\\nGOOG\\n0.113\\nDB\\n0.087\\nC\\n0.138\\nF\\n0.004\\nSCGLY\\n0.037\\nGOOG\\n0.154\\nBAR\\n0.200\\nIBM\\n0.034\\nDB\\n0.281\\nVW\\n0.040\\nOne might choose to explicitly align (or reindex) one of\\nthese 1D Series objects with the other before adding them,\\nusing the reindex method:\\n>>> s1.reindex(s2.index)\\nAAPL\\n0.0440877763224\\nBAR\\n0.199741007422\\nC\\n0.137747485628\\nDB\\n0.281070058049\\nF\\nNaN\\nGOOG\\n0.112861123629\\nIBM\\n0.0496445829129\\nHowever, we often ﬁnd it preferable to simply ignore the\\nstate of data alignment:\\n>>> s1 + s2\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nF\\nNaN\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\nSAP\\nNaN\\nSCGLY\\nNaN\\nVW\\nNaN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 3}, page_content='state of data alignment:\\n>>> s1 + s2\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nF\\nNaN\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\nSAP\\nNaN\\nSCGLY\\nNaN\\nVW\\nNaN\\nHere, the data have been automatically aligned based on\\ntheir labels and added together. The result object contains\\nthe union of the labels between the two objects so that no\\ninformation is lost. We will discuss the use of NaN (Not a\\nNumber) to represent missing data in the next section.\\nClearly, the user pays linear overhead whenever automatic\\ndata alignment occurs and we seek to minimize that overhead\\nto the extent possible. Reindexing can be avoided when\\nIndex objects are shared, which can be an effective strategy\\nin performance-sensitive applications. [Cython], a widely-\\nused tool for creating Python C extensions and interfacing\\nwith C/C++ code, has been utilized to speed up these core\\nalgorithms.\\nData alignment using DataFrame occurs automatically'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 3}, page_content='used tool for creating Python C extensions and interfacing\\nwith C/C++ code, has been utilized to speed up these core\\nalgorithms.\\nData alignment using DataFrame occurs automatically\\non both the column and row labels. This deeply integrated\\ndata alignment differs from any other tools outside of Python\\nthat we are aware of. Similar to the above, if the columns\\nthemselves are different, the resulting object will contain the\\nunion of the columns:\\n>>> df\\n>>> df2\\nAAPL\\nGOOG\\nAAPL\\n2009-12-28\\n211.6\\n622.9\\n2009-12-28\\n2.3e+07\\n2009-12-29\\n209.1\\n619.4\\n2009-12-29\\n1.587e+07\\n2009-12-30\\n211.6\\n622.7\\n2009-12-30\\n1.47e+07\\n2009-12-31\\n210.7\\n620\\n>>> df / df2\\nAAPL\\nGOOG\\n2009-12-28\\n9.199e-06\\nNaN\\n2009-12-29\\n1.318e-05\\nNaN\\n2009-12-30\\n1.44e-05\\nNaN\\n2009-12-31\\nNaN\\nNaN\\nThis may seem like a simple feature, but in practice it grants\\nimmense freedom as there is no longer a need to sanitize\\ndata from an untrusted source. For example, if you loaded\\ntwo data sets from a database and the columns and rows,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 3}, page_content='immense freedom as there is no longer a need to sanitize\\ndata from an untrusted source. For example, if you loaded\\ntwo data sets from a database and the columns and rows,\\nthey can be added together, say, without having to do any\\nchecking whether the labels are aligned. Of course, after doing\\nan operation between two data sets, you can perform an ad\\nhoc cleaning of the results using such functions as fillna\\nand dropna:\\n>>> (df / df2).fillna(0)\\nAAPL\\nGOOG\\n2009-12-28\\n9.199e-06\\n0\\n2009-12-29\\n1.318e-05\\n0\\n2009-12-30\\n1.44e-05\\n0\\n2009-12-31\\n0\\n0\\n>>> (df / df2).dropna(axis=1, how=’all’)\\nAAPL\\n2009-12-28\\n9.199e-06\\n2009-12-29\\n1.318e-05\\n2009-12-30\\n1.44e-05\\n2009-12-31\\nNaN\\nHandling missing data\\nIt is common for a data set to have missing observations.\\nFor example, a group of related economic time series stored\\nin a DataFrame may start on different dates. Carrying\\nout calculations in the presence of missing data can lead\\nboth to complicated code and considerable performance loss.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 3}, page_content='in a DataFrame may start on different dates. Carrying\\nout calculations in the presence of missing data can lead\\nboth to complicated code and considerable performance loss.\\nWe chose to use NaN as opposed to using the NumPy\\nMaskedArray object for performance reasons (which are\\nbeyond the scope of this paper), as NaN propagates in ﬂoating-\\npoint operations in a natural way and can be easily detected\\nin algorithms. While this leads to good performance, it comes\\nwith drawbacks: namely that NaN cannot be used in integer-\\ntype arrays, and it is not an intuitive “null” value in object or\\nstring arrays (though it is used in these arrays regardless).\\nWe regard the use of NaN as an implementation detail and\\nattempt to provide the user with appropriate API functions for\\nperforming common operations on missing data points. From\\nthe above example, we can use the dropna method to drop\\nmissing data, or we could use fillna to replace missing data\\nwith a speciﬁc value:\\n>>> (s1 + s2).dropna()\\nAAPL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 3}, page_content='the above example, we can use the dropna method to drop\\nmissing data, or we could use fillna to replace missing data\\nwith a speciﬁc value:\\n>>> (s1 + s2).dropna()\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\n>>> (s1 + s2).fillna(0)\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 4}, page_content='5\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nF\\n0.0\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\nSAP\\n0.0\\nSCGLY\\n0.0\\nVW\\n0.0\\nThe reindex and fillna methods are equipped with\\na couple simple interpolation options to propagate values\\nforward and backward, which is especially useful for time\\nseries data:\\n>>> ts\\n>>> ts2\\n2000-01-03\\n0.03825\\n2000-01-03\\n0.03825\\n2000-01-04\\n-1.9884\\n2000-01-06\\n-0.0588\\n2000-01-05\\n0.73255\\n2000-01-11\\n0.04410\\n2000-01-06\\n-0.0588\\n2000-01-14\\n-0.1786\\n2000-01-07\\n-0.4767\\n2000-01-10\\n1.98008\\n2000-01-11\\n0.04410\\n>>> ts3 = ts + ts2\\n>>> ts3\\n>>> ts3.fillna(method=’ffill’)\\n2000-01-03\\n0.07649\\n2000-01-03\\n0.07649\\n2000-01-04\\nNaN\\n2000-01-04\\n0.07649\\n2000-01-05\\nNaN\\n2000-01-05\\n0.07649\\n2000-01-06\\n-0.1177\\n2000-01-06\\n-0.1177\\n2000-01-07\\nNaN\\n2000-01-07\\n-0.1177\\n2000-01-10\\nNaN\\n2000-01-10\\n-0.1177\\n2000-01-11\\n0.08821\\n2000-01-11\\n0.08821\\n2000-01-14\\nNaN\\n2000-01-14\\n0.08821\\nSeries and DataFrame also have explicit arithmetic\\nmethods with which a fill_value can be used to specify'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 4}, page_content='2000-01-10\\n-0.1177\\n2000-01-11\\n0.08821\\n2000-01-11\\n0.08821\\n2000-01-14\\nNaN\\n2000-01-14\\n0.08821\\nSeries and DataFrame also have explicit arithmetic\\nmethods with which a fill_value can be used to specify\\na treatment of missing data in the computation. An occasional\\nchoice is to treat missing values as 0 when adding two\\nSeries objects:\\n>>> ts.add(ts2, fill_value=0)\\n2000-01-03\\n0.0764931953608\\n2000-01-04\\n-1.98842046359\\n2000-01-05\\n0.732553684194\\n2000-01-06\\n-0.117727627078\\n2000-01-07\\n-0.476754320696\\n2000-01-10\\n1.9800873096\\n2000-01-11\\n0.0882102892097\\n2000-01-14\\n-0.178640361674\\nCommon ndarray methods have been rewritten to auto-\\nmatically exclude missing data from calculations:\\n>>> (s1 + s2).sum()\\n1.3103630754662747\\n>>> (s1 + s2).count()\\n6\\nSimilar to R’s is.na function, which detects NA (Not Avail-\\nable) values, pandas has special API functions isnull and\\nnotnull for determining the validity of a data point. These\\ncontrast with numpy.isnan in that they can be used with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 4}, page_content='able) values, pandas has special API functions isnull and\\nnotnull for determining the validity of a data point. These\\ncontrast with numpy.isnan in that they can be used with\\ndtypes other than float and also detect some other markers\\nfor “missing” occurring in the wild, such as the Python None\\nvalue.\\n>>> isnull(s1 + s2)\\nAAPL\\nFalse\\nBAR\\nFalse\\nC\\nFalse\\nDB\\nFalse\\nF\\nTrue\\nGOOG\\nFalse\\nIBM\\nFalse\\nSAP\\nTrue\\nSCGLY\\nTrue\\nVW\\nTrue\\nNote that R’s NA value is distinct from NaN. NumPy core\\ndevelopers are currently working on an NA value implementa-\\ntion that will hopefully suit the needs of libraries like pandas\\nin the future.\\nHierarchical Indexing\\nA relatively recent addition to pandas is the ability for an\\naxis to have a hierarchical index, known in the library as a\\nMultiIndex. Semantically, this means that each a location\\non a single axis can have multiple labels associated with it.\\n>>> hdf\\nA\\nB\\nC\\nfoo\\none\\n-0.9884\\n0.09406\\n1.263\\ntwo\\n1.29\\n0.08242 -0.05576\\nthree\\n0.5366\\n-0.4897\\n0.3694\\nbar\\none\\n-0.03457 -2.484'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 4}, page_content='on a single axis can have multiple labels associated with it.\\n>>> hdf\\nA\\nB\\nC\\nfoo\\none\\n-0.9884\\n0.09406\\n1.263\\ntwo\\n1.29\\n0.08242 -0.05576\\nthree\\n0.5366\\n-0.4897\\n0.3694\\nbar\\none\\n-0.03457 -2.484\\n-0.2815\\ntwo\\n0.03071\\n0.1091\\n1.126\\nbaz\\ntwo\\n-0.9773\\n1.474\\n-0.06403\\nthree -1.283\\n0.7818\\n-1.071\\nqux\\none\\n0.4412\\n2.354\\n0.5838\\ntwo\\n0.2215\\n-0.7445\\n0.7585\\nthree\\n1.73\\n-0.965\\n-0.8457\\nHierarchical indexing can be viewed as a way to represent\\nhigher-dimensional data in a lower-dimensional data structure\\n(here, a 2D DataFrame). For example, we can select rows\\nfrom the above DataFrame by specifying only a label from\\nthe left-most level of the index:\\n>>> hdf.ix[’foo’]\\nA\\nB\\nC\\none\\n-0.9884\\n0.09406\\n1.263\\ntwo\\n1.29\\n0.08242 -0.05576\\nthree\\n0.5366\\n-0.4897\\n0.3694\\nOf course, if all of the levels are speciﬁed, we can select a\\nrow or column just as with a regular Index.\\n>>> hdf.ix[’foo’, ’three’]\\nA\\n0.5366\\nB\\n-0.4897\\nC\\n0.3694\\n# same result\\n>>> hdf.ix[’foo’].ix[’three’]\\nThe hierarchical index can be used with any axis. From the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 4}, page_content='>>> hdf.ix[’foo’, ’three’]\\nA\\n0.5366\\nB\\n-0.4897\\nC\\n0.3694\\n# same result\\n>>> hdf.ix[’foo’].ix[’three’]\\nThe hierarchical index can be used with any axis. From the\\npivot example earlier in the paper we obtained:\\n>>> pivoted = data.pivot(’date’, ’item’)\\n>>> pivoted\\nprice\\nvolume\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\n>>> pivoted[’volume’]\\nAAPL\\nGOOG\\n2009-12-28\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n1.257e+07\\n1.22e+06\\nThere are several utility methods for manipulating a\\nMultiIndex such as swaplevel and sortlevel:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 5}, page_content='6\\n>>> swapped = pivoted.swaplevel(0, 1, axis=1)\\n>>> swapped\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\nprice\\nprice\\nvolume\\nvolume\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\n>>> swapped[’AAPL’]\\nprice\\nvolume\\n2009-12-28\\n211.6\\n2.3e+07\\n2009-12-29\\n209.1\\n1.587e+07\\n2009-12-30\\n211.6\\n1.47e+07\\n2009-12-31\\n210.7\\n1.257e+07\\nHere is an example for sortlevel:\\n>>> pivoted.sortlevel(1, axis=1)\\nprice\\nvolume\\nprice\\nvolume\\nAAPL\\nAAPL\\nGOOG\\nGOOG\\n2009-12-28\\n211.6\\n2.3e+07\\n622.9\\n1.698e+06\\n2009-12-29\\n209.1\\n1.587e+07\\n619.4\\n1.425e+06\\n2009-12-30\\n211.6\\n1.47e+07\\n622.7\\n1.466e+06\\n2009-12-31\\n210.7\\n1.257e+07\\n620\\n1.22e+06\\nAdvanced pivoting and reshaping\\nClosely related to hierarchical indexing and the earlier pivoting\\nexample, we illustrate more advanced reshaping of data using\\nthe stack and unstack methods. stack reshapes by\\nremoving a level from the columns of a DataFrame object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 5}, page_content='example, we illustrate more advanced reshaping of data using\\nthe stack and unstack methods. stack reshapes by\\nremoving a level from the columns of a DataFrame object\\nand moving that level to the row labels, producing either a\\n1D Series or another DataFrame (if the columns were a\\nMultiIndex).\\n>>> df\\nAAPL\\nGOOG\\n2009-12-28\\n211.6\\n622.9\\n2009-12-29\\n209.1\\n619.4\\n2009-12-30\\n211.6\\n622.7\\n2009-12-31\\n210.7\\n620\\n>>> df.stack()\\n2009-12-28\\nAAPL\\n211.61\\nGOOG\\n622.87\\n2009-12-29\\nAAPL\\n209.1\\nGOOG\\n619.4\\n2009-12-30\\nAAPL\\n211.64\\nGOOG\\n622.73\\n2009-12-31\\nAAPL\\n210.73\\nGOOG\\n619.98\\n>>> pivoted\\nprice\\nvolume\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\n>>> pivoted.stack()\\nprice\\nvolume\\n2009-12-28\\nAAPL\\n211.6\\n2.3e+07\\nGOOG\\n622.9\\n1.698e+06\\n2009-12-29\\nAAPL\\n209.1\\n1.587e+07\\nGOOG\\n619.4\\n1.425e+06\\n2009-12-30\\nAAPL\\n211.6\\n1.47e+07\\nGOOG\\n622.7\\n1.466e+06\\n2009-12-31\\nAAPL\\n210.7\\n1.257e+07\\nGOOG\\n620'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 5}, page_content='2009-12-28\\nAAPL\\n211.6\\n2.3e+07\\nGOOG\\n622.9\\n1.698e+06\\n2009-12-29\\nAAPL\\n209.1\\n1.587e+07\\nGOOG\\n619.4\\n1.425e+06\\n2009-12-30\\nAAPL\\n211.6\\n1.47e+07\\nGOOG\\n622.7\\n1.466e+06\\n2009-12-31\\nAAPL\\n210.7\\n1.257e+07\\nGOOG\\n620\\n1.22e+06\\nBy default, the innermost level is stacked. The level to stack\\ncan be speciﬁed explicitly:\\n>>> pivoted.stack(0)\\nAAPL\\nGOOG\\n2009-12-28\\nprice\\n211.6\\n622.9\\nvolume\\n2.3e+07\\n1.698e+06\\n2009-12-29\\nprice\\n209.1\\n619.4\\nvolume\\n1.587e+07\\n1.425e+06\\n2009-12-30\\nprice\\n211.6\\n622.7\\nvolume\\n1.47e+07\\n1.466e+06\\n2009-12-31\\nprice\\n210.7\\n620\\nvolume\\n1.257e+07\\n1.22e+06\\nThe unstack method is the inverse of stack:\\n>>> df.stack()\\n>>> df.stack().unstack()\\n2009-12-28\\nAAPL\\n211.61\\nAAPL\\nGOOG\\nGOOG\\n622.87\\n2009-12-28\\n211.6\\n622.9\\n2009-12-29\\nAAPL\\n209.1\\n2009-12-29\\n209.1\\n619.4\\nGOOG\\n619.4\\n2009-12-30\\n211.6\\n622.7\\n2009-12-30\\nAAPL\\n211.64\\n2009-12-31\\n210.7\\n620\\nGOOG\\n622.73\\n2009-12-31\\nAAPL\\n210.73\\nGOOG\\n619.98\\nThese reshaping methods can be combined with built-in\\nDataFrame and Series method to select or aggregate data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 5}, page_content='AAPL\\n211.64\\n2009-12-31\\n210.7\\n620\\nGOOG\\n622.73\\n2009-12-31\\nAAPL\\n210.73\\nGOOG\\n619.98\\nThese reshaping methods can be combined with built-in\\nDataFrame and Series method to select or aggregate data\\nat a level. Here we take the maximum among AAPL and GOOG\\nfor each date / ﬁeld pair:\\n>>> pivoted.stack(0)\\nAAPL\\nGOOG\\n2009-12-28\\nprice\\n211.6\\n622.9\\nvolume\\n2.3e+07\\n1.698e+06\\n2009-12-29\\nprice\\n209.1\\n619.4\\nvolume\\n1.587e+07\\n1.425e+06\\n2009-12-30\\nprice\\n211.6\\n622.7\\nvolume\\n1.47e+07\\n1.466e+06\\n2009-12-31\\nprice\\n210.7\\n620\\nvolume\\n1.257e+07\\n1.22e+06\\n>>> pivoted.stack(0).max(1).unstack()\\nprice\\nvolume\\n2009-12-28\\n622.9\\n2.3e+07\\n2009-12-29\\n619.4\\n1.587e+07\\n2009-12-30\\n622.7\\n1.47e+07\\n2009-12-31\\n620\\n1.257e+07\\nThese kinds of aggregations are closely related to “group\\nby” operations which we discuss in the next section.\\nGroup By: grouping and aggregating data\\nA very common operation in SQL-like languages and gen-\\nerally in statistical data analysis is to group data by some'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 5}, page_content='Group By: grouping and aggregating data\\nA very common operation in SQL-like languages and gen-\\nerally in statistical data analysis is to group data by some\\nidentiﬁers and perform either an aggregation or transformation\\nof the data. For example, suppose we had a simple data set\\nlike this:\\n>>> df\\nA\\nB\\nC\\nD\\n0\\nfoo\\none\\n-1.834\\n1.903\\n1\\nbar\\none\\n1.772\\n-0.7472\\n2\\nfoo\\ntwo\\n-0.67\\n-0.309\\n3\\nbar\\nthree\\n0.04931\\n0.3939\\n4\\nfoo\\ntwo\\n-0.5215\\n1.861\\n5\\nbar\\ntwo\\n-3.202\\n0.9365\\n6\\nfoo\\none\\n0.7927\\n1.256\\n7\\nfoo\\nthree\\n0.1461\\n-2.655\\nWe could compute group means using the A column like\\nso:\\n>>> df.groupby(’A’).mean()\\nC\\nD\\nbar -0.4602\\n0.1944'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 6}, page_content='7\\nfoo -0.4173\\n0.4112\\nThe object returned by groupby is a special intermediate\\nobject with a lot of nice features. For example, you can use\\nit to iterate through the portions of the data set corresponding\\nto each group:\\n>>> for key, group in df.groupby(’A’):\\n...\\nprint key\\n...\\nprint group\\nbar\\nA\\nB\\nC\\nD\\n1\\nbar\\none\\n1.772\\n-0.7472\\n3\\nbar\\nthree\\n0.04931\\n0.3939\\n5\\nbar\\ntwo\\n-3.202\\n0.9365\\nfoo\\nA\\nB\\nC\\nD\\n0\\nfoo\\none\\n-1.834\\n1.903\\n2\\nfoo\\ntwo\\n-0.67\\n-0.309\\n4\\nfoo\\ntwo\\n-0.5215\\n1.861\\n6\\nfoo\\none\\n0.7927\\n1.256\\n7\\nfoo\\nthree\\n0.1461 -2.65\\nGrouping by multiple columns is also possible:\\ndf.groupby([’A’, ’B’]).mean()\\nC\\nD\\nbar\\none\\n1.772\\n-0.7472\\nthree\\n0.04931\\n0.3939\\ntwo\\n-3.202\\n0.9365\\nfoo\\none\\n-0.5205\\n1.579\\nthree\\n0.1461\\n-2.655\\ntwo\\n-0.5958\\n0.7762\\nThe default result of a multi-key groupby aggregation\\nis a hierarchical index. This can be disabled when calling\\ngroupby which may be useful in some settings:\\ndf.groupby([’A’, ’B’], as_index=False).mean()\\nA\\nB\\nC\\nD\\n0\\nbar\\none\\n1.772\\n-0.7472\\n1\\nbar\\nthree\\n0.04931\\n0.3939\\n2\\nbar\\ntwo\\n-3.202\\n0.9365\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 6}, page_content='groupby which may be useful in some settings:\\ndf.groupby([’A’, ’B’], as_index=False).mean()\\nA\\nB\\nC\\nD\\n0\\nbar\\none\\n1.772\\n-0.7472\\n1\\nbar\\nthree\\n0.04931\\n0.3939\\n2\\nbar\\ntwo\\n-3.202\\n0.9365\\n3\\nfoo\\none\\n-0.5205\\n1.579\\n4\\nfoo\\nthree\\n0.1461\\n-2.655\\n5\\nfoo\\ntwo\\n-0.5958\\n0.7762\\nIn a completely general setting, groupby operations are\\nabout mapping axis labels to buckets. In the above examples,\\nwhen we pass column names we are simply establishing a cor-\\nrespondence between the row labels and the group identiﬁers.\\nThere are other ways to do this; the most general is to pass a\\nPython function (for single-key) or list of functions (for multi-\\nkey) which will be invoked on each each label, producing a\\ngroup speciﬁcation:\\n>>> dat\\nA\\nB\\nC\\nD\\n2000-01-03\\n0.6371\\n0.672\\n0.9173\\n1.674\\n2000-01-04 -0.8178 -1.865\\n-0.23\\n0.5411\\n2000-01-05\\n0.314\\n0.2931 -0.6444 -0.9973\\n2000-01-06\\n1.913\\n-0.5867\\n0.273\\n0.4631\\n2000-01-07\\n1.308\\n0.426\\n-1.306\\n0.04358\\n>>> mapping\\n{’A’: ’Group 1’, ’B’: ’Group 2’,\\n’C’: ’Group 1’, ’D’: ’Group 2’}'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 6}, page_content='0.5411\\n2000-01-05\\n0.314\\n0.2931 -0.6444 -0.9973\\n2000-01-06\\n1.913\\n-0.5867\\n0.273\\n0.4631\\n2000-01-07\\n1.308\\n0.426\\n-1.306\\n0.04358\\n>>> mapping\\n{’A’: ’Group 1’, ’B’: ’Group 2’,\\n’C’: ’Group 1’, ’D’: ’Group 2’}\\n>>> for name, group in dat.groupby(mapping.get,\\n...\\naxis=1):\\n...\\nprint name; print group\\nGroup 1\\nA\\nC\\n2000-01-03\\n0.6371\\n0.9173\\n2000-01-04 -0.8178 -0.23\\n2000-01-05\\n0.314\\n-0.6444\\n2000-01-06\\n1.913\\n0.273\\n2000-01-07\\n1.308\\n-1.306\\nGroup 2\\nB\\nD\\n2000-01-03\\n0.672\\n1.674\\n2000-01-04 -1.865\\n0.5411\\n2000-01-05\\n0.2931 -0.9973\\n2000-01-06 -0.5867\\n0.4631\\n2000-01-07\\n0.426\\n0.04358\\nSome creativity with grouping functions will enable the\\nuser to perform quite sophisticated operations. The object re-\\nturned by groupby can either iterate, aggregate (with an\\narbitrary function), transform (compute a modiﬁed same-\\nsize version of each data group), or do a general apply-by-\\ngroup. While we do not have space to go into great detail with\\nexamples of each of these, the apply function is interesting in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 6}, page_content='size version of each data group), or do a general apply-by-\\ngroup. While we do not have space to go into great detail with\\nexamples of each of these, the apply function is interesting in\\nthat it attempts to combine the results of the aggregation into\\na pandas object. For example, we could group the df object\\nabove by column A, select just the C column, and apply the\\ndescribe function to each subgroup like so:\\n>>> df.groupby(’A’)[’C’].describe().T\\nbar\\nfoo\\ncount\\n3\\n5\\nmean\\n-0.4602\\n-0.4173\\nstd\\n2.526\\n0.9827\\nmin\\n-3.202\\n-1.834\\n10%\\n-2.552\\n-1.368\\n50%\\n0.04931 -0.5215\\n90%\\n1.427\\n0.5341\\nmax\\n1.772\\n0.7927\\nNote that, under the hood, calling describe generates\\nand passes a dynamic function to apply which invokes\\ndescribe on each group and glues the results together. We\\ntransposed the result with .T to make it more readable.\\nEasy spreadsheet-style pivot tables\\nAn obvious application combining groupby and reshaping\\noperations is creating pivot tables, a common way of sum-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 6}, page_content='Easy spreadsheet-style pivot tables\\nAn obvious application combining groupby and reshaping\\noperations is creating pivot tables, a common way of sum-\\nmarizing data in spreadsheet applications such as Microsoft\\nExcel. We’ll take a brief look at a tipping data set collected\\nfrom a restaurant ([Bryant]):\\n>>> tips.head()\\nsex\\nsmoker\\ntime\\nday\\nsize\\ntip_pct\\n1\\nFemale\\nNo\\nDinner\\nSun\\n2\\n0.05945\\n2\\nMale\\nNo\\nDinner\\nSun\\n3\\n0.1605\\n3\\nMale\\nNo\\nDinner\\nSun\\n3\\n0.1666\\n4\\nMale\\nNo\\nDinner\\nSun\\n2\\n0.1398\\n5\\nFemale\\nNo\\nDinner\\nSun\\n4\\n0.1468\\nThe pivot_table function in pandas takes a set of\\ncolumn names to group on the pivot table rows, another set to\\ngroup on the columns, and optionally an aggregation function\\nfor each group (which defaults to mean):\\n>>> import numpy as np\\n>>> from pandas import pivot_table\\n>>> pivot_table(tips, ’tip_pct’, rows=[’time’, ’sex’],\\ncols=’smoker’)\\nsmoker\\nNo\\nYes\\ntime\\nsex\\nDinner Female\\n0.1568\\n0.1851'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 7}, page_content='8\\nMale\\n0.1594\\n0.1489\\nLunch\\nFemale\\n0.1571\\n0.1753\\nMale\\n0.1657\\n0.1667\\nConveniently, the returned object is a DataFrame, so it can\\nbe further reshaped and manipulated by the user:\\n>>> table = pivot_table(tips, ’tip_pct’,\\nrows=[’sex’, ’day’],\\ncols=’smoker’, aggfunc=len)\\n>>> table\\nsmoker\\nNo\\nYes\\nsex\\nday\\nFemale Fri\\n2\\n7\\nSat\\n13\\n15\\nSun\\n14\\n4\\nThur\\n25\\n7\\nMale\\nFri\\n2\\n8\\nSat\\n32\\n27\\nSun\\n43\\n15\\nThur\\n20\\n10\\n>>> table.unstack(’sex’)\\nsmoker\\nNo\\nYes\\nsex\\nFemale\\nMale\\nFemale\\nMale\\nday\\nFri\\n2\\n2\\n7\\n8\\nSat\\n13\\n32\\n15\\n27\\nSun\\n14\\n43\\n4\\n15\\nThur\\n25\\n20\\n7\\n10\\nFor many users, this will be an attractive alternative to\\ndumping a data set into a spreadsheet for the sole purpose\\nof creating a pivot table.\\n>>> pivot_table(tips, ’size’,\\nrows=[’time’, ’sex’, ’smoker’],\\ncols=’day’, aggfunc=np.sum,\\nfill_value=0)\\nday\\nFri\\nSat\\nSun\\nThur\\ntime\\nsex\\nsmoker\\nDinner Female No\\n2\\n30\\n43\\n2\\nYes\\n8\\n33\\n10\\n0\\nDinner Male\\nNo\\n4\\n85\\n124\\n0\\nYes\\n12\\n71\\n39\\n0\\nLunch\\nFemale No\\n3\\n0\\n0\\n60\\nYes\\n6\\n0\\n0\\n17\\nLunch\\nMale\\nNo\\n0\\n0\\n0\\n50\\nYes\\n5\\n0\\n0\\n23\\nCombining or joining data sets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 7}, page_content='sex\\nsmoker\\nDinner Female No\\n2\\n30\\n43\\n2\\nYes\\n8\\n33\\n10\\n0\\nDinner Male\\nNo\\n4\\n85\\n124\\n0\\nYes\\n12\\n71\\n39\\n0\\nLunch\\nFemale No\\n3\\n0\\n0\\n60\\nYes\\n6\\n0\\n0\\n17\\nLunch\\nMale\\nNo\\n0\\n0\\n0\\n50\\nYes\\n5\\n0\\n0\\n23\\nCombining or joining data sets\\nCombining, joining, or merging related data sets is a quite\\ncommon operation. In doing so we are interested in associating\\nobservations from one data set with another via a merge key\\nof some kind. For similarly-indexed 2D data, the row labels\\nserve as a natural key for the join function:\\n>>> df1\\n>>> df2\\nAAPL\\nGOOG\\nMSFT\\nYHOO\\n2009-12-24\\n209\\n618.5\\n2009-12-24\\n31\\n16.72\\n2009-12-28\\n211.6\\n622.9\\n2009-12-28\\n31.17\\n16.88\\n2009-12-29\\n209.1\\n619.4\\n2009-12-29\\n31.39\\n16.92\\n2009-12-30\\n211.6\\n622.7\\n2009-12-30\\n30.96\\n16.98\\n2009-12-31\\n210.7\\n620\\n>>> df1.join(df2)\\nAAPL\\nGOOG\\nMSFT\\nYHOO\\n2009-12-24\\n209\\n618.5\\n31\\n16.72\\n2009-12-28\\n211.6\\n622.9\\n31.17\\n16.88\\n2009-12-29\\n209.1\\n619.4\\n31.39\\n16.92\\n2009-12-30\\n211.6\\n622.7\\n30.96\\n16.98\\n2009-12-31\\n210.7\\n620\\nNaN\\nNaN\\nOne might be interested in joining on something other than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 7}, page_content='16.72\\n2009-12-28\\n211.6\\n622.9\\n31.17\\n16.88\\n2009-12-29\\n209.1\\n619.4\\n31.39\\n16.92\\n2009-12-30\\n211.6\\n622.7\\n30.96\\n16.98\\n2009-12-31\\n210.7\\n620\\nNaN\\nNaN\\nOne might be interested in joining on something other than\\nthe index as well, such as the categorical data we presented\\nin an earlier section:\\n>>> data.join(cats, on=’item’)\\ncountry\\ndate\\nindustry item\\nvalue\\n0\\nUS\\n2009-12-28\\nTECH\\nGOOG\\n622.9\\n1\\nUS\\n2009-12-29\\nTECH\\nGOOG\\n619.4\\n2\\nUS\\n2009-12-30\\nTECH\\nGOOG\\n622.7\\n3\\nUS\\n2009-12-31\\nTECH\\nGOOG\\n620\\n4\\nUS\\n2009-12-28\\nTECH\\nAAPL\\n211.6\\n5\\nUS\\n2009-12-29\\nTECH\\nAAPL\\n209.1\\n6\\nUS\\n2009-12-30\\nTECH\\nAAPL\\n211.6\\n7\\nUS\\n2009-12-31\\nTECH\\nAAPL\\n210.7\\nThis is akin to a SQL join operation between two tables\\nor a VLOOKUP operation in a spreadsheet such as Excel. It\\nis possible to join on multiple keys, in which case the table\\nbeing joined is currently required to have a hierarchical index\\ncorresponding to those keys. We will be working on more\\njoining and merging methods in a future release of pandas.\\nPerformance and use for Large Data Sets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 7}, page_content='corresponding to those keys. We will be working on more\\njoining and merging methods in a future release of pandas.\\nPerformance and use for Large Data Sets\\nUsing DataFrame objects over homogeneous NumPy arrays\\nfor computation incurs overhead from a number of factors:\\n• Computational functions like sum, mean, and std have\\nbeen overridden to omit missing data\\n• Most of the axis Index data structures are reliant on the\\nPython dict for performing lookups and data alignment.\\nThis also results in a slightly larger memory footprint as\\nthe dict containing the label mapping is created once\\nand then stored.\\n• The internal BlockManager data structure consolidates\\nthe data of each type (ﬂoating point, integer, boolean,\\nobject) into 2-dimensional arrays. However, this is an\\nupfront cost that speeds up row-oriented computations\\nand data alignment later.\\n• Performing repeated lookups of values by label passes\\nthrough much more Python code than simple integer-\\nbased lookups on ndarray objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 7}, page_content='and data alignment later.\\n• Performing repeated lookups of values by label passes\\nthrough much more Python code than simple integer-\\nbased lookups on ndarray objects.\\nThe savvy user will learn what operations are not very\\nefﬁcient in DataFrame and Series and fall back on working\\ndirectly with the underlying ndarray objects (accessible\\nvia the values attribute) in such cases. What DataFrame\\nsacriﬁces in performance it makes up for in ﬂexibility and\\nexpressiveness.\\nWith 64-bit integers representing timestamps, pandas in\\nfact provides some of the fastest data alignment routines for\\ndifferently-indexed time series to be found in open source soft-\\nware. As working with large, irregularly time series requires\\nhaving a timestamp index, pandas is well-positioned to become\\nthe gold standard for high performance open source time series\\nprocessing.\\nWith regard to memory usage and large data sets, pandas\\nis currently only designed for use with in-memory data sets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 7}, page_content='the gold standard for high performance open source time series\\nprocessing.\\nWith regard to memory usage and large data sets, pandas\\nis currently only designed for use with in-memory data sets.\\nWe would like to expand its capability to work with data\\nsets that do not ﬁt into memory, perhaps transparently using\\nthe multiprocessing module or a parallel computing\\nbackend to orchestrate large scale computations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 8}, page_content='9\\npandas for R users\\nGiven the “DataFrame” name and feature overlap with the [R]\\nproject and its 3rd party packages, pandas will draw inevitable\\ncomparisons with R. pandas brings a robust, full-featured, and\\nintegrated data analysis toolset to Python while maintaining a\\nsimple and easy-to-use API. As nearly all data manipulations\\ninvolving data.frame objects in R can be easily expressed\\nusing the pandas DataFrame, it is relatively straightforward\\nin most cases to port R functions to Python. It would be\\nuseful to provide a migration guide for R users as we have\\nnot copied R’s naming conventions or syntax in most places,\\nrather naming based on common-sense and making the syntax\\nand API as “Pythonic” as possible.\\nR does not provide indexing functionality in nearly such a\\ndeeply integrated way as pandas does. For example, operations\\nbetween data.frame objects will proceed in R without\\nregard to whether the labels match as long as they are the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 8}, page_content='deeply integrated way as pandas does. For example, operations\\nbetween data.frame objects will proceed in R without\\nregard to whether the labels match as long as they are the\\nsame length and width. Some R packages, such as zoo and\\nxts provides indexed data structures with data alignment,\\nbut they are largely specialized to ordered time series data.\\nHierarchical indexing with constant-time subset selection is\\nanother signiﬁcant feature missing from R’s data structures.\\nOutside of the scope of this paper is a rigorous performance\\ncomparison of R and pandas. In almost all of the benchmarks\\nwe have run comparing R and pandas, pandas signiﬁcantly\\noutperforms R.\\nOther features of note\\nThere are many other features in pandas worth exploring for\\nthe interested users:\\n• Time series functionality: date range generation, shifting\\nand lagging, frequency conversion and forward/backward\\nﬁlling\\n• Integration with [matplotlib] to concisely generate plots\\nwith metadata'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 8}, page_content='• Time series functionality: date range generation, shifting\\nand lagging, frequency conversion and forward/backward\\nﬁlling\\n• Integration with [matplotlib] to concisely generate plots\\nwith metadata\\n• Moving window statistics (e.g. moving standard devia-\\ntion, exponentially weighted moving average) and moving\\nwindow linear and panel regression\\n• 3-dimensional Panel data structure for manipulating\\ncollections of DataFrame objects\\n• Sparse versions of the data structures\\n• Robust IO tools for reading and writing pandas objects to\\nﬂat ﬁles (delimited text, CSV, Excel) and HDF5 format\\nRelated packages\\nA number of other Python packages have some degree of\\nfeature overlap with pandas. Among these, la ([Larry]) is\\nthe most similar, as it implements a labeled ndarray object\\nintending to closely mimic NumPy arrays. Since ndarray\\nis only applicable many problems in its homogeneous (non-\\nstructured dtype) form, in pandas we have distanced our-\\nselves from ndarray to instead provide a more ﬂexible,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 8}, page_content='is only applicable many problems in its homogeneous (non-\\nstructured dtype) form, in pandas we have distanced our-\\nselves from ndarray to instead provide a more ﬂexible,\\n(potentially) heterogeneous, size-mutable data structure. The\\nreferences include a some other packages of interest.\\npandas will soon become a dependency of statsmodels\\n([StaM]), the main statistics and econometric library in Python,\\nto make statistical modeling and data analysis tools in Python\\nmore cohesive and integrated. We plan to combine pandas\\nwith a formula framework to make specifying statistical mod-\\nels easy and intuitive when working with a DataFrame of\\ndata, for example.\\nConclusions\\nWe believe that in the coming years there will be great oppor-\\ntunity to attract users in need of statistical data analysis tools\\nto Python who might have previously chosen R, MATLAB,\\nor another research environment. By designing robust, easy-\\nto-use data structures that cohere with the rest of the scientiﬁc'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 8}, page_content='to Python who might have previously chosen R, MATLAB,\\nor another research environment. By designing robust, easy-\\nto-use data structures that cohere with the rest of the scientiﬁc\\nPython stack, we can make Python a compelling choice for\\ndata analysis applications. In our opinion, pandas provides\\na solid foundation upon which a very powerful data analysis\\necosystem can be established.\\nREFERENCES\\n[pandas]\\nW. McKinney, pandas: a python data analysis library, http:\\n//pandas.sourceforge.net\\n[scipy2010]\\nW. McKinney, Data Structures for Statistical Computing in\\nPython Proceedings of the 9th Python in Science Conference,\\nhttp://http://conference.scipy.org/. 2010\\n[Larry]\\nK. Goodman. la / larry: ndarray with labeled axes, http://larry.\\nsourceforge.net/\\n[SciTS]\\nM. Knox, P. Gerard-Marchant, scikits.timeseries: python time\\nseries analysis, http://pytseries.sourceforge.net/\\n[StaM]\\nS. Seabold, J. Perktold, J. Taylor, statsmodels: statistical\\nmodeling in Python, http://statsmodels.sourceforge.net'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 8}, page_content='series analysis, http://pytseries.sourceforge.net/\\n[StaM]\\nS. Seabold, J. Perktold, J. Taylor, statsmodels: statistical\\nmodeling in Python, http://statsmodels.sourceforge.net\\n[SciL]\\nD. Cournapeau, et al., scikit-learn: machine learning in\\nPython, http://scikit-learn.sourceforge.net\\n[PyMC]\\nC. Fonnesbeck, A. Patil, D. Huard, PyMC: Markov Chain\\nMonte Carlo for Python, http://code.google.com/p/pymc/\\n[Tab]\\nD. Yamins, E. Angelino, tabular: tabarray data structure for\\n2D data, http://parsemydata.com/tabular/\\n[NumPy]\\nT. Oliphant, http://numpy.scipy.org\\n[SciPy]\\nE. Jones, T. Oliphant, P. Peterson, http://scipy.org\\n[matplotlib]\\nJ. Hunter, et al., matplotlib: Python plotting, http://matplotlib.\\nsourceforge.net/\\n[EPD]\\nEnthought, Inc., EPD: Enthought Python Distribution, http:\\n//www.enthought.com/products/epd.php\\n[Pythonxy]\\nP. Raybaut, Python(x,y): Scientiﬁc-oriented Python distribu-\\ntion, http://www.pythonxy.com/\\n[CRAN]\\nThe R Project for Statistical Computing, http://cran.r-project.\\norg/\\n[Cython]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 8}, page_content='[Pythonxy]\\nP. Raybaut, Python(x,y): Scientiﬁc-oriented Python distribu-\\ntion, http://www.pythonxy.com/\\n[CRAN]\\nThe R Project for Statistical Computing, http://cran.r-project.\\norg/\\n[Cython]\\nG. Ewing, R. W. Bradshaw, S. Behnel, D. S. Seljebotn, et al.,\\nThe Cython compiler, http://cython.org\\n[IPython]\\nFernando Pérez, Brian E. Granger, IPython: A System for\\nInteractive Scientiﬁc Computing, Computing in Science and\\nEngineering, vol. 9, no. 3, pp. 21-29, May/June 2007,\\ndoi:10.1109/MCSE.2007.53. http://ipython.org\\n[Grun]\\nBatalgi,\\nGrunfeld\\ndata\\nset,\\nhttp://www.wiley.com/legacy/\\nwileychi/baltagi/\\n[nipy]\\nJ. Taylor, F. Perez, et al., nipy: Neuroimaging in Python, http:\\n//nipy.sourceforge.net\\n[pydataframe] A. Straw, F. Finkernagel, pydataframe, http://code.google.com/\\np/pydataframe/\\n[R]\\nR Development Core Team. 2010, R: A Language and Envi-\\nronment for Statistical Computing, http://www.R-project.org\\n[MATLAB]\\nThe MathWorks Inc. 2010, MATLAB, http://www.mathworks.\\ncom\\n[Stata]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2011-11-04T12:57:54-04:00', 'source': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Python pandas.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2011-11-04T12:57:54-04:00', 'trapped': '', 'modDate': \"D:20111104125754-04'00'\", 'creationDate': \"D:20111104125754-04'00'\", 'page': 8}, page_content='[R]\\nR Development Core Team. 2010, R: A Language and Envi-\\nronment for Statistical Computing, http://www.R-project.org\\n[MATLAB]\\nThe MathWorks Inc. 2010, MATLAB, http://www.mathworks.\\ncom\\n[Stata]\\nStatCorp. 2010, Stata Statistical Software: Release 11 http:\\n//www.stata.com\\n[SAS]\\nSAS Institute Inc., SAS System, http://www.sas.com\\n[Bryant]\\nBryant, P. G. and Smith, M (1995) Practical Data Analysis:\\nCase Studies in Business Statistics. Homewood, IL: Richard\\nD. Irwin Publishing:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 0}, page_content='PyTorch: An Imperative Style, High-Performance\\nDeep Learning Library\\nAdam Paszke\\nUniversity of Warsaw\\nadam.paszke@gmail.com\\nSam Gross\\nFacebook AI Research\\nsgross@fb.com\\nFrancisco Massa\\nFacebook AI Research\\nfmassa@fb.com\\nAdam Lerer\\nFacebook AI Research\\nalerer@fb.com\\nJames Bradbury\\nGoogle\\njekbradbury@gmail.com\\nGregory Chanan\\nFacebook AI Research\\ngchanan@fb.com\\nTrevor Killeen\\nSelf Employed\\nkilleent@cs.washington.edu\\nZeming Lin\\nFacebook AI Research\\nzlin@fb.com\\nNatalia Gimelshein\\nNVIDIA\\nngimelshein@nvidia.com\\nLuca Antiga\\nOrobix\\nluca.antiga@orobix.com\\nAlban Desmaison\\nOxford University\\nalban@robots.ox.ac.uk\\nAndreas Köpf\\nXamla\\nandreas.koepf@xamla.com\\nEdward Yang\\nFacebook AI Research\\nezyang@fb.com\\nZach DeVito\\nFacebook AI Research\\nzdevito@cs.stanford.edu\\nMartin Raison\\nNabla\\nmartinraison@gmail.com\\nAlykhan Tejani\\nTwitter\\natejani@twitter.com\\nSasank Chilamkurthy\\nQure.ai\\nsasankchilamkurthy@gmail.com\\nBenoit Steiner\\nFacebook AI Research\\nbenoitsteiner@fb.com\\nLu Fang\\nFacebook\\nlufang@fb.com\\nJunjie Bai'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 0}, page_content='Alykhan Tejani\\nTwitter\\natejani@twitter.com\\nSasank Chilamkurthy\\nQure.ai\\nsasankchilamkurthy@gmail.com\\nBenoit Steiner\\nFacebook AI Research\\nbenoitsteiner@fb.com\\nLu Fang\\nFacebook\\nlufang@fb.com\\nJunjie Bai\\nFacebook\\njbai@fb.com\\nSoumith Chintala\\nFacebook AI Research\\nsoumith@gmail.com\\nAbstract\\nDeep learning frameworks have often focused on either usability or speed, but\\nnot both. PyTorch is a machine learning library that shows that these two goals\\nare in fact compatible: it provides an imperative and Pythonic programming style\\nthat supports code as a model, makes debugging easy and is consistent with other\\npopular scientiﬁc computing libraries, while remaining efﬁcient and supporting\\nhardware accelerators such as GPUs.\\nIn this paper, we detail the principles that drove the implementation of PyTorch\\nand how they are reﬂected in its architecture. We emphasize that every aspect of\\nPyTorch is a regular Python program under the full control of its user. We also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 0}, page_content='and how they are reﬂected in its architecture. We emphasize that every aspect of\\nPyTorch is a regular Python program under the full control of its user. We also\\nexplain how the careful and pragmatic implementation of the key components of\\nits runtime enables them to work together to achieve compelling performance.\\nWe demonstrate the efﬁciency of individual subsystems, as well as the overall\\nspeed of PyTorch on several common benchmarks.\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\narXiv:1912.01703v1  [cs.LG]  3 Dec 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 1}, page_content='1\\nIntroduction\\nWith the increased interest in deep learning in recent years, there has been an explosion of machine\\nlearning tools. Many popular frameworks such as Caffe [1], CNTK [2], TensorFlow [3], and\\nTheano [4], construct a static dataﬂow graph that represents the computation and which can then be\\napplied repeatedly to batches of data. This approach provides visibility into the whole computation\\nahead of time, and can theoretically be leveraged to improve performance and scalability. However, it\\ncomes at the cost of ease of use, ease of debugging, and ﬂexibility of the types of computation that\\ncan be represented.\\nPrior work has recognized the value of dynamic eager execution for deep learning, and some recent\\nframeworks implement this deﬁne-by-run approach, but do so either at the cost of performance\\n(Chainer [5]) or using a less expressive, faster language (Torch [6], DyNet [7]), which limits their\\napplicability.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 1}, page_content='(Chainer [5]) or using a less expressive, faster language (Torch [6], DyNet [7]), which limits their\\napplicability.\\nHowever, with careful implementation and design choices, dynamic eager execution can be achieved\\nlargely without sacriﬁcing performance. This paper introduces PyTorch, a Python library that\\nperforms immediate execution of dynamic tensor computations with automatic differentiation and\\nGPU acceleration, and does so while maintaining performance comparable to the fastest current\\nlibraries for deep learning. This combination has turned out to be very popular in the research\\ncommunity with, for instance, 296 ICLR 2019 submissions mentioning PyTorch.\\n2\\nBackground\\nFour major trends in scientiﬁc computing have become increasingly important for deep learning.\\nFirst, starting in the 1960s, the development of domain speciﬁc languages such as APL [8], MATLAB\\n[9], R [10] and Julia [11], turned multidimensional arrays (often referred to as tensors) into ﬁrst-class'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 1}, page_content='[9], R [10] and Julia [11], turned multidimensional arrays (often referred to as tensors) into ﬁrst-class\\nobjects supported by a comprehensive set of mathematical primitives (or operators) to manipulate\\nthem. Separately, libraries such as NumPy[12], Torch[6], Eigen[13] and Lush[14] made array-based\\nprogramming productive in general purpose languages such as Python, Lisp, C++ and Lua.\\nSecond, the development of automatic differentiation [15] made it possible to fully automate\\nthe daunting labor of computing derivatives. This made it signiﬁcantly easier to experiment with\\ndifferent machine learning approaches while still allowing for efﬁcient gradient based optimization.\\nThe autograd [16] package popularized the use of this technique for NumPy arrays, and similar\\napproaches are used in frameworks such as Chainer [5], DyNet [7], Lush [14], Torch [6], Jax [17]\\nand Flux.jl [18].\\nThird, with the advent of the free software movement, the scientiﬁc community moved away from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 1}, page_content='and Flux.jl [18].\\nThird, with the advent of the free software movement, the scientiﬁc community moved away from\\nclosed proprietary software such as Matlab[9], and towards the open-source Python ecosystem\\nwith packages like NumPy [12], SciPy [19], and Pandas [20]. This fulﬁlled most of the numerical\\nanalysis needs of researchers while allowing them to take advantage of a vast repository of libraries\\nto handle dataset preprocessing, statistical analysis, plotting, and more. Moreover, the openness,\\ninteroperability, and ﬂexibility of free software fostered the development of vibrant communities that\\ncould quickly address new or changing needs by extending the existing functionality of a library or if\\nneeded by developing and releasing brand new ones. While there is a rich offering of open-source\\nsoftware for neural networks in languages other than Python, starting with Lush [14] in Lisp, Torch [6]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 1}, page_content='software for neural networks in languages other than Python, starting with Lush [14] in Lisp, Torch [6]\\nin C++, Objective-C and Lua, EBLearn [21] in C++, Caffe [1] in C++, the network effects of a large\\necosystem such as Python made it an essential skill to jumpstart one’s research. Hence, since 2014,\\nmost deep learning frameworks converged on a Python interface as an essential feature.\\nFinally, the availability and commoditization of general-purpose massively parallel hardware such\\nas GPUs provided the computing power required by deep learning methods. Specialized libraries\\nsuch as cuDNN [22], along with a body of academic work (such as [23] and [24]), produced a\\nset of high-performance reusable deep learning kernels that enabled frameworks such as Caffe [1],\\nTorch7 [25], or TensorFlow [3] to take advantage of these hardware accelerators.\\nPyTorch builds on these trends by providing an array-based programming model accelerated by GPUs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 1}, page_content='Torch7 [25], or TensorFlow [3] to take advantage of these hardware accelerators.\\nPyTorch builds on these trends by providing an array-based programming model accelerated by GPUs\\nand differentiable via automatic differentiation integrated in the Python ecosystem.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 2}, page_content='3\\nDesign principles\\nPyTorch’s success stems from weaving previous ideas into a design that balances speed and ease of\\nuse. There are four main principles behind our choices:\\nBe Pythonic\\nData scientists are familiar with the Python language, its programming model, and its\\ntools. PyTorch should be a ﬁrst-class member of that ecosystem. It follows the commonly established\\ndesign goals of keeping interfaces simple and consistent, ideally with one idiomatic way of doing\\nthings. It also integrates naturally with standard plotting, debugging, and data processing tools.\\nPut researchers ﬁrst\\nPyTorch strives to make writing models, data loaders, and optimizers as\\neasy and productive as possible. The complexity inherent to machine learning should be handled\\ninternally by the PyTorch library and hidden behind intuitive APIs free of side-effects and unexpected\\nperformance cliffs.\\nProvide pragmatic performance\\nTo be useful, PyTorch needs to deliver compelling performance,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 2}, page_content='performance cliffs.\\nProvide pragmatic performance\\nTo be useful, PyTorch needs to deliver compelling performance,\\nalthough not at the expense of simplicity and ease of use. Trading 10% of speed for a signiﬁcantly\\nsimpler to use model is acceptable; 100% is not. Therefore, its implementation accepts added\\ncomplexity in order to deliver that performance. Additionally, providing tools that allow researchers\\nto manually control the execution of their code will empower them to ﬁnd their own performance\\nimprovements independent of those that the library provides automatically.\\nWorse is better [26]\\nGiven a ﬁxed amount of engineering resources, and all else being equal, the\\ntime saved by keeping the internal implementation of PyTorch simple can be used to implement\\nadditional features, adapt to new situations, and keep up with the fast pace of progress in the ﬁeld of\\nAI. Therefore it is better to have a simple but slightly incomplete solution than a comprehensive but'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 2}, page_content='AI. Therefore it is better to have a simple but slightly incomplete solution than a comprehensive but\\ncomplex and hard to maintain design.\\n4\\nUsability centric design\\n4.1\\nDeep learning models are just Python programs\\nIn a surprisingly short amount of time, machine learning grew from recognizing individual digits [27]\\ninto autonomously playing StarCraft [28]. Consequently, the neural networks themselves evolved\\nrapidly from simple sequences of feed forward layers into incredibly varied numerical programs\\noften composed of many loops and recursive functions. To support this growing complexity, PyTorch\\nforegoes the potential beneﬁts of a graph-metaprogramming based approach to preserve the imperative\\nprogramming model of Python. This design was pioneered for model authoring by Chainer[5] and\\nDynet[7]. PyTorch extends this to all aspects of deep learning workﬂows. Deﬁning layers, composing\\nmodels, loading data, running optimizers, and parallelizing the training process are all expressed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 2}, page_content='Dynet[7]. PyTorch extends this to all aspects of deep learning workﬂows. Deﬁning layers, composing\\nmodels, loading data, running optimizers, and parallelizing the training process are all expressed\\nusing the familiar concepts developed for general purpose programming.\\nThis solution ensures that any new potential neural network architecture can be easily implemented\\nwith PyTorch. For instance, layers (which in modern machine learning should really be understood\\nas stateful functions with implicit parameters) are typically expressed as Python classes whose\\nconstructors create and initialize their parameters, and whose forward methods process an input\\nactivation. Similarly, models are usually represented as classes that compose individual layers, but let\\nus state again that nothing forces the user to structure their code in that way. Listing 1 demonstrates\\nhow an entire model can be created by composing functionality provided by PyTorch such as 2d'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 2}, page_content='us state again that nothing forces the user to structure their code in that way. Listing 1 demonstrates\\nhow an entire model can be created by composing functionality provided by PyTorch such as 2d\\nconvolution, matrix multiplication, dropout, and softmax to classify gray-scale images. Note that\\nlinear layers are of course part of the library, but we show an example implementation to highlight\\nhow simple it is.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 3}, page_content='class LinearLayer(Module):\\nclass FullBasicModel(nn.Module):\\ndef __init__(self, in_sz, out_sz):\\ndef __init__(self):\\nsuper().__init__()\\nsuper().__init__()\\nt1 = torch.randn(in_sz, out_sz)\\nself.conv = nn.Conv2d(1, 128, 3)\\nself.w = nn.Parameter(t1)\\nself.fc = LinearLayer(128, 10)\\nt2 = torch.randn(out_sz)\\nself.b = nn.Parameter(t2)\\ndef forward(self, x):\\nt1 = self.conv(x)\\ndef forward(self, activations):\\nt2 = nn.functional.relu(t1)\\nt = torch.mm(activations, self.w)\\nt3 = self.fc(t1)\\nreturn t + self.b\\nreturn nn.functional.softmax(t3)\\nListing 1: A custom layer used as a building block for a simple but complete neural network.\\nThis “everything is a just a program” philosophy is not limited to just the models, and applies to\\noptimizers and data loaders as well. This facilitates the experimentation of new training techniques.\\nFor example, to implement the very popular generative adversarial networks, one needs to specify'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 3}, page_content='optimizers and data loaders as well. This facilitates the experimentation of new training techniques.\\nFor example, to implement the very popular generative adversarial networks, one needs to specify\\ntwo separate models (the generator and the discriminator), and two loss functions that depend on both\\nmodels at the same time. Rigid APIs would struggle with this setup, but the simple design employed\\nin PyTorch easily adapts to this setting as shown in Listing 2.\\ndiscriminator = create_discriminator()\\ngenerator = create_generator()\\noptimD = optim.Adam(discriminator.parameters())\\noptimG = optim.Adam(generator.parameters())\\ndef step(real_sample):\\n# (1) Update Discriminator\\nerrD_real = loss(discriminator(real_sample), real_label)\\nerrD_real.backward()\\nfake = generator(get_noise())\\nerrD_fake = loss(discriminator(fake.detach(), fake_label)\\nerrD_fake.backward()\\noptimD.step()\\n# (2) Update Generator\\nerrG = loss(discriminator(fake), real_label)\\nerrG.backward()\\noptimG.step()'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 3}, page_content='errD_fake = loss(discriminator(fake.detach(), fake_label)\\nerrD_fake.backward()\\noptimD.step()\\n# (2) Update Generator\\nerrG = loss(discriminator(fake), real_label)\\nerrG.backward()\\noptimG.step()\\nListing 2: Simpliﬁed training of a generative adversarial networks.\\nSince PyTorch programs execute eagerly, all the features of Python are available throughout the\\nwhole design process. Print statements, standard debuggers, and common visualization tools like\\nmatplotlib all work as expected. Users do not have to wait for lengthy compilation before they can\\nstart running their programs, and more importantly intermediate computations can be observed to\\nunderstand how a model works and whether its results are correct.\\n4.2\\nInteroperability and extensibility\\nEasy and efﬁcient interoperability is one of the top priorities for PyTorch because it opens the\\npossibility to leverage the rich ecosystem of Python libraries as part of user programs. Hence,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 3}, page_content='Easy and efﬁcient interoperability is one of the top priorities for PyTorch because it opens the\\npossibility to leverage the rich ecosystem of Python libraries as part of user programs. Hence,\\nPyTorch allows for bidirectional exchange of data with external libraries. For example, it provides\\na mechanism to convert between NumPy arrays and PyTorch tensors using the torch.from_numpy()\\nfunction and .numpy() tensor method. Similar functionality is also available to exchange data stored\\nusing the DLPack [29] format. Note that this exchange happens in both cases without any data\\ncopying – objects on both sides only describe how to interpret a memory region which is shared\\namong them. Hence, those operations are actually extremely cheap, and take constant time no matter\\nhow large the converted arrays are.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 4}, page_content='Moreover, many of the critical systems are designed speciﬁcally to be extensible. For instance, the\\nautomatic differentiation system allows users to add support for custom differentiable functions.\\nTo do that users can deﬁne a new subclass of torch.autograd.Function that implements forward()\\nand backward() methods, which specify the function and its derivative (or more formally the vector-\\nJacobian product). Similarly new datasets can be added by subclassing torch.utils.data.Dataset\\nand implementing two methods: __getitem__ (the indexing operator) and __len__ (the length op-\\nerator), making datasets behave like (possibly lazy) lists. How these work is completely up to the\\nimplementer, and many users leverage other Python packages for data loading. The DataLoader class\\nconsumes objects conforming to this interface and provides an iterator over the data which takes\\ncare of shufﬂing, batching, parallelization, and management of pinned CUDA memory to improve\\nthroughput.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 4}, page_content='care of shufﬂing, batching, parallelization, and management of pinned CUDA memory to improve\\nthroughput.\\nMost importantly, users are free to replace any component of PyTorch that does not meet the needs or\\nperformance requirements of their project. They are all designed to be completely interchangeable,\\nand PyTorch takes great care not to impose any particular solution.\\n4.3\\nAutomatic differentiation\\nSince gradient based optimization is vital to deep learning, PyTorch must be able to automatically\\ncompute gradients of models speciﬁed by our users, and those can be arbitrary Python programs.\\nHowever, Python is a dynamic programming language that allows changing most behaviors at\\nruntime, making ahead of time source-to-source differentiation cumbersome. Instead, PyTorch uses\\nthe operator overloading approach, which builds up a representation of the computed function every\\ntime it is executed. In its current implementation [30], PyTorch performs reverse-mode automatic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 4}, page_content='the operator overloading approach, which builds up a representation of the computed function every\\ntime it is executed. In its current implementation [30], PyTorch performs reverse-mode automatic\\ndifferentiation, which computes the gradient of a scalar output with respect to a multivariate input.\\nDifferentiating functions with more outputs than inputs is more efﬁciently executed using forward-\\nmode automatic differentiation, but this use case is less common for machine learning applications.\\nPyTorch can be easily extended to perform forward-mode differentiation using array-level dual\\nnumbers [31, 32].\\nAnother interesting and uncommon feature of our system is that it can differentiate through code\\nemploying mutation on tensors, which is one of the basic building blocks of imperative programs.\\nTo ensure safety, we have implemented a versioning system for tensors, which lets us track their\\nmodiﬁcations and ensure that we always use the data we expect. One interesting tradeoff is that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 4}, page_content='To ensure safety, we have implemented a versioning system for tensors, which lets us track their\\nmodiﬁcations and ensure that we always use the data we expect. One interesting tradeoff is that\\nwhile we could utilize techniques like copy-on-write to support arbitrary programs, we chose to not\\ngo down this path, as performance-wise it is usually beneﬁcial for the users to rewrite their code\\nto ensure that no copies have to be performed. Hence, while most mutations are benign and can\\nbe handled automatically, the really complicated cases result in a user error, which lets them know\\nthat they likely want to restructure the program. This allows us to avoid introducing subtle and\\nhard-to-ﬁnd performance cliffs.\\n5\\nPerformance focused implementation\\nRunning deep learning algorithms efﬁciently from a Python interpreter is notoriously challenging: for\\ninstance, the global interpreter lock [33] effectively ensures that only one of any number of concurrent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 4}, page_content='instance, the global interpreter lock [33] effectively ensures that only one of any number of concurrent\\nthreads is running at any given time. Deep learning frameworks based on the construction of a static\\ndata-ﬂow graph sidestep this problem by deferring the evaluation of the computation to a custom\\ninterpreter.\\nPyTorch solved the problem differently, by carefully optimizing every aspect of its execution while\\nsimultaneously empowering its users to easily leverage additional optimization strategies.\\n5.1\\nAn efﬁcient C++ core\\nDespite being closely integrated in the Python ecosystem, most of PyTorch is written in C++ to\\nachieve high performance. This core libtorch library implements the tensor data structure, the GPU\\nand CPU operators, and basic parallel primitives. It also provides the automatic differentiation system,\\nincluding the gradient formulas for most built-in functions. This ensures that the computation of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 4}, page_content='including the gradient formulas for most built-in functions. This ensures that the computation of the\\nderivatives of functions composed of core PyTorch operators is executed entirely in a multithreaded\\nevaluator which does not require holding the Python global interpreter lock [33]. Python bindings\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 5}, page_content='are generated using YAML meta-data ﬁles. An interesting side-effect of this approach is that it\\nallowed our community to quickly create bindings to multiple other languages resulting in projects\\nlike NimTorch [34], hasktorch [35] and others.\\nThis design also allowed us to create ﬁrst-class C++ bindings and modeling libraries that can be\\nused in places where Python is inconvenient, such as the game engine for Starcraft [36] or on mobile\\nplatforms. It is even possible to take the Python code describing a PyTorch model and run it without\\nPython using the TorchScript engine [37].\\n5.2\\nSeparate control and data ﬂow\\nPyTorch maintains a strict separation between its control (i.e. program branches, loops) and data ﬂow\\n(i.e. tensors and the operations performed on them). The resolution of the control ﬂow is handled\\nby Python and optimized C++ code executed on the host CPU, and result in a linear sequence of\\noperator invocations on the device. Operators can be run either on CPU or on GPU.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 5}, page_content='by Python and optimized C++ code executed on the host CPU, and result in a linear sequence of\\noperator invocations on the device. Operators can be run either on CPU or on GPU.\\nPyTorch is designed to execute operators asynchronously on GPU by leveraging the CUDA stream\\nmechanism [38] to queue CUDA kernel invocations to the GPUs hardware FIFO. This allows the\\nsystem to overlap the execution of Python code on CPU with tensor operators on GPU. Because\\nthe tensor operations usually take a signiﬁcant amount of time, this lets us saturate the GPU and\\nreach peak performance even in an interpreted language with fairly high overhead like Python. Note\\nthat this mechanism is nearly invisible to the user. Unless they implement their own multi-stream\\nprimitives all of the CPU-GPU synchronization is handled by the library.\\nPyTorch could leverage a similar mechanism to also execute operators asynchronously on the CPU.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 5}, page_content='primitives all of the CPU-GPU synchronization is handled by the library.\\nPyTorch could leverage a similar mechanism to also execute operators asynchronously on the CPU.\\nHowever the costs of cross-thread communication and synchronization would negate the performance\\nbeneﬁt of such an optimization.\\n5.3\\nCustom caching tensor allocator\\nAlmost every operator must dynamically allocate an output tensor to hold the result of its execution.\\nIt is therefore critical to optimize the speed of the dynamic memory allocators. PyTorch can rely on\\noptimized libraries [39–41] to handle this task on CPU. However, on GPU the cudaFree routine may\\nblock its caller until all previously queued work on all GPUs completes. To avoid this bottleneck,\\nPyTorch implements a custom allocator which incrementally builds up a cache of CUDA memory\\nand reassigns it to later allocations without further use of CUDA APIs. The incremental allocation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 5}, page_content='PyTorch implements a custom allocator which incrementally builds up a cache of CUDA memory\\nand reassigns it to later allocations without further use of CUDA APIs. The incremental allocation\\nis also crucial for better interoperability, because taking up all GPU memory ahead of time would\\nprevent the user from utilizing other GPU-enabled Python packages.\\nTo further improve its effectiveness, this allocator was tuned for the speciﬁc memory usage patterns of\\ndeep learning. For example, it rounds up allocations to multiples of 512 bytes to avoid fragmentation\\nissues. Moreover, it maintains a distinct pool of memory for every CUDA stream (work queue).\\nThe one-pool-per-stream design assumption simpliﬁes the implementation and improves the perfor-\\nmance of the allocator: because the CPU runs ahead of the GPU, memory is freed on the CPU before\\nits last use on the GPU ﬁnishes. Since streams serialize execution, if the free precedes the reallocation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 5}, page_content='its last use on the GPU ﬁnishes. Since streams serialize execution, if the free precedes the reallocation\\non the CPU, the same order will occur on the GPU. So the allocator can reallocate memory freed on\\nthe CPU immediately as long as the new allocation is used on the same stream as the freed region.\\nHowever, if an allocation was last used on one stream and then allocated on another, additional\\nsynchronization is needed.\\nThe one-pool-per-stream design seems limiting since the allocations end up fragmented per stream, but\\nin practice PyTorch almost never uses multiple streams. It is notoriously hard to write CUDA kernels\\nin a way that would let them cooperatively share the GPU because exact scheduling is hardware\\ncontrolled. In practice, kernel writers usually resort to monolithic kernels that combine multiple tasks.\\nData loading and distributed computing utilities are exceptions to the one stream design, and they'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 5}, page_content='Data loading and distributed computing utilities are exceptions to the one stream design, and they\\ncarefully insert additional synchronization to avoid bad interactions with the allocator.\\nWhile this design is susceptible to certain corner cases, it almost never exhibits unwanted behaviors\\nin practical code. Most of our users are not aware of its existence.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 6}, page_content='5.4\\nMultiprocessing\\nDue to the global interpreter lock (GIL) Python’s default implementation does not allow concurrent\\nthreads to execute in parallel. To alleviate this problem, the Python community has established a\\nstandard multiprocessing module, containing a number of utilities that allow users to easily spawn\\nchild processes and implement basic inter-process communication primitives.\\nHowever, the implementation of the primitives uses the same form of serialization used for on-disk\\npersistence, which is inefﬁcient when dealing with large arrays. Hence, PyTorch extends the Python\\nmultiprocessing module into torch.multiprocessing, which is a drop-in replacement for the\\nbuilt in package and automatically moves the data of tensors sent to other processes to shared memory\\ninstead of sending it over the communication channel.\\nThis design greatly improves performance and makes the process isolation weaker, resulting in a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 6}, page_content='instead of sending it over the communication channel.\\nThis design greatly improves performance and makes the process isolation weaker, resulting in a\\nprogramming model which more closely resembles regular threaded programs. Users can easily\\nimplement heavily parallel programs that operate on independent GPUs but later synchronize gradients\\nusing all-reduce style primitives.\\nAnother unique feature of this system is that it transparently handles sharing of CUDA tensors,\\nmaking it easy to implement techniques like Hogwild [42].\\n5.5\\nReference counting\\nUsers often design their models to utilize all memory available during training, and increasing batch\\nsizes is a common technique of speeding up the process. Therefore, to deliver great performance,\\nPyTorch has to treat memory as a scarce resource that it needs to manage carefully.\\nLibraries with eager semantics have to manage tensor memory without knowing how it will be used'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 6}, page_content='PyTorch has to treat memory as a scarce resource that it needs to manage carefully.\\nLibraries with eager semantics have to manage tensor memory without knowing how it will be used\\nin the future. Garbage collection is the typical way to handle this automatically because it has good\\namortized performance. In this approach, the runtime periodically investigates the state of the system,\\nenumerates used objects and frees everything else. However, by deferring the deallocation, it causes\\nthe program to use more memory overall [43]. Given the scarcity of GPU memory, these overheads\\nare unacceptable. In fact, Torch7 utilized the garbage collector built into Lua, and a common anti-\\npattern among the users was to sprinkle the program with explicit triggers to the garbage collector,\\nhoping that the memory errors go away.\\nPyTorch takes a different approach: it relies on a reference counting scheme to track the number of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 6}, page_content='hoping that the memory errors go away.\\nPyTorch takes a different approach: it relies on a reference counting scheme to track the number of\\nuses of each tensor, and frees the underlying memory immediately once this count reaches zero. Note\\nthat PyTorch tracks both references internal to the libtorch library and external references made by\\nusers in their Python code by integrating with Python’s own reference counting mechanism. This\\nensures that memory is released exactly when tensors become unneeded.\\nOne notable caveat is that we can only guarantee the desired performance characteristics in implemen-\\ntations of languages that either already utilize reference counting (CPython, Swift, but not PyPy or\\nmany scripting languages such as Lua), and those that allow for user-deﬁned behavior for assignment,\\ncopies, and moves (e.g. C++, Rust). Bindings to implementations that do not satisfy those criteria\\nwill have to implement their own specialized memory management on top of PyTorch.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 6}, page_content='copies, and moves (e.g. C++, Rust). Bindings to implementations that do not satisfy those criteria\\nwill have to implement their own specialized memory management on top of PyTorch.\\n6\\nEvaluation\\nIn this section we compare the performance of PyTorch with several other commonly-used deep\\nlearning libraries, and ﬁnd that it achieves competitive performance across a range of tasks. All\\nexperiments were performed on a workstation with two Intel Xeon E5-2698 v4 CPUs and one\\nNVIDIA Quadro GP100 GPU.\\n6.1\\nAsynchronous dataﬂow\\nWe start by quantifying the ability of PyTorch to asynchronously execute dataﬂow on GPU. We use\\nthe built-in proﬁler [44] to instrument various benchmarks and record a timeline of the execution of a\\nsingle training step.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 7}, page_content='Figure 1 shows a representative timeline of execution for the ﬁrst few operations of a ResNet-50\\nmodel. The host CPU which queues the work quickly outpaces the execution of the operators on\\nthe GPU. This allows PyTorch to achieve almost perfect device utilization. In this example, GPU\\nexecution takes around three times longer than CPU scheduling. The exact ratio depends on the\\nrelative performance of the host CPU and the GPU, as well as the number of elements in each tensor\\nand the average arithmetic complexity of the ﬂoating point computations to be performed on the\\nGPU.\\nFigure 1: A trace of the ﬁrst few operators of Resnet-50. The top row depicts the execution of the control\\nﬂow running on the host CPU. The gray areas are Python code executed by its interpreter. The colored areas\\ncorrespond to the work done on the host CPU to queue various operators (convolution, batch normalization, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 7}, page_content='correspond to the work done on the host CPU to queue various operators (convolution, batch normalization, and\\nso on). The bottom row shows the corresponding execution of those operators on the GPU. The arrows pair the\\ntwo events in time.\\n6.2\\nMemory management\\nWe used the NVIDIA proﬁler to trace the execution of the CUDA runtime as well as the execution\\nof the CUDA kernels launched during one training iteration of the ResNet-50 model. As shown in\\nFigure 2, the behavior of the ﬁrst iteration differs signiﬁcantly from that of subsequent ones. At\\nﬁrst, calls to the CUDA memory management functions (cudaMalloc and cudaFree) slow down the\\nexecution quite dramatically by blocking the CPU thread for long periods of time, hence lowering\\nthe utilization of the GPU. This effect disappears in subsequent iterations as the PyTorch caching\\nmemory allocator starts reusing previously allocated regions.\\nFigure 2: Annotated traces of the execution of ResNet-50 on GPU.\\n6.3\\nBenchmarks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 7}, page_content='memory allocator starts reusing previously allocated regions.\\nFigure 2: Annotated traces of the execution of ResNet-50 on GPU.\\n6.3\\nBenchmarks\\nFinally, we can get an overall sense of single-machine eager mode performance of PyTorch by com-\\nparing it to three popular graph-based deep learning frameworks (CNTK, MXNet and TensorFlow), a\\ndeﬁne-by-run framework (Chainer), and production oriented platform (PaddlePaddle). The Appendix\\ndetails all the steps needed to reproduce our setup.\\nOur results are summarized in Table 1. On all the benchmarks, the performance of PyTorch is within\\n17% of that of of the fastest framework. We attribute this result to the fact that these tools ofﬂoad\\nmost of the computation to the same version of the cuDNN and cuBLAS libraries.\\nFramework\\nThroughput (higher is better)\\nAlexNet\\nVGG-19\\nResNet-50\\nMobileNet\\nGNMTv2\\nNCF\\nChainer\\n778 ± 15\\nN/A\\n219 ± 1\\nN/A\\nN/A\\nN/A\\nCNTK\\n845 ± 8\\n84 ± 3\\n210 ± 1\\nN/A\\nN/A\\nN/A\\nMXNet\\n1554 ± 22\\n113 ± 1\\n218 ± 2\\n444 ± 2\\nN/A\\nN/A\\nPaddlePaddle'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 7}, page_content='AlexNet\\nVGG-19\\nResNet-50\\nMobileNet\\nGNMTv2\\nNCF\\nChainer\\n778 ± 15\\nN/A\\n219 ± 1\\nN/A\\nN/A\\nN/A\\nCNTK\\n845 ± 8\\n84 ± 3\\n210 ± 1\\nN/A\\nN/A\\nN/A\\nMXNet\\n1554 ± 22\\n113 ± 1\\n218 ± 2\\n444 ± 2\\nN/A\\nN/A\\nPaddlePaddle\\n933 ± 123\\n112 ± 2\\n192 ± 4\\n557 ± 24\\nN/A\\nN/A\\nTensorFlow\\n1422 ± 27\\n66 ± 2\\n200 ± 1\\n216 ± 15\\n9631 ± 1.3%\\n4.8e6 ± 2.9%\\nPyTorch\\n1547 ± 316\\n119 ± 1\\n212 ± 2\\n463 ± 17\\n15512 ± 4.8%\\n5.4e6 ± 3.4%\\nTable 1: Training speed for 6 models using 32bit ﬂoats. Throughput is measured in images per second for the\\nAlexNet, VGG-19, ResNet-50, and MobileNet models, in tokens per second for the GNMTv2 model, and in\\nsamples per second for the NCF model. The fastest speed for each model is shown in bold.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 8}, page_content='6.4\\nAdoption\\nThe validity of design decisions and their impact on ease-of-use is hard to measure. As a proxy,\\nwe tried to quantify how well the machine learning community received PyTorch by counting how\\noften various machine learning tools (including Caffe, Chainer, CNTK, Keras, MXNet, PyTorch,\\nTensorFlow, and Theano) are mentioned on arXiv e-Prints since the initial release of PyTorch in\\nJanuary 2017. In Figure 3 we report the monthly number of mentions of the word \"PyTorch\" as a\\npercentage of all mentions among these deep learning frameworks. We counted tools mentioned\\nmultiple times in a given paper only once, and made the search case insensitive to account for various\\nspellings.\\nFigure 3: Among arXiv papers each month that mention common deep learning frameworks, percentage of\\nthem that mention PyTorch.\\n7\\nConclusion and future work\\nPyTorch has become a popular tool in the deep learning research community by combining a focus'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 8}, page_content='them that mention PyTorch.\\n7\\nConclusion and future work\\nPyTorch has become a popular tool in the deep learning research community by combining a focus\\non usability with careful performance considerations. In addition to continuing to support the latest\\ntrends and advances in deep learning, in the future we plan to continue to improve the speed and\\nscalability of PyTorch. Most notably, we are working on the PyTorch JIT: a suite of tools that\\nallow PyTorch programs to be executed outside of the Python interpreter where they can be further\\noptimized. We also intend to improve support for distributed computation by providing efﬁcient\\nprimitives for data parallelism as well as a Pythonic library for model parallelism based around\\nremote procedure calls.\\n8\\nAcknowledgements\\nWe are grateful to the PyTorch community for their feedback and contributions that greatly inﬂuenced\\nthe design and implementation of PyTorch. We thank all the PyTorch core team members, contributors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 8}, page_content='We are grateful to the PyTorch community for their feedback and contributions that greatly inﬂuenced\\nthe design and implementation of PyTorch. We thank all the PyTorch core team members, contributors\\nand package maintainers including Ailing Zhang, Alex Suhan, Alfredo Mendoza, Alican Bozkurt,\\nAndrew Tulloch, Ansha Yu, Anthony Shoumikhin, Bram Wasti, Brian Vaughan, Christian Puhrsch,\\nDavid Reiss, David Riazati, Davide Libenzi, Dmytro Dzhulgakov, Dwaraj Rajagopal, Edward Yang,\\nElias Ellison, Fritz Obermeyer, George Zhang, Hao Lu, Hong Xu, Hung Duong, Igor Fedan, Ilia\\nCherniavskii, Iurii Zdebskyi, Ivan Kobzarev, James Reed, Jeff Smith, Jerry Chen, Jerry Zhang, Jiakai\\nLiu, Johannes M. Dieterich, Karl Ostmo, Lin Qiao, Martin Yuan, Michael Suo, Mike Ruberry, Mikhail\\nZolothukhin, Mingzhe Li, Neeraj Pradhan, Nick Korovaiko, Owen Anderson, Pavel Belevich, Peter\\nJohnson, Pritam Damania, Raghuraman Krishnamoorthi, Richard Zou, Roy Li, Rui Zhu, Sebastian'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 8}, page_content='Zolothukhin, Mingzhe Li, Neeraj Pradhan, Nick Korovaiko, Owen Anderson, Pavel Belevich, Peter\\nJohnson, Pritam Damania, Raghuraman Krishnamoorthi, Richard Zou, Roy Li, Rui Zhu, Sebastian\\nMessmer, Shen Li, Simon Wang, Supriya Rao, Tao Xu, Thomas Viehmann, Vincent Quenneville-\\nBelair, Vishwak Srinivasan, Vitaly Fedyunin, Wanchao Liang, Wei Yang, Will Feng, Xiaomeng Yang,\\nXiaoqiang Zheng, Xintao Chen, Yangqing Jia, Yanli Zhao, Yinghai Lu and Zafar Takhirov.\\nReferences\\n[1] Yangqing \"Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,\\nSergio Guadarrama, and Trevor\" Darrell. \"caffe: Convolutional architecture for fast feature\\nembedding\". \"arXiv preprint arXiv:1408.5093\", \"2014\".\\n[2] Frank Seide and Amit Agarwal. Cntk: Microsoft’s open-source deep-learning toolkit. In\\nProceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery\\nand Data Mining, KDD ’16, pages 2135–2135, New York, NY, USA, 2016. ACM.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 9}, page_content='[3] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\\nGreg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\\nAndrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,\\nManjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek\\nMurray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\\nTalwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete\\nWarden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-\\nscale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.\\n[4] Theano Development Team. Theano: A Python framework for fast computation of mathematical\\nexpressions. arXiv e-prints, abs/1605.02688, May 2016.\\n[5] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 9}, page_content='expressions. arXiv e-prints, abs/1605.02688, May 2016.\\n[5] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open\\nsource framework for deep learning. In Proceedings of Workshop on Machine Learning Systems\\n(LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing\\nSystems (NIPS), 2015.\\n[6] Ronan Collobert, Samy Bengio, and Johnny Mariéthoz. Torch: a modular machine learning\\nsoftware library. Technical report, Idiap, 2002.\\n[7] G. Neubig, C. Dyer, Y. Goldberg, A. Matthews, W. Ammar, A. Anastasopoulos, M. Balles-\\nteros, D. Chiang, D. Clothiaux, T. Cohn, K. Duh, M. Faruqui, C. Gan, D. Garrette, Y. Ji,\\nL. Kong, A. Kuncoro, G. Kumar, C. Malaviya, P. Michel, Y. Oda, M. Richardson, N. Saphra,\\nS. Swayamdipta, and P. Yin. DyNet: The Dynamic Neural Network Toolkit. ArXiv e-prints,\\nJanuary 2017.\\n[8] Philip S. Abrams. An APL Machine. PhD thesis, Stanford University, 1970.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 9}, page_content='S. Swayamdipta, and P. Yin. DyNet: The Dynamic Neural Network Toolkit. ArXiv e-prints,\\nJanuary 2017.\\n[8] Philip S. Abrams. An APL Machine. PhD thesis, Stanford University, 1970.\\n[9] The MathWorks, Inc., Natick, Massachusetts, United States. MATLAB and Statistics Toolbox.\\n[10] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for\\nStatistical Computing, Vienna, Austria.\\n[11] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to\\nnumerical computing. SIAM review, 59(1):65–98, 2017.\\n[12] Travis Oliphant.\\nNumPy:\\nA guide to NumPy.\\nUSA: Trelgol Publishing, 2006.\\nhttp://www.numpy.org/.\\n[13] Gaël Guennebaud, Benoît Jacob, et al. Eigen v3. http://eigen.tuxfamily.org, 2010.\\n[14] Y LeCun and L Bottou.\\nLush reference manual.\\nTechnical report, code available at\\nhttp://lush.sourceforge.net, 2002.\\n[15] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 9}, page_content='Lush reference manual.\\nTechnical report, code available at\\nhttp://lush.sourceforge.net, 2002.\\n[15] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark\\nSiskind. Automatic differentiation in machine learning: A survey. J. Mach. Learn. Res.,\\n18(1):5595–5637, January 2017.\\n[16] Dougal Maclaurin. Modeling, Inference and Optimization with Composable Differentiable\\nProcedures. PhD thesis, Harvard University, April 2016.\\n[17] Matthew Johnson et. al. Jax. https://github.com/google/jax, 2018.\\n[18] Mike Innes et. al. Flux.jl. https://github.com/FluxML/Flux.jl, 2018.\\n[19] Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientiﬁc tools for\\nPython, 2001–. http://www.scipy.org/.\\n[20] Wes McKinney. Data structures for statistical computing in python. In Proceedings of the 9th\\nPython in Science Conference, 51-56, 2010.\\n[21] Pierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Eblearn: Open-source energy-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 9}, page_content='Python in Science Conference, 51-56, 2010.\\n[21] Pierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Eblearn: Open-source energy-based\\nlearning in c++. In 2009 21st IEEE International Conference on Tools with Artiﬁcial Intelligence,\\npages 693–697. IEEE, 2009.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 10}, page_content='[22] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan D. Cohen, John Tran, Bryan\\nCatanzaro, and Evan Shelhamer.\\ncudnn: Efﬁcient primitives for deep learning.\\nCoRR,\\nabs/1410.0759, 2014.\\n[23] Andrew Lavin. maxdnn: An efﬁcient convolution kernel for deep learning with maxwell gpus,\\nJanuary 2015.\\n[24] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. 2016 IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), pages 4013–4021, 2016.\\n[25] Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet. Torch7: A matlab-like environment\\nfor machine learning. In NIPS 2011, 2011.\\n[26] Richard Gabriel. The rise of worse is better. http://dreamsongs.com/RiseOfWorseIsBetter.html.\\n[27] Yann\\nLeCun\\nand\\nCorinna\\nCortes.\\nMNIST\\nhandwritten\\ndigit\\ndatabase.\\nhttp://yann.lecun.com/exdb/mnist/.\\n[28] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 10}, page_content='[27] Yann\\nLeCun\\nand\\nCorinna\\nCortes.\\nMNIST\\nhandwritten\\ndigit\\ndatabase.\\nhttp://yann.lecun.com/exdb/mnist/.\\n[28] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\\nMichelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, John\\nQuan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David\\nSilver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence,\\nAnders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement\\nlearning. CoRR, abs/1708.04782, 2017.\\n[29] DMLC. Dlpack: Open in memory tensor structure. https://github.com/dmlc/dlpack.\\n[30] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\\npytorch. In NIPS Workshop, 2017.\\n[31] Dan Piponi. Automatic differentiation, C++ templates, and photogrammetry. J. Graphics, GPU,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 10}, page_content='pytorch. In NIPS Workshop, 2017.\\n[31] Dan Piponi. Automatic differentiation, C++ templates, and photogrammetry. J. Graphics, GPU,\\n& Game Tools, 9(4):41–55, 2004.\\n[32] Holger Leuck and Hans-Hellmut Nagel. Automatic differentiation facilitates of-integration\\ninto steering-angle-based road vehicle tracking. In 1999 Conference on Computer Vision and\\nPattern Recognition (CVPR ’99), 23-25 June 1999, Ft. Collins, CO, USA, pages 2360–2365,\\n1999.\\n[33] The\\nPython\\nteam.\\nThe\\ncpython\\nglobal\\ninterpreter\\nlock.\\nhttps://wiki.python.org/moin/GlobalInterpreterLock.\\n[34] Giovanni Petrantoni and Jörg Wollenschläger.\\nNimtorch.\\nhttps://github.com/fragcolor-\\nxyz/nimtorch.\\n[35] Austin\\nHuang,\\nJunji\\nHashimoto,\\nand\\nSam\\nStites.\\nHasktorch.\\nhttps://github.com/hasktorch/hasktorch.\\n[36] G. Synnaeve, Z. Lin, J. Gehring, D. Gant, V. Mella, V. Khalidov, N. Carion, and N. Usunier.\\nForward modeling for partial observation strategy games - a starcraft defogger. In Advances in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 10}, page_content='[36] G. Synnaeve, Z. Lin, J. Gehring, D. Gant, V. Mella, V. Khalidov, N. Carion, and N. Usunier.\\nForward modeling for partial observation strategy games - a starcraft defogger. In Advances in\\nNeural Information Processing Systems, pages 10761–10771, 2018.\\n[37] The PyTorch team. Torch Script. https://pytorch.org/docs/stable/jit.html.\\n[38] Justin Luitjens. Cuda streams. GPU technology conference, 2014.\\n[39] Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson. Hoard:\\nA scalable memory allocator for multithreaded applications. In Proceedings of the Ninth\\nInternational Conference on Architectural Support for Programming Languages and Operating\\nSystems, ASPLOS IX, pages 117–128, New York, NY, USA, 2000. ACM.\\n[40] J. Evans. A scalable concurrent malloc(3) implementation for freebsd. In In BSDCan — The\\nTechnical BSD Conference, May 2006.\\n[41] S. Ghemawat and P. Menage. Tcmalloc: Thread-caching malloc.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-12-05T01:43:03+00:00', 'source': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Pytorch_Tutorial.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-12-05T01:43:03+00:00', 'trapped': '', 'modDate': 'D:20191205014303Z', 'creationDate': 'D:20191205014303Z', 'page': 11}, page_content='[42] Benjamin Recht, Christopher Ré, Stephen J. Wright, and Feng Niu. Hogwild: A lock-free\\napproach to parallelizing stochastic gradient descent. In Advances in Neural Information\\nProcessing Systems 24: 25th Annual Conference on Neural Information Processing Systems\\n2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain., pages 693–701,\\n2011.\\n[43] Matthew Hertz and Emery D. Berger. Quantifying the performance of garbage collection vs.\\nexplicit memory management. In Proceedings of the 20th Annual ACM SIGPLAN Conference\\non Object-oriented Programming, Systems, Languages, and Applications, OOPSLA ’05, pages\\n313–326, New York, NY, USA, 2005. ACM.\\n[44] The PyTorch team. Pytorch Autograd Proﬁler. https://pytorch.org/docs/1.0.1/autograd.html#proﬁler.\\n12'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 0}, page_content=\"Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n132 \\nA Research of Challenges and Solutions in Retrieval \\nAugmented Generation (RAG) Systems \\nJiafeng Gu * \\nSchool of CS and Math, University of Puget Sound, WA, United States \\n* Corresponding Author Email: jgu@pugetsound.edu \\nAbstract. Retrieval-Augmented Generation (RAG) systems represent a significant innovation in the \\nfield of Natural Language Processing (NLP), ingeniously integrating Large Language Models (LLMs) \\nwith dynamic external knowledge retrieval. This amalgamation not only enhances the models' \\nresponsiveness to real-world knowledge but also addresses the limitations of conventional \\ngenerative models in terms of knowledge update velocity and factual accuracy. This review \\nexamines the challenges faced by RAG systems and their solutions. It delves into the central \\narchitecture of RAG systems, encompassing retrieval components, generative components, and\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 0}, page_content='examines the challenges faced by RAG systems and their solutions. It delves into the central \\narchitecture of RAG systems, encompassing retrieval components, generative components, and \\nknowledge bases, with a particular focus on recent advancements that have expanded the \\nboundaries of performance and functionality. The study critically analyzes major challenges such as \\nretrieval efficiency and dynamic knowledge management. This paper evaluates various advanced \\nsolutions proposed in recent literature, comparing their efficacy and discussing the trade-offs \\ninvolved. Ultimately, this paper aims to provide researchers, developers, and users of RAG systems \\nwith a comprehensive perspective, fostering ongoing innovation and the expansion of applications \\nin this domain. \\nKeywords: Retrieval augmented generation, natural language processing, information retrieval, \\nknowledge base. \\n1. Introduction'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 0}, page_content=\"in this domain. \\nKeywords: Retrieval augmented generation, natural language processing, information retrieval, \\nknowledge base. \\n1. Introduction \\nRetrieval-Augmented Generation (RAG) systems have emerged as a groundbreaking approach in \\nnatural language processing, tackling the fundamental limitations of traditional Large Language \\nModels (LLMs). By leveraging the power of LLMs with dynamic access to external knowledge, RAG \\nsystems represent a significant advancement in AI and Natural Language Processing (NLP) [1]. This \\ninnovative approach allows for the generation of more accurate, relevant, and up-to-date responses \\nacross a wide range of applications. The significance of RAG research lies in its potential to transform \\nAI applications fundamentally. These systems enhance text accuracy and relevance, improve AI's \\ncapability to handle complex, knowledge-intensive tasks, and enable continuous knowledge updating\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 0}, page_content=\"AI applications fundamentally. These systems enhance text accuracy and relevance, improve AI's \\ncapability to handle complex, knowledge-intensive tasks, and enable continuous knowledge updating \\nwithout the need for constant model retraining. This dynamic integration of retrieval and generation \\nmechanisms addresses the longstanding challenge of knowledge staleness in pre-trained language \\nmodels, opening new avenues for more adaptive and context-aware AI systems. \\nCurrent challenges in RAG systems span various aspects of their architecture and functionality. \\nWhile recent advancements have made significant strides, they often come with their own limitations. \\nFor instance, the Retrieval-Enhanced Transformer (RETRO) has shown impressive scalability, \\ncapable of retrieving from databases with trillions of tokens and demonstrating competitive \\nperformance with models 25 times its size [2]. However, RETRO faces challenges in computational\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 0}, page_content='capable of retrieving from databases with trillions of tokens and demonstrating competitive \\nperformance with models 25 times its size [2]. However, RETRO faces challenges in computational \\nefficiency and the possibility of mistakes spreading in its iterative retrieval process. Another cutting-\\nedge approach, the atlas model, employs few-shot learning with retrieval augmented language models, \\ngetting great performance on various knowledge-intensive tasks [3]. Despite its impressive \\nperformance, Atlas still faces challenges in efficiently updating its knowledge base and may struggle \\nwith queries that require real-time information retrieval and integration. \\nThis paper aims to provide a comprehensive review of these challenges and the proposed cutting-\\nedge solutions. It analyzes the architecture and components of RAG systems in depth, identifying key \\nchallenges and their impact on system performance. By critically reviewing and comparing existing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 0}, page_content='challenges and their impact on system performance. By critically reviewing and comparing existing \\nsolutions, this paper highlights both their achievements and limitations. Furthermore, this work'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 1}, page_content='Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n133 \\nexplores promising future research directions that could address current limitations but may also \\nintroduce new challenges in data processing and model design. \\n2. Architecture of RAG System \\nRAG systems consist of three primary components: the retrieval component, the generation \\ncomponent, and the knowledge base. Each plays a crucial role in producing accurate, relevant, and \\nup-to-date responses (Fig.1). The system retrieves relevant content based on user queries using this \\nembedded knowledge base. The retrieved chunks are then combined with the original query to form \\na prompt, which is processed by a LLM to generate the final response.  \\n \\nFig 1. The workflow of a RAG system (Photo/Picture credit: Original).  \\n2.1. Retrieval Component \\nThe retrieval component serves as the critical bridge between user input and the vast repository of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 1}, page_content='Fig 1. The workflow of a RAG system (Photo/Picture credit: Original).  \\n2.1. Retrieval Component \\nThe retrieval component serves as the critical bridge between user input and the vast repository of \\ninformation stored in the knowledge base. Its primary function is to identify and extract relevant \\ninformation based on the input query. This process involves several complex steps, including query \\nunderstanding, eﬀicient searching, and relevance ranking. \\nRecent advancements in dense retrieval methods, such as those proposed by Karpukhin et al., have \\nsignificantly improved the effectiveness of this component [4]. These methods leverage dense vector \\nrepresentations of both queries and documents, enabling more nuanced semantic matching compared \\nto traditional lexical retrieval approaches. The main advantage of this component lies in its ability to \\naccess and utilize vast amounts of external knowledge, potentially overcoming the limitations of static'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 1}, page_content='access and utilize vast amounts of external knowledge, potentially overcoming the limitations of static \\nknowledge inherent in traditional language models. Hybrid retrieval approaches have also emerged \\nas a promising direction. For instance, GAO proposed a method combining sparse and dense retrieval \\ntechniques [5]. This approach aims to leverage the strengths of both lexical and semantic matching, \\npotentially offering more robust performance across diverse query types. \\nHowever, challenges persist in achieving optimal performance, particularly in terms of query \\ninterpretation and balancing semantic relevance with diversity in the retrieved information. The \\nretrieval component must not only consider the semantic similarity between the query and potential \\nmatches but also ensure a diverse set of relevant information to provide comprehensive context for \\nthe generation task.  \\n2.2. Generation Component'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 1}, page_content='matches but also ensure a diverse set of relevant information to provide comprehensive context for \\nthe generation task.  \\n2.2. Generation Component \\nThe generation component, typically based on a large language model, is responsible for producing \\nthe final output in RAG systems. This crucial element integrates the retrieved information with the \\noriginal input to generate coherent, contextually appropriate, and informative responses. The'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 2}, page_content=\"Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n134 \\ngeneration process involves several key steps: context integration, text generation, and output \\nrefinement. \\nRecent work has shown promising results in improving the generation component's ability to \\neffectively utilize retrieved information. A significant breakthrough in this area is the Fusion-in-\\nDecoder model proposed by lzacard [6]. This model processes all retrieved passages jointly in the \\ndecoder, allowing for more effective integration of information from multiple sources. This approach \\ndemonstrates the potential for RAG systems to adapt quickly to new tasks and domains with minimal \\nfine-tuning. Another notable advancement is the development of iterative retrieval-generation models, \\nas demonstrated by Shuster [7]. These models involve multiple rounds of retrieval and generation, \\nenabling the system to handle complex queries that may require multi-step reasoning or information\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 2}, page_content='as demonstrated by Shuster [7]. These models involve multiple rounds of retrieval and generation, \\nenabling the system to handle complex queries that may require multi-step reasoning or information \\ngathering. Researchers have also explored integrating external knowledge graphs and structured data \\nwithin the generation process. Xu Yichong proposed an approach that leverages both retrieved textual \\ninformation and structured knowledge, potentially improving the factual accuracy and logical \\ncoherence of generated outputs [8]. \\nThe strength of this component lies in its ability to generate fluent, coherent, and contextually \\nrelevant responses. By leveraging the power of large language models and augmenting them with \\nretrieved information, RAG systems can produce outputs that are both linguistically sophisticated and \\nfactually grounded. \\nHowever, significant challenges remain. Ensuring factual consistency between the generated'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 2}, page_content='factually grounded. \\nHowever, significant challenges remain. Ensuring factual consistency between the generated \\ncontent and the retrieved information is a critical issue. The model must accurately incorporate the \\nretrieved facts while maintaining the overall coherence and fluency of the generated text. Additionally, \\nmaintaining consistency and coherence across longer outputs poses another significant challenge, \\nrequiring sophisticated mechanisms for long-range dependency modeling and content planning. \\n2.3. Knowledge Base \\nThe knowledge base serves as the external memory of the RAG system. Recent research has \\nexplored various approaches to knowledge base design, including the integration of diverse data \\nformats. \\nOne significant innovation is the creation of dynamic knowledge bases that can be efficiently \\nupdated. The Generative Pseudo-Labeling (GPL) method proposed by Wang Kexin, allows for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 2}, page_content='formats. \\nOne significant innovation is the creation of dynamic knowledge bases that can be efficiently \\nupdated. The Generative Pseudo-Labeling (GPL) method proposed by Wang Kexin, allows for \\ncontinuous learning and updating of the knowledge base [9]. This approach enables RAG systems to \\nincorporate new information without the need for full retraining, which is particularly crucial in \\ndomains with rapidly evolving knowledge. \\nResearchers have also explored multi-modal knowledge bases. For instance, Gao introduced a \\nmulti-modal retrieval-augmented framework that can process and integrate information from both \\ntextual and visual sources [10]. This advancement allows RAG systems to leverage a broader range \\nof information types, potentially enhancing their ability to handle complex, multi-modal queries. \\n3. Challenges in RAG Systems \\nDynamic knowledge management presents complex challenges in keeping the knowledge base up-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 2}, page_content='3. Challenges in RAG Systems \\nDynamic knowledge management presents complex challenges in keeping the knowledge base up-\\nto-date while maintaining system performance. This is particularly critical in domains with rapidly \\nevolving information, such as news, scientific research, or social media trends. \\n3.1. Scalability \\nThe fundamental challenge of scalability lies in the curse of dimensionality. As the volume of data \\nincreases, the search space grows exponentially, making it computationally intractable to perform \\nexact nearest neighbor search in high-dimensional spaces. This is particularly problematic for dense \\nvector representations used in modern retrieval systems. \\nIn high-dimensional spaces, the concept of \"nearest\" neighbor becomes less meaningful due to the \\nphenomenon known as \"distance concentration\". As dimensionality increases, the ratio of the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 3}, page_content='Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n135 \\ndistances of the nearest and farthest neighbors to a given point approaches 1, making it difficult to \\ndistinguish between close and far points [11]. This phenomenon significantly impacts the \\neffectiveness of traditional similarity search algorithms. \\nWhile approximate methods like Locality-Sensitive Hashing (LSH) or Hierarchical Navigable \\nSmall World (HNSW) graphs offer potential solutions, they introduce a complex trade-off between \\naccuracy and speed. For instance, LSH may miss some nearest neighbors, while HNSW requires \\ncareful tuning of its graph structure to balance between search speed and index build time. Optimizing \\nthese trade-offs remains a significant challenge, especially as the scale of data continues to grow. \\nRecent research has explored hybrid approaches to address these scalability issues. For example,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 3}, page_content='these trade-offs remains a significant challenge, especially as the scale of data continues to grow. \\nRecent research has explored hybrid approaches to address these scalability issues. For example, \\nthe ScaNN method combines quantization for fast in-memory search with anisotropic vector \\nquantization for reduced search space [12]. However, such methods still struggle with dynamic \\nupdates to the index, which is crucial for real-time RAG systems. The challenge of developing \\nscalable methods that can adapt to varying conditions while maintaining retrieval quality remains an \\nopen problem in the field. \\n3.2. Query Reformulation \\nQuery reformulation in RAG systems faces significant challenges stemming from the semantic \\ngap between user queries and knowledge base content. This process involves complex natural \\nlanguage understanding and generation tasks, requiring sophisticate models to capture nuanced \\nsemantic relationships.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 3}, page_content='language understanding and generation tasks, requiring sophisticate models to capture nuanced \\nsemantic relationships. \\nA key challenge is handling biased or loaded queries while maintaining objectivity. For instance, \\na query like \"Why are vaccines harmful?\" contains a biased premise that the system must recognize \\nand neutralize to ensure balanced information retrieval. Developing methods to detect and mitigate \\nsuch biases without completely disregarding user intent remains an open problem. Another significant \\nchallenge lies in adapting queries to specific domains or temporal contexts. User queries often contain \\ndomain-specific jargon or references to current events that require specialized knowledge to interpret \\ncorrectly. Balancing the need for domain expertise with general language understanding is a complex \\ntask that current systems struggle to achieve consistently. \\nThe temporal aspect of queries presents its own set of challenges. Queries implicitly referencing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 3}, page_content=\"task that current systems struggle to achieve consistently. \\nThe temporal aspect of queries presents its own set of challenges. Queries implicitly referencing \\ncurrent events or time-sensitive information require the reformulation process to incorporate temporal \\ncontext. Striking the right balance between current relevance and historical context is particularly \\ndifficult and requires sophisticated temporal reasoning capabilities [13]. \\n3.3. Latency \\nThe core challenge of latency in RAG systems stems from the fundamental trade-off between \\nresponse time and result quality. In interactive applications, the system must carefully balance the \\ndepth of retrieval against the user's patience threshold. This balancing act is particularly critical as the \\nretrieval depth significantly impacts both latency and result quality; deeper retrieval can provide more \\ncomprehensive results but at the cost of increased response time.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 3}, page_content='retrieval depth significantly impacts both latency and result quality; deeper retrieval can provide more \\ncomprehensive results but at the cost of increased response time. \\nOne of the primary factors contributing to latency is the complexity of multi-step retrieval \\nprocesses, often necessary for handling sophisticated queries or performing multi-hop reasoning. As \\nthe number of retrieval steps increases, managing cumulative latency becomes increasingly \\nchallenging. Each additional step not only adds to the overall response time but also introduces \\npotential points of failure or inconsistency in the retrieval process. \\nThe latency challenge is further exacerbated in scenarios requiring real-time knowledge base \\nupdates. Ensuring that newly added information is immediately available for retrieval, without \\ncompromising query response times, presents a significant technical hurdle. This is particularly'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 3}, page_content='updates. Ensuring that newly added information is immediately available for retrieval, without \\ncompromising query response times, presents a significant technical hurdle. This is particularly \\nproblematic in domains with rapidly changing information, where the relevance of retrieval results \\ncan degrade quickly if the knowledge base is not continuously updated [14].'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 4}, page_content='Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n136 \\n4. Solutions and Advancements in RAG Systems \\nRecent years have witnessed significant advancements in RAG systems, addressing key challenges \\nin retrieval efficiency, scalability, and knowledge integration. This section explores two innovative \\napproaches that represent the cutting edge of RAG technology. Self-RAG, introduced by Asai et al., \\nand represents a significant shift in RAG system design. It incorporates retrieval, generation, and \\nevaluation into a single framework, allowing the model to iteratively improve its own performance. \\nThis self-improving capability addresses challenges in query reformulation and result quality \\nassessment, potentially leading to more accurate and relevant responses over time [15]. \\nGraphRAG, developed by Microsoft, takes a different approach by leveraging graph structures for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 4}, page_content=\"assessment, potentially leading to more accurate and relevant responses over time [15]. \\nGraphRAG, developed by Microsoft, takes a different approach by leveraging graph structures for \\nknowledge representation. This framework uses large language models to extract structured data from \\nunstructured text, building labeled knowledge graphs to support various applications. GraphRAG's \\nuse of graph machine learning algorithms for semantic aggregation and hierarchical analysis enables \\nit to answer high-level abstract or summary questions, showcasing the potential of structured \\nknowledge in RAG systems [16]. \\nTable 1. Comparison of advanced RAG Solutions. \\nModel \\nKey feature \\nSelf-Improvement Reasoning Capability \\nSelf-RAG \\nIterative Refinement \\nHigh \\nAdaptive \\nGraphRAG Graph-based Representation \\nModerate \\nHigh \\n \\nAs illustrated in Table 1, these two approaches offer unique strengths and address different aspects\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 4}, page_content=\"Iterative Refinement \\nHigh \\nAdaptive \\nGraphRAG Graph-based Representation \\nModerate \\nHigh \\n \\nAs illustrated in Table 1, these two approaches offer unique strengths and address different aspects \\nof RAG system design. Self-RAG focuses on iterative self-improvement and adaptive reasoning, \\nwhile GraphRAG introduces structured knowledge representation for enhanced reasoning capabilities. \\nThese advancements collectively represent significant progress in addressing the core challenges \\nof RAG systems. However, each approach also introduces new complexities and trade-offs. For \\ninstance, while GraphRAG's structured knowledge representation offers powerful reasoning \\ncapabilities, it may face challenges in domains where information is inherently unstructured or rapidly \\nchanging. Similarly, the iterative processes in Self-RAG, while powerful, may introduce additional \\ncomputational overhead. \\n5. Future Directions for RAG Systems\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 4}, page_content='changing. Similarly, the iterative processes in Self-RAG, while powerful, may introduce additional \\ncomputational overhead. \\n5. Future Directions for RAG Systems \\nAs the explore promising future research directions for RAG systems, several key areas emerge \\nthat could significantly advance the field while remaining grounded in current technological \\ntrajectories. \\nOne promising direction is the integration of RAG systems with LLMs that have multimodal \\ncapabilities. This combination could enable RAG systems to not only retrieve and process textual \\ninformation but also understand and generate content across various modalities such as images, audio, \\nand video. For instance, a multimodal RAG system could retrieve relevant images or video clips \\nalongside textual information, providing more comprehensive and contextually rich responses. This \\nintegration could be particularly valuable in fields like medical diagnosis, where the ability to retrieve'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 4}, page_content='integration could be particularly valuable in fields like medical diagnosis, where the ability to retrieve \\nand analyze both textual reports and medical imaging data could enhance diagnostic accuracy. \\nAnother exciting avenue is the exploration of dynamic knowledge graphs within RAG systems. \\nBy continuously updating and refining a structured knowledge representation based on new \\ninformation retrieved, RAG systems could develop more nuanced and up-to-date understanding of \\ncomplex topics. This approach could involve real-time fact-checking and information validation \\nmechanisms, potentially mitigating the spread of misinformation and ensuring the reliability of \\ngenerated content. The dynamic nature of these knowledge graphs could also allow RAG systems to \\nadapt more quickly to emerging topics and changing information landscapes. \\nA third promising direction is the integration of RAG systems with robotics and embodied AI.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 4}, page_content='adapt more quickly to emerging topics and changing information landscapes. \\nA third promising direction is the integration of RAG systems with robotics and embodied AI. \\nThis combination could lead to robots that not only interact with their environment but also leverage'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 5}, page_content=\"Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n137 \\nvast knowledge bases to make informed decisions and provide rich, contextual information. For \\nexample, a RAG-enhanced robot in a manufacturing setting could access and apply complex technical \\nknowledge in real-time, improving problem-solving capabilities and adaptability. In healthcare, robot \\nassistants equipped with RAG systems could provide personalized care by combining real-time \\npatient data with comprehensive medical knowledge. Furthermore, in educational settings, RAG-\\npowered robotic tutors could offer personalized learning experiences by dynamically retrieving and \\npresenting information tailored to each student's needs and learning style. This fusion of RAG \\ntechnology with robotics could significantly enhance the physical world interaction capabilities of AI \\nsystems, opening up new possibilities in fields ranging from space exploration to disaster response,\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 5}, page_content=\"systems, opening up new possibilities in fields ranging from space exploration to disaster response, \\nwhere quick access to vast amounts of relevant information could be crucial for mission success and \\nsafety. \\n6. Conclusion \\nThis paper conducts a comprehensive analysis of recent advancements in RAG systems, with a \\nparticular emphasis on key challenges and solutions such as scalability, query reformulation, latency, \\nand RAG system architectures. This work examines innovative approaches such as RETRO's \\ncapability in handling massive datasets, Self-RAG's adaptive query reformulation, and GraphRAG's \\nstructured knowledge representation. The analysis reveals that while significant progress has been \\nmade across each domain, trade-offs remain prevalent. Enhancements in scalability often come at the \\ncost of increased computational complexity. For instance, while RETRO demonstrates its superiority\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 5}, page_content='cost of increased computational complexity. For instance, while RETRO demonstrates its superiority \\non large-scale datasets, the computational overhead and energy consumption cannot be overlooked. \\nMeanwhile, advanced query reformulation techniques like Self-RAG show promise in tackling \\ncomplex queries but may introduce additional latency, posing a challenge for real-time application \\nscenarios. Additionally, the structured knowledge representation methods employed by GraphRAG \\nenhance reasoning capabilities but still face hurdles when confronted with unstructured or rapidly \\nevolving information. \\nTo address these issues, future research can explore multiple directions. Firstly, integrating \\ncutting-edge hardware architectures with optimization algorithms to reduce computational \\ncomplexity can improve the overall performance of systems. Secondly, developing more efficient \\nquery reformulation algorithms to minimize latency ensures that systems maintain real-time'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 5}, page_content=\"complexity can improve the overall performance of systems. Secondly, developing more efficient \\nquery reformulation algorithms to minimize latency ensures that systems maintain real-time \\nresponsiveness even when dealing with intricate problems. Moreover, building dynamic models for \\nboth structured and unstructured information will be a crucial area of research, aiming to increase the \\nflexibility and adaptability of knowledge representation. \\nReference \\n[1] Lewis, Patrick, Scott Reed, Jack Urbanek, and Nando de Freitas. Retrieval-augmented generation for \\nknowledge-intensive NLP tasks. Advances in Neural Information Processing Systems 33 2020: 9459-\\n9474. \\n[2] Borgeaud, Sebastian, Arthur Mensch, Guillaume Lample, and Marc'Aurelio Ranzato. Improving language \\nmodels by retrieving from trillions of tokens. International conference on machine learning. PMLR, 2022. \\n[3] Izacard, Gautier, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 5}, page_content='[3] Izacard, Gautier, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. \\nJournal of Machine Learning Research 24.251 2023: 1-43. \\n[4] Karpukhin, Vladimir, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Alexander Kolesnikov, and \\nSebastian Ruder. Dense passage retrieval for open-domain question answering. arXiv preprint \\narXiv:2004.04906 2020. \\n[5] GAO, Luyu, Wei-Sheng Chin, Yu-Chia Chen, and Cho-Jui Hsieh. Complement lexical retrieval model \\nwith semantic residual embeddings. Advances in Information Retrieval: 43rd European Conference on IR \\nResearch, ECIR 2021, Virtual Event, March 28-April 1, 2021, Part I 43.  \\n[6] Izacard, Gautier, and Edouard Grave. Leveraging passage retrieval with generative models for open \\ndomain question answering. arXiv preprint arXiv:2007.01282 2020.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 6}, page_content='Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n138 \\n[7] Shuster, Kurt, Alexander M. Rush, and Jason Weston. Retrieval augmentation reduces hallucination in \\nconversation. arXiv preprint arXiv:2104.07567 2021. \\n[8] Xu, Yichong, Xiaojun Wan, Xiaoyan Zhu, and Xipeng Qiu. Fusing context into knowledge graph for \\ncommonsense question answering. arXiv preprint arXiv:2012.04808 2020. \\n[9] Wang, Kexin, Zhenzhong Lan, and Jianfeng Gao. GPL: Generative pseudo labeling for unsupervised \\ndomain adaptation of dense retrieval. arXiv preprint arXiv:2112.07577 2021. \\n[10] Shibata, Tetsutaro. Asymptotics of solution curves of Kirchhoff type elliptic equations with logarithmic \\nKirchhoff function. Qualitative Theory of Dynamical Systems 22.2 2023: 64. \\n[11] Peng, Dehua, Zhipeng Gui, and Huayi Wu. Interpreting the curse of dimensionality from distance \\nconcentration and manifold effect. arXiv preprint arXiv:2401.00422 2023.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 6}, page_content='[11] Peng, Dehua, Zhipeng Gui, and Huayi Wu. Interpreting the curse of dimensionality from distance \\nconcentration and manifold effect. arXiv preprint arXiv:2401.00422 2023. \\n[12] Hassantabar, Shayan, Zeyu Wang, and Niraj K. Jha. SCANN: Synthesis of compact and accurate neural \\nnetworks. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 41.9 2021: \\n3012-3025. \\n[13] Dhingra, Bhuwan, and Graham Neubig. Time-aware language models as temporal knowledge bases. \\nTransactions of the Association for Computational Linguistics 10 2022: 257-273. \\n[14] GAO, Yunfan, Zhenzhong LAN, and Jianfeng GAO. Retrieval-augmented generation for large language \\nmodels: A survey. ArXiv preprint arXiv: 2312.10997 2023. \\n[15] Asai, Akari, and Masaaki Komachi. Self-rag: Learning to retrieve, generate, and critique through self-\\nreflection. arXiv preprint arXiv:2310.11511 2023. \\n[16] Edge, Darren, and Peter J. Stuckey. From local to global: A graph rag approach to query-focused'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2024-12-25T16:59:51+08:00', 'source': '..\\\\data\\\\pdf\\\\RAG.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'DRP', 'subject': '', 'keywords': '', 'moddate': '2024-12-25T16:59:51+08:00', 'trapped': '', 'modDate': \"D:20241225165951+08'00'\", 'creationDate': \"D:20241225165951+08'00'\", 'page': 6}, page_content='reflection. arXiv preprint arXiv:2310.11511 2023. \\n[16] Edge, Darren, and Peter J. Stuckey. From local to global: A graph rag approach to query-focused \\nsummarization. arXiv preprint arXiv:2404.16130 2024.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 0}, page_content='Research on Tensor Flow with a system for large-scale machine \\nlearning \\nHafiz Nur \\nGenovasi University College, Malaysia \\nGoogle Brain \\nAbstract \\nTensorFlow is a machine learning system that operates at \\nlarge \\nscale \\nand \\nin \\nheterogeneous \\nenvironments. \\nTensorFlow \\nuses \\ndataflow \\ngraphs \\nto \\nrepresent \\ncomputation, shared state, and the operations that mutate \\nthat state. It maps the nodes of a dataflow graph across \\nmany machines in a cluster, and within a machine across \\nmultiple computational devices, including multicore CPUs, \\ngeneralpurpose GPUs, and custom designed ASICs known as \\nTensor Processing Units (TPUs). This architecture gives \\nflexibility to the application developer: whereas in previous \\n“parameter server” designs the management of shared \\nstate is built into the system, TensorFlow enables \\ndevelopers to experiment with novel optimizations and \\ntraining algorithms. TensorFlow supports a variety of \\napplications, with particularly strong support for training'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 0}, page_content='developers to experiment with novel optimizations and \\ntraining algorithms. TensorFlow supports a variety of \\napplications, with particularly strong support for training \\nand inference on deep neural networks. Several Google \\nservices use TensorFlow in production, we have released it \\nas an open-source project, and it has become widely used \\nfor machine learning research. In this paper, we describe \\nthe TensorFlow dataflow model in contrast to existing \\nsystems, and demonstrate the compelling performance \\nthat \\nTensorFlow \\nachieves \\nfor \\nseveral \\nreal-world \\napplications. \\n1 Introduction \\nIn recent years, machine learning has driven advances in \\nmany different fields [3, 5, 23, 24, 30, 27, 40, 45, 48, 50, 55, \\n68, 69, 73, 76]. We attribute this success to the invention of \\nmore sophisticated machine learning models [42, 51], the \\n                                                                \\n \\navailability of large datasets for tackling problems in these'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 0}, page_content='more sophisticated machine learning models [42, 51], the \\n                                                                \\n \\navailability of large datasets for tackling problems in these \\nfields [10, 65], and the development of software platforms \\nthat enable the easy use \\nof large amounts of computational resources for training \\nsuch models on these large datasets [14, 21]. \\nWe introduce the TensorFlow system\\n1\\n for \\nexperimenting with new models, training them on \\nlarge datasets, and moving them into production. We \\nhave based TensorFlow on years of experience with \\nour first-generation system, DistBelief [21], both \\nsimplifying and generalizing it to enable researchers to \\nexplore a wider variety of ideas with relative ease. \\nTensorFlow supports both large-scale training and \\ninference: it efficiently uses hundreds of powerful \\n(GPU-enabled) servers for fast training, and it runs \\ntrained models for inference in production on various'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 0}, page_content='inference: it efficiently uses hundreds of powerful \\n(GPU-enabled) servers for fast training, and it runs \\ntrained models for inference in production on various \\nplatforms, ranging from large distributed clusters in a \\ndatacenter, down to performing inference locally on \\nmobile devices. At the same time, it is flexible and \\ngeneral enough to support experimentation and \\nresearch into new machine learning models and \\nsystem-level optimizations. \\nTensorFlow uses a unified dataflow graph to \\nrepresent both the computation in an algorithm and \\nthe state on which the algorithm operates. We draw \\ninspiration from the high-level programming models \\nof dataflow systems [2, 22, 75], and the low-level \\nefficiency of parameter servers [14, 21, 46]. Unlike \\ntraditional dataflow systems, in which graph vertices \\nrepresent functional computation on immutable data, \\nTensorFlow allows vertices to represent computations \\nthat own or update mutable state. Edges carry tensors'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 0}, page_content='represent functional computation on immutable data, \\nTensorFlow allows vertices to represent computations \\nthat own or update mutable state. Edges carry tensors \\n(multi-dimensional arrays) between nodes, and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 1}, page_content='2 \\nTensorFlow transparently inserts the appropriate \\ncommunication \\nbetween \\ndistributed \\nsubcomputations. By unifying the computation and \\nstate management in a single programming model, \\nTensorFlow allows programmers to experiment with \\ndifferent parallelization schemes that, for example, \\noffload computation onto the servers that hold the \\nshared state to reduce the amount of network traffic. \\nWe have also built various coordination protocols, and \\nachieved encouraging results with synchronous \\nreplication, echoing recent results [11, 19] that \\ncontradict \\nthe \\ncommonly \\nheld \\nbelief \\nthat \\nasynchronous replication is required for scalable \\nlearning [14, 21, 46]. \\nOver the past year, more than 60 teams at Google have \\nused TensorFlow, and we have released the system as an \\nopen-source project. Thanks to our large community of \\nusers we have gained experience with many different \\nmachine learning applications. In this paper, we focus on'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 1}, page_content='open-source project. Thanks to our large community of \\nusers we have gained experience with many different \\nmachine learning applications. In this paper, we focus on \\nneural network training as a challenging systems problem, \\nand select two representative applications from this space: \\nimage classification and language modeling. These \\napplications \\nstress \\ncomputational \\nthroughput \\nand \\naggregate model size respectively, and we use them both to \\ndemonstrate the extensibility of TensorFlow, and to \\nevaluate the efficiency and scalability of our present \\nimplementation. \\n2 Background & Motivation \\nTo make the case for developing TensorFlow, we start by \\noutlining the requirements for a large-scale machine \\nlearning system (§2.1), then consider how related work \\nmeets or does not meet those requirements (§2.2). \\n2.1 Requirements \\nDistributed execution A cluster of powerful computers can \\nsolve many machine learning problems more efficiently, \\nusing more data and larger models.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 1}, page_content='2.1 Requirements \\nDistributed execution A cluster of powerful computers can \\nsolve many machine learning problems more efficiently, \\nusing more data and larger models. \\nMachine learning algorithms generally perform better \\nwith more training data. For example, recent breakthroughs \\nin image classification models have benefited from the \\npublic ImageNet dataset, which contains 136 gigabytes of \\ndigital images [65]; and language modeling has benefited \\nfrom efforts like the One Billion Word Benchmark [10]. The \\nscale of these datasets motivates a dataparallel approach \\nto training: a distributed file system holds the data, and a \\nset of workers processes different subsets of data in \\nparallel. Data-parallelism eliminates the I/O bottleneck for \\ninput data, and any preprocessing operations can be \\napplied to input records independently. \\nEffective learned models for image recognition, language \\nmodeling, document clustering, and many other problems'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 1}, page_content='applied to input records independently. \\nEffective learned models for image recognition, language \\nmodeling, document clustering, and many other problems \\nhave a large number of parameters. For example, the \\ncurrent state-of-the-art image classification model, ResNet, \\nuses 2.3 million floating-point parameters to classify images \\ninto one of 1000 categories [26]. The One Billion Word \\nBenchmark has a vocabulary of 800,000 words, and it has \\nbeen used to train language models with 1.04 billion \\nparameters [39]. A distributed system can shard the model \\nacross many processes, to increase the available network \\nbandwidth when many workers are simultaneously reading \\nand updating the model. \\nA distributed system for model training must use \\nthe network efficiently. Many scalable algorithms \\ntrain a model using mini-batch gradient descent [21, \\n47], where a worker reads the current version of the \\nmodel and a small batch of input examples, calculates'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 1}, page_content='train a model using mini-batch gradient descent [21, \\n47], where a worker reads the current version of the \\nmodel and a small batch of input examples, calculates \\nan update to the model that reduces a loss function \\non those examples, and applies the update to the \\nmodel. Mini-batch methods are most effective when \\neach worker uses the most current model as a starting \\npoint, which requires a large amount of data to be \\ntransferred to the worker with low latency. \\nAccelerator support Machine learning algorithms \\noften perform expensive computations, such as matrix \\nmultiplication and multi-dimensional convolution, \\nwhich are highly parallelizable, but have many data \\ndependencies \\nthat \\nrequire \\na \\ntightly \\ncoupled \\nimplementation. The recent availability of general-\\npurpose GPUs has provided a large number of cores \\nthat can operate on fast local memory. For example, a \\nsingle NVIDIA Titan X GPU card has 6 TFLOPS peak \\nperformance [60]. In 2012, state-ofthe-art results for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 1}, page_content='that can operate on fast local memory. For example, a \\nsingle NVIDIA Titan X GPU card has 6 TFLOPS peak \\nperformance [60]. In 2012, state-ofthe-art results for \\ndifferent image classification tasks were achieved \\nusing 16,000 CPU cores for three days [45], and using \\ntwo GPUs for six days [42]. Since then, GPU vendors \\nhave innovated in their support for machine learning: \\nNVIDIA’s cuDNN library [13] for GPU-based neural \\nnetwork training accelerates several popular image \\nmodels by 2–4× when using version R4 in place of R2 \\n[15]. \\nIn addition to general-purpose devices, many \\nspecialpurpose accelerators for deep learning have'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 2}, page_content='3 \\nachieved significant performance improvements and \\npower savings. At Google, our colleagues have built \\nthe Tensor Processing Unit (TPU) specifically for \\nmachine learning, and it achieves an order of \\nmagnitude improvement in performance-per-watt \\ncompared to alternative state-of-the-art technology \\n[38]. The Movidius Deep Learning Accelerator uses a \\nlow-power Myriad 2 processor with custom vector \\nprocessing units that accelerate many machine \\nlearning and computer vision algorithms [53]. \\nOvtcharov \\net \\nal. \\nhave \\nachieved \\nsignificant \\nperformance improvements and power savings for \\nsome convolutional models using field programmable \\ngate arrays (FPGAs) [58]. Since it is difficult to predict \\nthe next popular architecture for executing machine \\nlearning algorithms, we require that TensorFlow uses \\na portable programming model that can target a \\ngeneric device abstraction, and allows its operations \\nto be specialized for new architectures as they \\nemerge.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 2}, page_content='a portable programming model that can target a \\ngeneric device abstraction, and allows its operations \\nto be specialized for new architectures as they \\nemerge. \\nTraining & inference support In addition to training, \\nscalable and high-performance inference is a \\nrequirement for using models in production [18]. \\nDepending on the nature of the application, the \\ninference may be required to produce results with \\nvery low latency in an interactive service, or execute \\non a disconnected mobile device. If the model is large, \\nit might require multiple servers to participate in each \\ninference computation, and thus require distributed \\ncomputation support. Developers benefit when they \\ncan use the same code to define a model for both \\ntraining and inference. Training and inference demand \\nsimilar performance, so we prefer a common \\nwelloptimized system for both computations. Since \\ninference can be computationally intensive (e.g., an \\nimage classification model might perform 5 billion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 2}, page_content='welloptimized system for both computations. Since \\ninference can be computationally intensive (e.g., an \\nimage classification model might perform 5 billion \\nFLOPS per image [70]), it must be possible to \\naccelerate it with GPUs. \\nExtensibility Single-machine machine learning frameworks \\n[36, 2, 17] have extensible programming models that enable \\ntheir users to advance the state of the art with new \\napproaches, such as adversarial learning [25] and deep \\nreinforcement learning [51]. We seek a system that \\nprovides the same ability to experiment, and also allows \\nusers to scale up the same code to run in production. The \\nsystem must support expressive control-flow and stateful \\nconstructs, while also satisfying our other requirements. \\n2.2 Related work \\nSingle-machine frameworks Many machine learning \\nresearchers carry out their work on a single—often \\nGPUequipped—computer [41, 42], and many flexible \\nsinglemachine frameworks have emerged to support this'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 2}, page_content='researchers carry out their work on a single—often \\nGPUequipped—computer [41, 42], and many flexible \\nsinglemachine frameworks have emerged to support this \\nscenario. Caffe [36] is a high-performance framework for \\ntraining declaratively specified convolutional neural \\nnetworks that runs on multicore CPUs and GPUs. Theano [2] \\nallows programmers to express a model as a dataflow \\ngraph, and generates efficient compiled code for training \\nthat model. Torch [17] has an imperative programming \\nmodel for scientific computation (including machine \\nlearning) that supports fine-grained control over the order \\nof execution and memory utilization. \\nWhile these frameworks do not satisfy our requirement \\nfor distributed execution, TensorFlow’s programming \\nmodel is close to Theano’s dataflow representation \\n(§3). \\nBatch dataflow systems Starting with MapReduce [22], \\nbatch dataflow systems have been applied to a large \\nnumber of machine learning algorithms [71], and more'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 2}, page_content='(§3). \\nBatch dataflow systems Starting with MapReduce [22], \\nbatch dataflow systems have been applied to a large \\nnumber of machine learning algorithms [71], and more \\nrecent systems have focused on increasing expressivity and \\nperformance. DryadLINQ [74] adds a high-level query \\nlanguage that supports more sophisticated algorithms than \\nMapReduce. Spark [75] extends DryadLINQ with the ability \\nto cache previously computed datasets in memory, and is \\ntherefore better suited to iterative machine learning \\nalgorithms (such as k-means clustering and logistic \\nregression) when the input data fit in memory. Dandelion \\nextends DryadLINQ to support generating code for GPUs \\n[63] and FPGAs [16]. \\nThe principal limitation of a batch dataflow system \\nis that it requires the input data to be immutable, and \\nall of the subcomputations to be deterministic, so that \\nthe system can re-execute subcomputations when \\nmachines in the cluster fail. This feature—which is'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 2}, page_content='all of the subcomputations to be deterministic, so that \\nthe system can re-execute subcomputations when \\nmachines in the cluster fail. This feature—which is \\nbeneficial for many conventional workloads—makes \\nupdating a machine learning model a heavy operation. \\nFor example, the SparkNet system for training deep \\nneural networks on Spark takes 20 seconds to \\nbroadcast weights and collect updates from five \\nworkers [52]. As a result, these systems must process \\nlarger batches in each model update step, which slows'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 3}, page_content='4 \\nconvergence [9]. We show in Subsection 6.3 that \\nTensorFlow can train larger models on larger clusters \\nwith step times as short as 2 seconds. \\nWhile not a batch dataflow system, Naiad [54] \\naugments a dataflow model with streaming execution, \\nstateful vertices, and structured timestamps (“timely \\ndataflow”) that enable it to handle incremental \\nupdates and iterative algorithms in the same \\ncomputation. Naiad represents iteration using cyclic \\ndataflow graphs, which together with mutable state \\nmake it possible to implement algorithms that require \\nmillisecond-scale latencies for coordination. Naiad is \\ndesigned for computing on sparse, discrete data, and \\ndoes not support GPU (or any other form of) \\nacceleration, but we borrow aspects of timely \\ndataflow iteration in Subsection 3.4. \\nParameter servers Inspired by work on distributed \\nkey-value stores, a parameter server architecture uses \\na set of servers to manage shared state that is updated'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 3}, page_content='Parameter servers Inspired by work on distributed \\nkey-value stores, a parameter server architecture uses \\na set of servers to manage shared state that is updated \\nby a set of data-parallel workers. Unlike a standard \\nkey-value store, the write operation in a parameter \\nserver is specialized for parameter updates: it is \\ntypically an associative and commutative combiner, \\nlike addition-assignment (+=), that is applied to the \\ncurrent parameter value and the incoming update to \\nproduce a new parameter value. \\nParameter servers emerged as an architecture for \\nscalable topic modeling [66], and our previous system \\nDistBelief [21] showed how a similar architecture \\ncould be applied to deep neural network training. \\nProject Adam [14] demonstrated an efficient \\nparameter \\nserver \\narchitecture \\nfor \\ntraining \\nconvolutional neural networks, and Li et al.’s \\n“Parameter Server” [46] added innovations in \\nconsistency models, fault tolerance, and elastic'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 3}, page_content='parameter \\nserver \\narchitecture \\nfor \\ntraining \\nconvolutional neural networks, and Li et al.’s \\n“Parameter Server” [46] added innovations in \\nconsistency models, fault tolerance, and elastic \\nrescaling. Despite earlier skepticism that parameter \\nservers would be compatible with GPU acceleration \\n[14], Cui et al. have recently shown that GeePS [19], a \\nparameter server specialized for use with GPUs, can \\nachieve speedups on modest-sized clusters. \\nMXNet [12] is a recent system that uses a parameter \\nserver to scale training, supports GPU acceleration, and \\nincludes a flexible programming model with interfaces for \\nmany languages. While MXNet partially fulfills our \\nextensibility requirements, the parameter server is \\n“privileged” code, which makes it difficult for researchers to \\ncustomize the handling of large models (§4.2). \\nThe parameter server architecture meets most of our \\nrequirements, and our DistBelief [21] uses parameter'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 3}, page_content='customize the handling of large models (§4.2). \\nThe parameter server architecture meets most of our \\nrequirements, and our DistBelief [21] uses parameter \\nservers with a Caffe-like model definition format [36] to \\ngreat effect. We found this architecture to be insufficiently \\nextensible, because adding a new optimization algorithm, \\nor \\nexperimenting \\nwith \\nan \\nunconventional \\nmodel \\narchitecture would require our users to modify the \\nparameter server implementation, which uses C++ for \\nperformance. While some of the practitioners who use that \\nsystem are comfortable with making these changes, the \\nmajority are accustomed to writing models in high-level \\nlanguages, such as Python and Lua, and the complexity of \\nthe highperformance parameter server implementation is a \\nbarrier to entry. With TensorFlow we therefore sought a \\nhighlevel programming model that allows users to \\ncustomize the code that runs in all parts of the system (§3). \\n3 TensorFlow execution model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 3}, page_content='barrier to entry. With TensorFlow we therefore sought a \\nhighlevel programming model that allows users to \\ncustomize the code that runs in all parts of the system (§3). \\n3 TensorFlow execution model \\nTensorFlow uses a single dataflow graph to represent all \\ncomputation and state in a machine learning algorithm, \\nincluding the individual mathematical operations, the \\nparameters and their update rules, and the input \\npreprocessing \\n(Figure \\n1). \\nDataflow \\nmakes \\nthe \\ncommunication between subcomputations explicit, and \\ntherefore makes it easy to execute independent \\ncomputations in parallel, and partition the computation \\nacross multiple distributed devices. Dataflow TensorFlow \\ndiffers from batch dataflow systems (§2.2) in two respects: \\n• The model supports multiple concurrent executions on \\noverlapping subgraphs of the overall graph. \\n• Individual vertices may have mutable state that can be \\nshared between different executions of the graph.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 3}, page_content='overlapping subgraphs of the overall graph. \\n• Individual vertices may have mutable state that can be \\nshared between different executions of the graph. \\nThe key observation in the parameter server architecture \\n[21, 14, 46] is that mutable state is crucial when training \\nvery large models, because it becomes possible to make in-\\nplace updates to very large parameters, and propagate \\nthose updates to parallel training steps as quickly as \\npossible. Dataflow with mutable state enables TensorFlow \\nto mimic the functionality of a parameter server, but with \\nadditional flexibility, because it becomes possible to \\nexecute arbitrary dataflow subgraphs on the machines that \\nhost the shared model parameters. As a result, our users \\nhave been able to experiment with different optimization \\nalgorithms, consistency schemes, and parallelization \\nstrategies.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 4}, page_content='5 \\n3.1 Dataflow graph elements \\nIn a TensorFlow graph, each vertex represents an \\natomic unit of computation, and each edge represents \\nthe output from or input to a vertex. We refer to the \\ncomputation at vertices as operations, and the values \\nthat flow along edges as tensors, because TensorFlow \\nis designed for mathematical computation, and uses \\ntensors (or multidimensional arrays) to represent all \\ndata in those computations. \\nTensors In TensorFlow, we model all data as tensors \\n(dense n-dimensional arrays) with each element \\nhaving one of a small number of primitive types, such \\nas int32, float32, or string. Tensors naturally represent \\nthe inputs to and results of the common mathematical \\noperations in many machine learning algorithms: for \\nexample, a matrix multiplication takes two 2-D tensors \\nand produces a 2-D tensor; and a mini-batch 2-D \\nconvolution takes two 4-D tensors and produces \\nanother 4-D tensor. \\nAll tensors in TensorFlow are dense. This decision'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 4}, page_content='and produces a 2-D tensor; and a mini-batch 2-D \\nconvolution takes two 4-D tensors and produces \\nanother 4-D tensor. \\nAll tensors in TensorFlow are dense. This decision \\nensures that the lowest levels of the system can have \\nsimple implementations for memory allocation and \\nserialization, which reduces the overhead imposed by \\nthe framework. To represent sparse tensors, \\nTensorFlow offers two alternatives: either encode the \\ndata into variable-length string elements of a dense \\ntensor, or use a tuple of dense tensors (e.g., an n-D \\nsparse tensor with m non-zero elements could be \\nrepresented an m×n index matrix and a length-m \\nvalue vector). The size of a tensor can vary in one or \\nmore dimensions, making it possible to represent \\nsparse tensors with differing numbers of elements, at \\nthe cost of more sophisticated shape inference. \\nOperations An operation takes m ≥ 0 tensors as input, \\nand produces n ≥ 0 tensors as output. An operation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 4}, page_content='the cost of more sophisticated shape inference. \\nOperations An operation takes m ≥ 0 tensors as input, \\nand produces n ≥ 0 tensors as output. An operation \\nhas a named “type” (such as Const, MatMul, or Assign) \\nand may have zero or more compile-time attributes \\nthat determine its behavior. An operation can be \\ngeneric and variadic at compile-time: its attributes \\ndetermine both the expected types and arity of its \\ninputs and outputs. preprocessing, training, and \\ncheckpointing state. \\nFor example, the simplest operation Const has no inputs \\nand a single output. Const has an attribute T that \\ndetermines the type of its output, and an attribute Value \\nthat determines the value that it produces. AddN is variadic: \\nit has a type attribute T, and an integer attribute N that \\ndefines how many inputs (of type T) it accepts. \\nStateful operations: variables An operation can contain \\nmutable state that is read and/or written each time it'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 4}, page_content='defines how many inputs (of type T) it accepts. \\nStateful operations: variables An operation can contain \\nmutable state that is read and/or written each time it \\nexecutes. A Variable operation owns a mutable buffer that \\nis used to store the shared parameters of a model as it is \\ntrained. A Variable has no inputs, and produces a reference \\nhandle, which acts as a typed capability for reading and \\nwriting the buffer. A Read operation takes a reference \\nhandle as input, and outputs the value of the variable as a \\ndense tensor. Several operations can modify the underlying \\nbuffer: for example, AssignAdd takes a reference handle r \\nand a tensor value x, and when executed performs the \\nupdate State0[r] ← State[r]+x. Subsequent Read(r) \\noperations produce the value State0[r]. \\nStateful operations: queues TensorFlow includes several \\nqueue implementations, which support more advanced \\nforms of coordination. The simplest queue is FIFOQueue,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 4}, page_content='Stateful operations: queues TensorFlow includes several \\nqueue implementations, which support more advanced \\nforms of coordination. The simplest queue is FIFOQueue, \\nwhich owns an internal queue of tensors, and supports \\nconcurrent access. Like a Variable, the FIFOQueue \\noperation produces a reference handle that can be \\nconsumed by one of the standard queue operations, such \\nas Enqueue and Dequeue. These operations respectively \\npush their input onto the tail of the queue, or pop the head \\nelement and output it. Enqueue will block if its given queue \\nis full, and Dequeue will block if its given queue is empty. \\nWhen queues are used in an input preprocessing pipeline, \\nthis blocking provides backpressure; it also supports \\nsynchronization (§4.4). \\n \\nFigure 1: A schematic TensorFlow dataflow graph for a training pipeline contains subgraphs for reading input data,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 5}, page_content='6 \\n3.2 Partial and concurrent execution \\nTensorFlow uses the dataflow graph to represent all \\npossible computations in a particular application, and the \\nAPI for executing a graph allows the client to specify the \\nsubgraph that should be executed. A subgraph is specified \\ndeclaratively: the client selects zero or more edges to feed \\ninput tensors into the dataflow, and one or more edges to \\nfetch output tensors from the dataflow; the runtime then \\nprunes the graph to contain the necessary set of operations. \\nEach invocation of the API is called a step, and TensorFlow \\nsupports multiple concurrent steps on the same graph, \\nwhere stateful operations enable coordination between the \\nsteps. \\nFigure 1 shows a typical training application, with \\nmultiple subgraphs that execute concurrently, and \\ninteract through shared variables and queues. The \\ncore training subgraph depends on a set of model \\nparameters, and input batches from a queue. Many'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 5}, page_content='interact through shared variables and queues. The \\ncore training subgraph depends on a set of model \\nparameters, and input batches from a queue. Many \\nconcurrent steps of the training subgraph update the \\nmodel based on different input batches, to implement \\ndata-parallel training. To fill the input queue, \\nconcurrent preprocessing steps transform individual \\ninput records (e.g., decoding images and applying \\nrandom distortions), and a separate I/O subgraph \\nreads records from a distributed file system. A \\ncheckpointing subgraph runs periodically for fault \\ntolerance (§4.3). \\nPartial and concurrent execution is responsible for \\nmuch of TensorFlow’s flexibility. Adding mutable state \\nand coordination via queues makes it possible to \\nspecify a wide variety of model architectures in \\n“unprivileged” code, which enables advanced users to \\nexperiment without modifying the internals of the \\nTensorFlow runtime. \\n3.3 Distributed execution \\nDataflow simplifies distributed execution, because it'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 5}, page_content='experiment without modifying the internals of the \\nTensorFlow runtime. \\n3.3 Distributed execution \\nDataflow simplifies distributed execution, because it \\nmakes communication between subcomputations \\nexplicit. In principle, the same TensorFlow program \\ncan be deployed to a distributed cluster of GPUs for \\ntraining, a cluster of TPUs for serving, and a cellphone \\nfor mobile inference. \\nEach operation resides on a particular device, such \\nas a CPU or GPU in a particular task. A device is \\nresponsible for executing a kernel for each operation \\nassigned to it. \\nTensorFlow allows multiple kernels to be registered for a \\nsingle operation, with specialized implementations for a \\nparticular device or data type (see §5 for details). For many \\noperations, such as element-wise operators (Add, Sub, \\netc.), we use a single kernel implementation that can be \\ncompiled for CPU and GPU using different compilers. \\nThe TensorFlow runtime places operations on devices,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 5}, page_content='etc.), we use a single kernel implementation that can be \\ncompiled for CPU and GPU using different compilers. \\nThe TensorFlow runtime places operations on devices, \\nsubject to implicit or explicit device constraints in the graph. \\nThe placement algorithm computes a feasible set of devices \\nfor each operation, calculates the sets of operations that \\nmust be colocated, and selects a satisfying device for each \\ncolocation group. Stateful operations and operations their \\nstate must be placed on the same device, which leads to \\nimplicit colocation constraints. In addition, the user may \\nspecify partial device preferences such as “any device in a \\nparticular task”, or “a GPU in any task”, and the runtime will \\nrespect these constraints. A typical training application will \\nuse client-side programming constructs to add constraints \\nsuch that, for example, parameters are distributed among a \\nset of “PS” tasks. \\nOnce the operations in a graph have been placed, and the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 5}, page_content='use client-side programming constructs to add constraints \\nsuch that, for example, parameters are distributed among a \\nset of “PS” tasks. \\nOnce the operations in a graph have been placed, and the \\npartial subgraph has been computed for a step (§3.2), \\nTensorFlow partitions the operations into per-device \\nsubgraphs. A per-device subgraph for device d contains all \\nof the operations that were assigned to d, with additional \\nSend and Recv operations that replace edges across device \\nboundaries. Send transmits its single input to a specified \\ndevice as soon as the tensor is available, using a rendezvous \\nkey to name the value. Recv has a single output, and blocks \\nuntil the value for a specified rendezvous key is available \\nlocally, before producing that value. Send and Recv have \\nspecialized implementations for several device-type pairs; \\nwe describe some of these in Section 5. \\nWe optimized TensorFlow for executing large subgraphs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 5}, page_content='specialized implementations for several device-type pairs; \\nwe describe some of these in Section 5. \\nWe optimized TensorFlow for executing large subgraphs \\nrepeatedly with low latency. Once the graph for a step has \\nbeen pruned, placed, and partitioned, its subgraphs are \\ncached in their respective devices. A client session \\nmaintains the mapping from step definitions to cached \\nsubgraphs, so that a distributed step on a large graph can \\nbe initiated with one small message to each participating \\ntask. This model favors static, reusable graphs, but it can \\nsupport dynamic computations using dynamic control flow, \\nas the next subsection describes. \\n3.4 Dynamic control flow \\nMost evaluation in TensorFlow is strict: all inputs to an \\noperation must be computed before the operation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 6}, page_content='7 \\nexecutes. Advanced algorithms—such as efficiently training \\na recurrent neural network [37]—require dynamic control \\nflow, which for efficiency requires non-strict evaluation. \\n \\nFigure 2: A conditional graph using Switch and Merge \\nTensorFlow supports conditional control flow using \\nthe primitive Switch and Merge operations, which are \\nbased on Arvind and Culler’s original dynamic \\ndataflow architectures [4]. Switch acts like a \\ndemultiplexer: it takes a data input and a control \\ninput, and uses the control input to select which of its \\ntwo outputs should produce a value. The Switch \\noutput not taken receives a special dead value, which \\npropagates recursively through the rest of the graph \\nuntil it reaches a Merge operation. Merge acts like a \\nmultiplexer: it forwards at most one non-dead input \\nto its output, or produces a dead output if both of its \\ninputs are dead. We use these primitives to build a \\nnonstrict conditional subgraph (Figure 2) that'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 6}, page_content='to its output, or produces a dead output if both of its \\ninputs are dead. We use these primitives to build a \\nnonstrict conditional subgraph (Figure 2) that \\nexecutes one of two branches, based on the runtime \\nvalue of a tensor. \\nSwitch and Merge also support iteration. The \\nimplementation of loops in TensorFlow is based on \\nSwitch and Merge [4], with additional structural \\nconstraints based on timely dataflow [54] to simplify \\nthe distributed execution state. Like timely dataflow, \\nTensorFlow supports multiple concurrent iterations \\nand nested loops, but simplifies memory management \\nby restricting each operation to producing a single \\nvalue per output per iteration. \\n4 Extensibility case studies \\nBy choosing a unified dataflow graph to represent all \\ncomputation in TensorFlow, we have enabled users to \\nexperiment with features that were built into the \\nruntime of our previous system [21]. In this section, \\nwe discuss four extensions to TensorFlow that we'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 6}, page_content='experiment with features that were built into the \\nruntime of our previous system [21]. In this section, \\nwe discuss four extensions to TensorFlow that we \\nhave built using simple dataflow primitives and “user-\\nlevel” code. \\n4.1 Differentiation and optimization \\nMany learning algorithms train a set of parameters \\nusing some variant of stochastic gradient descent \\n(SGD), which entails computing the gradients of a cost \\nfunction with respect to those parameters, then \\nupdating the parameters based on those gradients. \\nWe implement a user-level library for TensorFlow that \\nautomatically differentiates expressions. A user can, \\nfor example, define a neural network as a composition \\nof layers and a loss function, and the library will derive \\nthe backpropagation [64]. \\nThe differentiation algorithm performs breadth-first \\nsearch to identify all of the backwards paths from the target \\noperation (e.g., a loss function) to a set of parameters, and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 6}, page_content='The differentiation algorithm performs breadth-first \\nsearch to identify all of the backwards paths from the target \\noperation (e.g., a loss function) to a set of parameters, and \\nsums the partial gradients that each path contributes. Our \\nusers frequently specialize the gradients for some \\noperations, and they have implemented optimizations like \\nbatch normalization [32] and gradient clipping [59] to \\naccelerate training and make it more robust. We have \\nextended the algorithm to differentiate conditional and \\niterative \\nsubcomputations \\n(§3.4), \\nand \\ndeveloped \\ntechniques for managing GPU memory when iterating (and \\naccumulating intermediate values) over long sequences in \\nthe input data (similar to GeePS [19]). \\nTensorFlow users can also experiment with a wide range \\nof optimization algorithms, which compute new values for \\nthe parameters in each training step. SGD is easy to \\nimplement in a parameter server: for each parameter W,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 6}, page_content='of optimization algorithms, which compute new values for \\nthe parameters in each training step. SGD is easy to \\nimplement in a parameter server: for each parameter W, \\ngradient ∂L/∂W, and learning rate α, the update rule is W0 \\n← W − α × ∂L/∂W. A parameter server can implement SGD \\nby using -= as the write operation, and writing α × ∂L/∂W \\nto each W after a training step. \\nHowever, there are many more advanced optimization \\nschemes that are difficult to express as a single write \\noperation. For example, the Momentum algorithm \\naccumulates a “velocity” for each parameter based on its \\ngradient over multiple iterations, then computes the \\nparameter update from that accumulation; and many \\nrefinements to this algorithm have been proposed [67]. To \\nimplement Momentum in DistBelief [21], we had to modify \\nthe C++ code of the parameter server to change the \\nrepresentation of parameter data, and execute arbitrary \\ncode in the write operation; such modifications are beyond'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 6}, page_content='the C++ code of the parameter server to change the \\nrepresentation of parameter data, and execute arbitrary \\ncode in the write operation; such modifications are beyond \\nthe majority of our users. Optimization algorithms are the \\nT r u e   b r a n c h \\nF a l s e   b r a n c h \\nS w i t c h \\nM e r g e \\nT r u e'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 7}, page_content='8 \\ntopic of active research, and our users have implemented \\nseveral on top of TensorFlow, including Momentum, \\nAdagrad, Adadelta, RMSProp, Adam, and L-BFGS. These can \\nbe built in TensorFlow using Variable operations and \\nprimitive mathematical operations without needing to \\nmodify the underlying system, which makes it easy to \\nexperiment with new algorithms as they emerge. \\n4.2 Handling very large models \\nTo train a model on high-dimensional data, such as words in \\na corpus of text [7], it is common to use a distributed \\nrepresentation, which embeds a training example as a \\npattern of activity across several neurons, which can be \\n \\nFigure 3: Schematic dataflow graph for a sparse \\nembedding layer containing a two-way sharded \\nembedding matrix. \\nlearned by backpropagation [29]. For example, in a \\nlanguage model, a training example might be a sparse \\nvector with non-zero entries corresponding to the IDs \\nof words in a vocabulary, and the distributed'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 7}, page_content='language model, a training example might be a sparse \\nvector with non-zero entries corresponding to the IDs \\nof words in a vocabulary, and the distributed \\nrepresentation for each word will be a lower-\\ndimensional vector [6]. \\nInference proceeds by multiplying a batch of b \\nsparse vectors against an n×d embedding matrix, \\nwhere n is the number of words in the vocabulary, and \\nd is the desired dimensionality, to produce a much \\nsmaller b × d dense matrix representation; for \\ntraining, most optimization algorithms modify only \\nthe rows of the embedding matrix that were read by \\nthe sparse multiplication. In many TensorFlow models \\nthat process sparse data, n×d can amount to gigabytes \\nof parameters: e.g., a large language model may use \\nover 109 parameters with a vocabulary of 800,000 \\nwords [39], and we have experience with document \\nmodels [20] where the parameters occupy several \\nterabytes. Such models are too large to copy to a \\nworker on every use, or even to store in RAM on a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 7}, page_content='models [20] where the parameters occupy several \\nterabytes. Such models are too large to copy to a \\nworker on every use, or even to store in RAM on a \\nsingle host. \\nWe implement sparse embedding layers in the \\nTensorFlow graph as a composition of primitive \\noperations. Figure 3 shows a simplified graph for an \\nembedding layer that is split across two parameter \\nserver tasks. The core operation of this subgraph is \\nGather, which extracts a sparse set of rows from a \\ntensor, and TensorFlow colocates this operation with \\nthe variable on which it operates. The dynamic \\npartition (Part) operation divides the incoming indices \\ninto variable-sized tensors that contain the indices \\ndestined for each shard, and the dynamic static \\n(Stitch) operation reassembles the partial results from \\neach shard into a single result tensor. Each of these \\noperations has a corresponding gradient, so it \\nsupports automatic differentiation (§4.1), and the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 7}, page_content='each shard into a single result tensor. Each of these \\noperations has a corresponding gradient, so it \\nsupports automatic differentiation (§4.1), and the \\nresult is a set of sparse update operations that act on \\njust the values that were originally gathered from each \\nof the shards. \\nWhile sparse reads and updates are possible in a \\nparameter server [46], TensorFlow adds the flexibility \\nto offload arbitrary computation onto the devices that \\nhost \\nthe \\nshared \\nparameters. \\nFor \\nexample, \\nclassification models typically use a softmax classifier \\nthat multiplies the final output by a weight matrix with \\nc columns, where c is the number of possible classes; \\nfor a language model, c is the size of the vocabulary, \\nwhich can be large. Our users have experimented with \\nseveral schemes to accelerate the softmax calculation. \\nThe first is similar to an optimization in Project Adam \\n[14], whereby the weights are sharded across several \\ntasks, and the multiplication and gradient calculation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 7}, page_content='The first is similar to an optimization in Project Adam \\n[14], whereby the weights are sharded across several \\ntasks, and the multiplication and gradient calculation \\nare colocated with the shards. More efficient training \\nis possible using a sampled softmax [35], which \\nperforms a sparse multiplication based on the true \\nclass for an example and a set of randomly sampled \\nfalse classes. We compare the performance of these \\ntwo schemes in §6.4. \\n4.3 Fault tolerance \\nTraining a model can take several hours or days, even using \\na large number of machines [21, 14]. It is desirable to be \\nable to train a model using non-dedicated resources, for \\nexample using a cluster manager, like Mesos [28] or Borg \\n[72], that does not guarantee availability of the same \\nresources for the duration of the training process.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 8}, page_content='9 \\nTherefore, a TensorFlow job is likely to experience failure \\nduring the training process, and we require some form of \\nfault tolerance. However, failures are unlikely to be so \\ncommon that individual operations need fault tolerance, so \\na mechanism like Spark’s RDDs [75] would impose \\nsignificant overhead for little benefit. There is no need to \\nmake every write to the parameter state durable, because \\nwe can recompute any update from the input data, and \\nmany learning algorithms do not require strong consistency \\n[62]. Although we do not use strong consistency for the \\ntraining state, we rely on a system like Chubby [8] or \\nZooKeeper [31] to map task IDs to IP addresses. \\nWe implement user-level checkpointing for fault \\ntolerance in TensorFlow, using primitive operations in the \\ngraph (Figure 1): Save writes one or more tensors to a \\ncheckpoint file, and Restore reads one or more tensors from \\na checkpoint file. Our typical configuration connects each'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 8}, page_content='graph (Figure 1): Save writes one or more tensors to a \\ncheckpoint file, and Restore reads one or more tensors from \\na checkpoint file. Our typical configuration connects each \\nVariable in a task to the same Save operation, with one Save \\nper task, to maximize the I/O bandwidth to a distributed file \\nsystem. The Restore operations read named tensors from a \\nfile, and a standard Assign stores the restored value in its \\nrespective variable. During training, a typical client runs all \\nof the Save operations periodically to produce a new \\ncheckpoint; when the client starts up, it attempts to Restore \\nthe latest checkpoint. \\nTensorFlow includes a client library for constructing the \\nappropriate graph structure, and invoking Save and Restore \\nas necessary. This behavior is customizable: the user can \\napply different policies to subsets of the variables in a \\nmodel, or customize the checkpoint retention scheme. For \\nexample, many users retain checkpoints with the highest'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 8}, page_content='apply different policies to subsets of the variables in a \\nmodel, or customize the checkpoint retention scheme. For \\nexample, many users retain checkpoints with the highest \\nscore in a custom evaluation metric. The implementation is \\nalso reusable: it may be used for model fine-tuning and \\nunsupervised pre-training [43, 45], which are forms of \\ntransfer learning, in which the parameters of a model \\ntrained on one task (e.g. recognizing general images) are \\nused as the starting point for another task (e.g. recognizing \\nparticular breeds of dog). Having checkpoint and parameter \\nmanagement as programmable operations in the graph \\ngives users the flexibility to implement schemes like these \\nand others that we have not anticipated. \\nThe checkpointing library does not attempt to \\nproduce consistent checkpoints: if training and \\ncheckpointing execute concurrently, the checkpoint \\nmay include none, all, or some of the updates from the \\ntraining step. This is no problem for models that we'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 8}, page_content='checkpointing execute concurrently, the checkpoint \\nmay include none, all, or some of the updates from the \\ntraining step. This is no problem for models that we \\ntrain by asynchronous gradient descent [21]. \\nConsistent \\ncheckpoints \\nrequire \\nadditional \\nsynchronization to ensure that checkpointing does not \\nrun concurrently with update operations. For \\nexample, one can use the scheme in next subsection \\nto take a checkpoint after the synchronous update \\nstep. \\n4.4 Synchronous replica coordination \\nSGD is robust to asynchrony [62], and previous \\nsystems \\ntrain \\ndeep \\nneural \\nnetworks \\nusing \\nasynchronous parameter updates [21, 14], which are \\nbelieved scalable because they maintain high \\nthroughput in the presence of stragglers. The \\nincreased throughput comes at the cost of training \\nsteps using stale data. Some have recently revisited \\nthe assumption that synchronous training does not \\nscale [11, 19]. Since GPUs enable training with \\nhundreds—rather than thousands [45]—of machines,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 8}, page_content='the assumption that synchronous training does not \\nscale [11, 19]. Since GPUs enable training with \\nhundreds—rather than thousands [45]—of machines, \\nit may be possible to train a model synchronously in \\nless time than asynchronous training on the same \\nmachines. \\nThough we designed TensorFlow for asynchronous \\ntraining, we have begun experimenting with \\nsynchronous methods. The TensorFlow graph enables \\nusers to change how parameters are read and written \\nwhen training a model, and we implement three \\nalternatives. In the asynchronous case (Figure 4(a)), \\neach worker reads the current value when the step \\nbegins, and applies its gradient to the different current \\nvalue at the end: this ensures high utilization, but the \\nindividual steps use stale information, making each \\nstep less effective. The synchronous cases use queues \\n(§3.1) to coordinate execution: a blocking queue acts \\nas a barrier to ensure that all workers read the same \\nparameter version, and a second queue accumulates'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 8}, page_content='(§3.1) to coordinate execution: a blocking queue acts \\nas a barrier to ensure that all workers read the same \\nparameter version, and a second queue accumulates \\nmul-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 9}, page_content='10 \\nFigure 5: The layered TensorFlow architecture. \\ntiple gradient updates in order to apply them atomically. \\nThe simple synchronous version (Figure 4(b)) accumulates \\nupdates from all workers before applying them, but slow \\nworkers limit overall throughput. \\nTo mitigate stragglers, we implement backup workers \\n(Figure 4(c), [11]), which are similar to MapReduce backup \\ntasks [22]. Whereas MapReduce starts backup tasks \\nreactively—after \\ndetecting \\na \\nstraggler—our \\nbackup \\nworkers run proactively, and the aggregation takes the first \\nm of n updates produced. We exploit the fact that SGD \\nsamples training data randomly, so each worker processes \\na different random batch. In Subsection 6.3 we show how \\nbackup workers improve throughput by up to 15%. \\n5 Implementation \\nWe implement TensorFlow as an extensible, crossplatform \\nlibrary. Figure 5 illustrates the system architecture: a thin C \\nAPI separates user-level in various languages from the core'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 9}, page_content='We implement TensorFlow as an extensible, crossplatform \\nlibrary. Figure 5 illustrates the system architecture: a thin C \\nAPI separates user-level in various languages from the core \\nlibrary. In this section, we discuss the implementation of the \\nvarious components. \\nThe core TensorFlow library is implemented in C++ for \\nportability and performance: it runs on several operating \\nsystems including Linux, Mac OS X, Android, and iOS; the \\nx86 and various ARM-based CPU architectures; and \\nNVIDIA’s Kepler, Maxwell, and Pascal GPU \\nmicroarchitectures. The implementation is open-source, \\nand we have accepted several external contributions that \\nenable TensorFlow to run on other architectures. \\nThe distributed master translates user requests into \\nexecution across a set of tasks. Given a graph and a \\nstep definition, it prunes (§3.2) and partitions (§3.3) \\nthe graph to obtain subgraphs for each participating \\ndevice, and caches these subgraphs so that they may'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 9}, page_content='step definition, it prunes (§3.2) and partitions (§3.3) \\nthe graph to obtain subgraphs for each participating \\ndevice, and caches these subgraphs so that they may \\nbe re-used in subsequent steps. Since the master sees \\nthe overall computation for a step, it applies standard \\noptimizations \\nsuch \\nas \\ncommon \\nsubexpression \\nelimination and constant folding; pruning is a form of \\ndead code elimination. It then coordinates execution \\nof the optimized subgraphs across a set of tasks. \\nThe dataflow executor in each task handles \\nrequests from the master, and schedules the \\nexecution of the kernels that comprise a local \\nsubgraph. We optimize the dataflow executor for \\nrunning large, fine-grained graphs with low overhead; \\nour current implementation dispatches approximately \\n2,000,000 null operations per second. The dataflow \\nexecutor dispatches kernels to local devices and runs \\nkernels in parallel when possible: e.g., by using \\nmultiple cores in a CPU device, or multiple streams on'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 9}, page_content='executor dispatches kernels to local devices and runs \\nkernels in parallel when possible: e.g., by using \\nmultiple cores in a CPU device, or multiple streams on \\na GPU. \\nThe runtime contains over 200 standard operations, \\nincluding mathematical, array manipulation, control \\nflow, and state management operations. Many of the \\noperation \\nkernels \\nare \\nimplemented \\nusing \\nEigen::Tensor [34], which uses C++ templates to \\ngenerate efficient parallel code for multicore CPUs \\nand GPUs; however, we liberally use libraries like \\ncuDNN [13] to implement kernels where a more \\nefficient specialization is possible. We have also \\nimplemented support for quantization, which enables \\nfaster inference in environments such as mobile \\ndevices and high-throughput datacenter applications, \\nand use the gemmlowp low-precision matrix \\nmultiplication library [33] to accelerate quantized \\ncomputation. \\nWe specialize Send and Recv operations for each pair of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 9}, page_content='and use the gemmlowp low-precision matrix \\nmultiplication library [33] to accelerate quantized \\ncomputation. \\nWe specialize Send and Recv operations for each pair of \\nsource and destination device types. Transfers between \\nlocal CPU and GPU devices use the cudaMemcpyAsync() API \\nto overlap computation and data transfer; transfers \\nbetween two local GPUs use DMA to relieve pressure on the \\nhost. For transfers between tasks, TensorFlow supports \\n \\nFigure 4: Three parameter synchronization schemes for a single parameter in data-parallel training (§4.4): (a) \\nasynchronous, (b) synchronous without backup workers, and (c) synchronous with backup workers.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 10}, page_content='11 \\nmultiple protocols, including gRPC over TCP, and RDMA \\nover Converged Ethernet. We are also investigating \\noptimizations for GPU-to-GPU communication that use \\ncollective operations [57]. \\nSection 4 describes features that we implement totally \\nabove the C API, in user-level code. Typically, users compose \\nstandard operations to build higher-level abstractions, such \\nas neural network layers, optimization algorithms (§4.1), \\nand sharded embedding computations (§4.2). TensorFlow \\nsupports multiple client languages, and we have prioritized \\nsupport for Python and C++, because our internal users are \\nmost familiar with these languages. As features become \\nmore established, we typically port them to C++, so that \\nusers can access an optimized implementation from all \\nclient languages. \\nIf it is difficult or inefficient to represent a \\nsubcomputation as a composition of operations, users can \\nregister additional kernels that provide an efficient'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 10}, page_content='client languages. \\nIf it is difficult or inefficient to represent a \\nsubcomputation as a composition of operations, users can \\nregister additional kernels that provide an efficient \\nimplementation written in C++. We have found it profitable \\nto hand-implement fused kernels for some performance \\ncritical operations, such as a the ReLU and Sigmoid \\nactivation functions and their corresponding gradients. We \\nare currently investigating automatic kernel fusion using \\nHalide [61] and other compiler-based techniques. \\nIn addition to the core runtime, our colleagues have built \\nseveral tools that aid users of TensorFlow. These include \\nserving infrastructure for running inference in production, a \\nvisualization dashboard that enables users to follow the \\nprogress of a training run, a graph visualizer that helps users \\nto understand the connections in a model, and a distributed \\nprofiler that traces the execution of a computation across'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 10}, page_content='progress of a training run, a graph visualizer that helps users \\nto understand the connections in a model, and a distributed \\nprofiler that traces the execution of a computation across \\nmultiple devices and tasks. We describe these tools in an \\nextended whitepaper [1], and they can be downloaded \\nfrom the project repository. \\n6 Evaluation \\nIn this section, we evaluate the performance of TensorFlow \\non several synthetic and realistic workloads. Unless \\notherwise stated, we run all experiments on a shared \\nproduction cluster, and all figures plot median values with \\nerror bars showing the 10th and 90th percentiles. \\nHere we focus on system performance metrics, rather \\nthan learning objectives like time to accuracy. TensorFlow is \\na system that allows machine learning practitioners and \\nresearchers to experiment with new techniques, and this \\nevaluation demonstrates that the system (i) has little \\noverhead, and (ii) can employ large amounts of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 10}, page_content='researchers to experiment with new techniques, and this \\nevaluation demonstrates that the system (i) has little \\noverhead, and (ii) can employ large amounts of \\ncomputation to accelerate real-world applications. While \\ntechniques like synchronous replication can enable some \\nmodels to converge in fewer steps overall, we defer the \\nanalysis of such improvements to other papers. \\n6.1 Single-machine benchmarks \\nAlthough TensorFlow is a system for “large-scale” \\nmachine learning, it is imperative that scalability does \\nnot mask poor performance at small scales [49]. Table \\n1 contains results from Chintala’s independent \\nbenchmark of convolutional models on TensorFlow \\nand three singlemachine frameworks [15]. All \\nframeworks use a six-core Intel Core i7-5930K CPU at \\n3.5GHz and an NVIDIA Titan X GPU. \\nLibrary \\nTraining step time (ms) \\nAlexNet \\nOverfeat \\nOxfordNet GoogleNet \\nCaffe [36] \\n324 \\n823 \\n1068 \\n1935 \\nNeon [56] \\n87 \\n211 \\n320 \\n270 \\nTorch [17] \\n81 \\n268 \\n529 \\n470 \\nTensorFlow \\n81 \\n279'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 10}, page_content='Library \\nTraining step time (ms) \\nAlexNet \\nOverfeat \\nOxfordNet GoogleNet \\nCaffe [36] \\n324 \\n823 \\n1068 \\n1935 \\nNeon [56] \\n87 \\n211 \\n320 \\n270 \\nTorch [17] \\n81 \\n268 \\n529 \\n470 \\nTensorFlow \\n81 \\n279 \\n540 \\n445 \\nTable 1: Step times for training four convolutional \\nmodels with different libraries, using one GPU. All \\nresults are for training with 32-bit floats. The fastest \\nlibrary for each model is shown in bold. \\nTable 1 shows that TensorFlow achieves shorter \\nstep times than Caffe [36], and performance within 6% \\nof the latest version of Torch [17]. We attribute the \\nsimilar performance of TensorFlow and Torch to the \\nfact that both use the same version of the cuDNN \\nlibrary [13], which implements the convolution and \\npooling operations on the critical path for training; \\nCaffe uses open-source implementations for these \\noperations that are simpler but less efficient than \\ncuDNN. \\nThe \\nNeon \\nlibrary \\n[56] \\noutperforms \\nTensorFlow on three of the models, by using \\nhandoptimized \\nconvolutional'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 10}, page_content='operations that are simpler but less efficient than \\ncuDNN. \\nThe \\nNeon \\nlibrary \\n[56] \\noutperforms \\nTensorFlow on three of the models, by using \\nhandoptimized \\nconvolutional \\nkernels \\n[44] \\nimplemented in assembly language; in principle, we \\ncould implement these kernels in TensorFlow, but we \\nhave not yet done so. \\n6.2 Synchronous replica microbenchmark \\nThe performance of our coordination implementation \\n(§4.4) is the main limiting factor for scaling with \\nadditional machines. Figure 6 shows that number of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 11}, page_content='12 \\nnull training steps that TensorFlow performs per \\nsecond for varying model sizes, and increasing \\nnumbers of synchronous workers. In a null training \\nstep, a worker fetches the \\n \\nFigure 6: Baseline throughput for synchronous replication \\nwith a null model. Sparse accesses enable TensorFlow to \\nhandle larger models, such as embedding matrices (§4.2). \\nshared model parameters from 16 PS tasks, performs a \\ntrivial computation, and sends updates to the parameters. \\nThe Scalar curve in Figure 6 shows the best performance \\nthat we could expect for a synchronous training step, \\nbecause only a single 4-byte value is fetched from each PS \\ntask. The median step time is 1.8ms using a single worker, \\ngrowing to 8.8ms with 100 workers. These times measure \\nthe overhead of the synchronization mechanism, and \\ncapture some of the noise that we expect when running on \\na shared cluster. \\nThe Dense curves show the performance of a null step \\nwhen the worker fetches the entire model. We repeat the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 11}, page_content='capture some of the noise that we expect when running on \\na shared cluster. \\nThe Dense curves show the performance of a null step \\nwhen the worker fetches the entire model. We repeat the \\nexperiment with models of size 100MB and 1GB, with the \\nparameters sharded equally over 16 PS tasks. The median \\nstep time for 100MB increases from 147ms with one worker \\nto 613ms with 100 workers. For 1GB, it increases from 1.01s \\nwith one worker to 7.16s with 100 workers. \\nFor large models, it is typical that a training step accesses \\nonly a subset of the parameters, and the Sparse curves \\nshow the throughput of the embedding lookup operation \\nfrom Subsection 4.2. Each worker reads 32 randomly \\nselected entries from a large embedding matrix containing \\n1GB or 16GB of data. As expected, the step times do not \\nvary with the size of the embedding, and TensorFlow \\nachieves step times ranging from 5 to 20ms. \\n6.3 Image classification \\nDeep neural networks have achieved breakthrough'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 11}, page_content='vary with the size of the embedding, and TensorFlow \\nachieves step times ranging from 5 to 20ms. \\n6.3 Image classification \\nDeep neural networks have achieved breakthrough \\nperformance on computer vision tasks such as recognizing \\nobjects in photographs [42], and these tasks are a key \\napplication for TensorFlow at Google. Training a network to \\nhigh accuracy requires a large amount of computation, and \\nwe use TensorFlow to scale out the computation across a \\ncluster of GPU-enabled servers. In these experiments, we \\nfocus on Google’s Inception-v3 model, which achieves \\n78.8% accuracy the ILSVRC 2012 image classification \\nchallenge [70]; the same techniques apply to other deep \\nconvolutional models—such as Microsoft’s ResNet [26]—\\nthat TensorFlow users have implemented. We investigate \\nthe scalability of training the Inception-v3 model using \\nmultiple replicas. We configure a TensorFlow job with 17 PS \\ntasks, and vary the number of worker tasks. Each worker'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 11}, page_content='the scalability of training the Inception-v3 model using \\nmultiple replicas. We configure a TensorFlow job with 17 PS \\ntasks, and vary the number of worker tasks. Each worker \\ntask has one NVIDIA K40 GPU and 5 IvyBridge cores, and a \\nPS task has 8 IvyBridge cores. We investigate the effect of \\ncoordination (§4.4) on training performance, using up to \\n200 workers to validate recent promising results for \\nsynchronous training [11, 19]. In particular, if synchronous \\ntraining can be made efficient, a model such as Inception-\\nV3 will train in fewer steps, and converge to a higher \\naccuracy than with asynchronous training [11]. \\nTraining throughput improves to 2,300 images per \\nsecond as we increase the number of workers to 200, \\nbut with diminishing returns (Figure 7(a)). Figures 7(b) \\nand (c) explain the limits to scaling: as we add more \\nworkers, the step time increases, because there is \\nmore contention on the PS tasks, both at the network'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 11}, page_content='and (c) explain the limits to scaling: as we add more \\nworkers, the step time increases, because there is \\nmore contention on the PS tasks, both at the network \\ninterface and in the aggregation of updates. As \\nexpected, for all configurations, synchronous steps \\nare longer than asynchronous steps, because all \\nworkers must wait for the slowest worker to catch up \\nbefore starting the next step. While the median \\nsynchronous step is approximately 10% longer than an \\nasynchronous step with the same workers, above the \\n90th percentile the synchronous performance \\ndegrades \\nsharply, \\nbecause \\nstragglers \\ndisproportionately impact the tail. \\nTo mitigate tail latency, we can add backup workers, \\nso that a step completes when the first m of n tasks \\nproduce gradients. Figure 8 shows the effect on step \\ntime of adding backup workers to a 50-worker \\nInception training job. Each additional backup worker \\nup to and including the fourth reduces the median'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 11}, page_content='time of adding backup workers to a 50-worker \\nInception training job. Each additional backup worker \\nup to and including the fourth reduces the median \\nstep time, because the probability of a straggler'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 12}, page_content='13 \\naffecting the step decreases. Adding a fifth backup \\nworker slightly degrades performance, because the \\n51st worker (i.e., the first whose result is discarded) is \\nmore likely to be a non-straggler that generates more \\nincoming traffic for the PS tasks. Figure 8 also plots the \\nnormalized speedup for each configuration, which we \\ndefine as t(b)/t(0)×50/(50+b) (where t(b) is the \\nmedian step time with b backup workers), and which \\ndiscounts the speedup by the fraction of additional \\nresources consumed. Although adding 4 backup \\nworkers achieves the shortest overall step time \\n(1.93s), adding 3 achieves the highest normalized \\nspeedup (9.5%), and hence trains the model to the \\nsame quality using less aggregate GPU-time. \\nFigure 8: Backup workers reduce the step time for 50worker \\nInception-v3 training. 4 backup workers give the shortest \\noverall step time, but 3 backup workers are most efficient \\nwhen we normalize for the total resources used. \\n6.4 Language modeling'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 12}, page_content='Inception-v3 training. 4 backup workers give the shortest \\noverall step time, but 3 backup workers are most efficient \\nwhen we normalize for the total resources used. \\n6.4 Language modeling \\nGiven a sequence of words, a language model predicts the \\nmost probable next word [6]. Therefore, language models \\nare integral to predictive text, speech recognition, and \\ntranslation applications. In this experiment, we investigate \\nhow TensorFlow can train a recurrent neural network (viz. \\nLSTM-512-512 [39]) to model the text in the One Billion \\nWord Benchmark [10]. The vocabulary size |V | limits the \\nperformance of training, because the final layer must \\ndecode the output state into probabilities for each of |V | \\nclasses [35]. The resulting parameters can be large (|V |×d \\nfor output state dimension d) so we use the techniques for \\nhandling large models from Subsection 4.2. We use a \\nrestricted vocabulary of the most common 40,000 words—\\ninstead of the full 800,000 words [10]—in order to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 12}, page_content='handling large models from Subsection 4.2. We use a \\nrestricted vocabulary of the most common 40,000 words—\\ninstead of the full 800,000 words [10]—in order to \\nexperiment with smaller configurations. \\nFigure 9 shows the training throughput, measured in \\nFigure 9: Increasing the number of PS tasks leads to \\nincreased throughput for language model training, by \\nparallelizing the softmax computation. Sampled softmax \\nincreases throughput by performing less computation. \\nwords per second, for varying numbers of PS and \\nworker tasks, and two softmax implementations. The \\nfull softmax (dashed lines) multiplies each output by a \\n \\nFigure 7: (a) Inception-v3 training throughput increases with up to 200 workers. However, adding more workers \\ngets diminishing returns because the step time increases for both (b) asynchronous and (c) synchronous \\nreplication. \\n \\n0 \\n1 \\n2 \\n3 \\n4 \\n5 \\nNumber of backup workers \\n1.9 \\n2.0 \\n2.1 \\n2.2 \\n2.3 \\n2.4 \\n2.5 \\nStep time \\n1.00 \\n1.02 \\n1.04 \\n1.06 \\n1.08 \\nSpeedup'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 13}, page_content='14 \\n512 × 40,000 weight matrix sharded across the PS \\ntasks. Adding more PS tasks increases the throughput, \\nbecause TensorFlow can exploit distributed model \\nparallelism [21, 41] and perform the multiplication \\nand gradient calculation on the PS tasks, as in Project \\nAdam [14]. Adding a second PS task is more effective \\nthan increasing from 4 to 32, or 32 to 256 workers. \\nEventually the throughput saturates, as the LSTM \\ncalculations dominate the training step. \\nThe sampled softmax (solid lines) reduces the data \\ntransferred and the computation performed at the PS \\ntasks [35]. Instead of a dense weight matrix, it \\nmultiplies the output by a random sparse matrix \\ncontaining weights for the true class and a random \\nsample of false classes. We sample 512 classes for \\neach batch, which reduces the softmax data transfer \\nand computation by a factor of 78. \\n7 Conclusions \\nWe have described the TensorFlow system and its \\nextensible dataflow-based programming model. The core'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 13}, page_content='and computation by a factor of 78. \\n7 Conclusions \\nWe have described the TensorFlow system and its \\nextensible dataflow-based programming model. The core \\nidea of this paper is that TensorFlow’s dataflow \\nrepresentation subsumes existing work on parameter \\nserver systems, and offers a uniform programming model \\nthat allows users to harness large-scale heterogeneous \\nsystems, both for production tasks and for experimenting \\nwith new approaches. We have shown several examples of \\nhow the TensorFlow programming model supports \\nexperimentation (§4) and demonstrated that the resulting \\nimplementations are performant and scalable (§6). \\nOur initial experience with TensorFlow is encouraging. A \\nlarge number of groups at Google have deployed \\nTensorFlow in production, and TensorFlow is helping our \\nresearch colleagues to make new advances in machine \\nlearning. Since we released TensorFlow as open-source \\nsoftware, over 8,000 people have forked the source code'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 13}, page_content='research colleagues to make new advances in machine \\nlearning. Since we released TensorFlow as open-source \\nsoftware, over 8,000 people have forked the source code \\nrepository, the binary distribution has been downloaded \\n500,000 times, and our users have published dozens of \\nmachine learning models that use TensorFlow. \\nTensorFlow is a work in progress. Its flexible dataflow \\nrepresentation enables power users to achieve excellent \\nperformance, but we have not yet determined default \\npolicies that work well for most users. Further research on \\nautomatic optimization should bridge this gap. On the \\nsystem level, we are actively developing algorithms for \\nautomatic placement, kernel fusion, memory management, \\nand scheduling. While the current implementations of \\nmutable state and fault tolerance suffice for applications \\nwith weak consistency requirements, we expect that some \\nTensorFlow applications will require stronger consistency,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 13}, page_content='mutable state and fault tolerance suffice for applications \\nwith weak consistency requirements, we expect that some \\nTensorFlow applications will require stronger consistency, \\nand we are investigating how to build such policies at user-\\nlevel. Finally, our users are demanding, and some have \\nbegun to chafe at the limitations of a static dataflow graph, \\nespecially for algorithms like deep reinforcement learning \\n[51]. Therefore, we face the intriguing problem of providing \\na system that transparently and efficiently uses distributed \\nresources, even when the structure of the computation \\nunfolds dynamically. \\nBy sharing the implementation of TensorFlow and \\nengaging with the research community, we hope that this \\nwork will spur further research in distributed systems and \\nmachine learning. \\nAcknowledgments \\nWe gratefully acknowledge contributions from our \\ncolleagues within Google, and from members of the wider \\nmachine learning community. In particular, we appreciate'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 13}, page_content='Acknowledgments \\nWe gratefully acknowledge contributions from our \\ncolleagues within Google, and from members of the wider \\nmachine learning community. In particular, we appreciate \\nthe feedback we have received both from the rest of the \\nGoogle Brain team and the hundreds of DistBelief and \\nTensorFlow users that has helped us improve the usability \\nof functionality of the system. \\nMany individuals have contributed to TensorFlow, \\nincluding: John Giannandrea (for creating a supportive \\nresearch environment); Irina Kofman, Amy McDonald \\nSandjideh, and Phing Turner (project management); \\nAshish Agarwal, Dave Andersen, Anelia Angelova, \\nEugene Brevdo, Yaroslav Bulatov, Jerjou Cheng, \\nMaciek Chociej, Craig Citro, Greg Corrado, George \\nDahl, Andrew Dai, Lucy Gao, mig Gerard, Ian \\nGoodfellow, Stephan Gouws, Gunhan Gulsoy, Steinar \\nGunderson, Andrew Harp, Peter Hawkins, Yangqing \\nJia, Rafal Jozefowicz, Łukasz Kaiser, Naveen Kumar, \\nGeoffrey Hinton, Mrinal Kalakrishnan, Anjuli Kannan,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 13}, page_content='Gunderson, Andrew Harp, Peter Hawkins, Yangqing \\nJia, Rafal Jozefowicz, Łukasz Kaiser, Naveen Kumar, \\nGeoffrey Hinton, Mrinal Kalakrishnan, Anjuli Kannan, \\nRasmus Larsen, \\nYutaka Leon-Suematsu, Frank Li, Peter Liu, Xiaobing \\nLiu, Olivia Nordquist, Chris Olah, Nishant Patil, \\nSaurabh Saxena, Mike Schuster, Andrew Selle, Pierre \\nSermanet, Noam Shazeer, Jonathon Shlens, Jascha \\nSohl-Dickstein, Ilya Sutskever, Kunal Talwar, Philip \\nTucker, Vincent Vanhoucke, Oriol Vinyals, Chad \\nWhipkey, Yonghui Wu, Ke Yang, Zongheng Yang, and \\nYao Zhang (general contributions to the project); Shan \\nCarter, Doug Fritz, Patrick Hurst, Dilip Krishnan, Dan \\nMane, Daniel Smilkov, Fer-´ nanda Viegas, Martin \\nWattenberg, James Wexler, Jimbo´ Wilson, Kanit \\nWongsuphasawat, Cassandra Xia, and the Big Picture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 14}, page_content='15 \\nteam (visualization); Chris Leary, Robert Hundt, \\nRobert Springer, Cliff Young, and the Stream Executor \\nteam (accelerator support); Norm Jouppi and the \\nteam that created the Tensor Processing Unit; Kayur \\nPatel, Michael Piatek, and the coLab team; and the \\ngrowing community of open-source contributors and \\nusers who have helped make TensorFlow better. \\nReferences \\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, \\nZ. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, \\nM. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, \\nG. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,´ \\nM. Kudlur, J. Levenberg, D. Mane, R. Monga, S. \\nMoore, D. G. Murray, C. Olah, M. Schuster, J. \\nShlens, B. Steiner, I. Sutskever, K. Talwar, P. A. \\nTucker, V. Vanhoucke, V. Vasudevan, F. B. \\nViegas, O. Vinyals, P. Warden, M. Watten-´ berg, \\nM. Wicke, Y. Yu, and X. Zheng. Tensorflow: Large-\\nscale machine learning on heterogeneous \\ndistributed systems. CoRR, abs/1603.04467, \\n2016. \\narxiv.org/abs/1603.04467.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 14}, page_content='M. Wicke, Y. Yu, and X. Zheng. Tensorflow: Large-\\nscale machine learning on heterogeneous \\ndistributed systems. CoRR, abs/1603.04467, \\n2016. \\narxiv.org/abs/1603.04467. \\nSoftware \\navailable from tensorflow.org. \\n[2] R. Al-Rfou, G. Alain, A. Almahairi, C. Angermueller, D. \\nBahdanau, N. Ballas, F. Bastien, J. Bayer, A. Belikov, A. \\nBelopolsky, Y. Bengio, A. Bergeron, J. Bergstra, V. \\nBisson, J. Bleecher Snyder, N. Bouchard, N. Boulanger-\\nLewandowski, X. Bouthillier, A. de Brebisson, O. \\nBreuleux, P.-´ L. Carrier, K. Cho, J. Chorowski, P. \\nChristiano, T. Cooijmans, M.-A. Cotˆ e, M. C´ otˆ e, A. \\nCourville,´ Y. N. Dauphin, O. Delalleau, J. Demouth, G. \\nDesjardins, S. Dieleman, L. Dinh, M. Ducoffe, V. \\nDumoulin, S. Ebrahimi Kahou, D. Erhan, Z. Fan, O. \\nFirat, M. Germain, X. Glorot, I. Goodfellow, M. \\nGraham, C. Gulcehre, P. Hamel, I. Harlouchet, J.-P. \\nHeng, B. Hidasi, S. Honari, A. Jain, S. Jean, K. Jia, M. \\nKorobov, V. Kulkarni, A. Lamb, P. Lamblin, E. Larsen, C.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 14}, page_content='Graham, C. Gulcehre, P. Hamel, I. Harlouchet, J.-P. \\nHeng, B. Hidasi, S. Honari, A. Jain, S. Jean, K. Jia, M. \\nKorobov, V. Kulkarni, A. Lamb, P. Lamblin, E. Larsen, C. \\nLaurent, S. Lee, S. Lefrancois, S. Lemieux, N. Leonard, \\nZ. Lin, J. A. Livezey,´ C. Lorenz, J. Lowin, Q. Ma, P.-A. \\nManzagol, O. Mastropietro, R. T. McGibbon, R. \\nMemisevic, B. van Merrienboer, V. Michalski, M. \\nMirza,¨ A. Orlandi, C. Pal, R. Pascanu, M. Pezeshki, C. \\nRaffel, D. Renshaw, M. Rocklin, A. Romero, M. Roth, P. \\nSadowski, J. Salvatier, F. Savard, J. Schluter, J. \\nSchulman, G. Schwartz, I. V. Serban,¨ D. Serdyuk, S. \\nShabanian, E. Simon, S. Spieckermann, S. R. \\nSubramanyam, J. Sygnowski, J. Tanguay, G. van Tulder, \\nJ. Turian, S. Urban, P. Vincent, F. Visin, H. de Vries, D. \\nWarde-Farley, D. J. Webb, M. Willson, K. Xu, L. Xue, L. \\nYao, S. Zhang, and Y. Zhang. Theano: A Python \\nframework for fast computation of mathematical \\nexpressions. arXiv e-prints, abs/1605.02688, May \\n2016. arxiv.org/abs/1605.02688.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 14}, page_content='Yao, S. Zhang, and Y. Zhang. Theano: A Python \\nframework for fast computation of mathematical \\nexpressions. arXiv e-prints, abs/1605.02688, May \\n2016. arxiv.org/abs/1605.02688. \\n[3] A. Angelova, A. Krizhevsky, and V. Vanhoucke. \\nPedestrian detection with a large-field-of-view deep \\nnetwork. In Robotics and Automation (ICRA), 2015 \\nIEEE International Conference on, pages 704–711. \\nIEEE, 2015. CalTech PDF. \\n[4] Arvind and D. E. Culler. Annual review of computer \\nscience vol. 1, 1986. chapter Dataflow Architectures, \\npages \\n225–253. \\n1986. \\nwww.dtic.mil/cgi-\\nbin/GetTRDoc?Location=U2& \\ndoc=GetTRDoc.pdf&AD=ADA166235. \\n[5] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object \\nrecognition with visual attention. arXiv preprint \\narXiv:1412.7755, 2014. arxiv.org/abs/1412.7755. \\n[6] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A \\nneural probabilistic language model. \\nJournal of Machine Learning Research, 3:1137– \\n1155, \\n2003. \\nwww.iro.umontreal.ca/˜lisa/pointeurs/'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 14}, page_content='[6] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A \\nneural probabilistic language model. \\nJournal of Machine Learning Research, 3:1137– \\n1155, \\n2003. \\nwww.iro.umontreal.ca/˜lisa/pointeurs/ \\nBengioDucharmeVincentJauvin jmlr.pdf. \\n[7] T. Brants and A. Franz. Web 1T 5-gram version 1, 2006. \\ncatalog.ldc.upenn.edu/LDC2006T13. \\n[8] M. \\nBurrows. \\nThe \\nChubby \\nlock \\nservice \\nfor \\nlooselycoupled distributed systems. In Proceedings of \\nthe 7th Symposium on Operating Systems Design and \\nImplementation, OSDI ’06, pages 335–350, Berkeley, \\nCA, \\nUSA, \\n2006. \\nUSENIX \\nAssociation. \\nwww.usenix.org/legacy/event/osdi06/tech/full \\npapers/burrows/burrows.pdf. \\n[9] R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu. Sample \\nsize selection in optimization methods for machine \\nlearning. Mathematical Programming, 134(1):127–\\n155, 2012. dx.doi.org/10.1007/s10107012-0572-5. \\n[10] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, \\nand P. Koehn. One billion word benchmark for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 14}, page_content='155, 2012. dx.doi.org/10.1007/s10107012-0572-5. \\n[10] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, \\nand P. Koehn. One billion word benchmark for \\nmeasuring progress in statistical language modeling. \\nCoRR, abs/1312.3005, 2013. arxiv.org/abs/1312.3005.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 15}, page_content='16 \\n[11] J. Chen, R. Monga, S. Bengio, and R. Jozefowicz. \\nRevisiting \\ndistributed \\nsynchronous \\nSGD. \\nIn \\nInternational Conference on Learning Representations \\nWorkshop Track, 2016. arxiv.org/abs/1604.00981. \\n[12] T. Chen, \\nM. Li, \\nY. Li, \\nM. Lin, N. Wang, \\nM. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. \\nMXNet: A flexible and efficient machine learning \\nlibrary for heterogeneous distributed systems. In \\nProceedings of the Workshop on Machine \\nLearning \\nSystems \\nat \\nNeural \\nInformation \\nProcessing Systems (LearningSys), Dec. 2015. \\nwww.cs.cmu.edu/ muli/file/mxnet-learning-sys.pdf. \\n[13] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. \\nTran, B. Catanzaro, and E. Shelhamer. cuDNN: Efficient \\nprimitives \\nfor \\ndeep \\nlearning. \\narXiv \\npreprint \\narXiv:1410.0759, 2014. arxiv.org/abs/1410.0759. \\n[14] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. \\nProject Adam: Building an efficient and scalable deep \\nlearning training system. In 11th USENIX Symposium'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 15}, page_content='[14] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. \\nProject Adam: Building an efficient and scalable deep \\nlearning training system. In 11th USENIX Symposium \\non Operating Systems Design and Implementation \\n(OSDI 14), pages 571–582, 2014. \\nwww.usenix.org/system/files/conference/osdi14/ \\nosdi14-paper-chilimbi.pdf. \\n[15] S. Chintala. convnet-benchmarks, \\n2016. \\ngithub.com/soumith/convnet-benchmarks. \\n[16] E. S. Chung, J. D. Davis, and J. Lee. LINQits: Big data on \\nlittle clients. In Proceedings of the 40th Annual \\nInternational Symposium on Computer Architecture, \\nISCA ’13, pages 261–272, New York, NY, USA, 2013. \\nACM. doi.acm.org/10.1145/2485922.2485945. \\n[17] R. Collobert, S. Bengio, and J. Mariethoz.´ Torch: A \\nmodular machine learning software library. Technical \\nreport, \\nIDIAP, \\n2002. \\ninfoscience.epfl.ch/record/82802/files/rr02-46.pdf. \\n[18] D. Crankshaw, P. Bailis, J. E. Gonzalez, H. Li, Z. Zhang, \\nM. J. Franklin, A. Ghodsi, and M. I. Jordan. The missing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 15}, page_content='report, \\nIDIAP, \\n2002. \\ninfoscience.epfl.ch/record/82802/files/rr02-46.pdf. \\n[18] D. Crankshaw, P. Bailis, J. E. Gonzalez, H. Li, Z. Zhang, \\nM. J. Franklin, A. Ghodsi, and M. I. Jordan. The missing \\npiece in complex analytics: Low latency, scalable \\nmodel management and serving with Velox. In CIDR \\n2015, Seventh Biennial Conference on Innovative Data \\nSystems Research, Asilomar, CA, USA, January 4-7, \\n2015, \\nOnline \\nProceedings, \\n2015. \\narxiv.org/abs/1409.3809. \\n[19] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and E. P. \\nXing. GeePS: Scalable deep learning on distributed \\nGPUs with a GPUspecialized parameter server. In \\nProceedings of the Eleventh European Conference on \\nComputer \\nSystems, \\nEuroSys \\n’16, \\n2016. \\nwww.pdl.cmu.edu/PDLFTP/CloudComputing/GeePS-\\ncui-eurosys16.pdf. \\n[20] A. Dai, C. Olah, and Q. V. Le. Document embedding \\nwith \\nparagraph \\nvectors. \\narXiv \\npreprint \\narXiv:1507.07998, 2015. arxiv.org/abs/1507.07998.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 15}, page_content='cui-eurosys16.pdf. \\n[20] A. Dai, C. Olah, and Q. V. Le. Document embedding \\nwith \\nparagraph \\nvectors. \\narXiv \\npreprint \\narXiv:1507.07998, 2015. arxiv.org/abs/1507.07998. \\n[21] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. \\nV. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. \\nYang, and A. Y. Ng. Large scale distributed deep \\nnetworks. In NIPS, 2012. Google Research PDF. \\n[22] J. Dean and S. Ghemawat. Mapreduce: Simplified data \\nprocessing on large clusters. In Proceedings of the 6th \\nConference on Symposium on Opearting Systems \\nDesign & Implementation - Volume 6, OSDI’04, \\nBerkeley, CA, USA, 2004. USENIX Association. \\nresearch.google.com/archive/mapreduceosdi04.pdf. \\n[23] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, \\nT. Mikolov, et al. DeVISE: A deep visualsemantic \\nembedding model. In Advances in Neural Information \\nProcessing \\nSystems, \\npages \\n2121–2129, \\n2013. \\nresearch.google.com/pubs/archive/41473.pdf. \\n[24] J. Gonzalez-Dominguez, I. Lopez-Moreno, P. J.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 15}, page_content='embedding model. In Advances in Neural Information \\nProcessing \\nSystems, \\npages \\n2121–2129, \\n2013. \\nresearch.google.com/pubs/archive/41473.pdf. \\n[24] J. Gonzalez-Dominguez, I. Lopez-Moreno, P. J. \\nMoreno, and J. Gonzalez-Rodriguez. Frame-by-frame \\nlanguage identification in short utterances using deep \\nneural networks. Neural Networks, 64:49–58, 2015. \\nresearch.google.com/en//pubs/archive/42929.pdf. \\n[25] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. \\nWarde-Farley, S. Ozair, A. C. Courville, and Y. Bengio. \\nGenerative adversarial nets. In Advances in Neural \\nInformation \\nProcessing \\nSystems \\n27: \\nAnnual \\nConference on Neural Information Processing Systems \\n2014, December 8-13 2014, Montreal, Quebec, \\nCanada, \\npages \\n2672– \\n2680, \\n2014. \\npapers.nips.cc/paper/5423-generativeadversarial-\\nnets. \\n[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual \\nlearning \\nfor \\nimage \\nrecognition. \\nCoRR, \\nabs/1512.03385, 2015. arxiv.org/abs/1512.03385.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 15}, page_content='nets. \\n[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual \\nlearning \\nfor \\nimage \\nrecognition. \\nCoRR, \\nabs/1512.03385, 2015. arxiv.org/abs/1512.03385. \\n[27] G. Heigold, V. Vanhoucke, A. Senior, P. Nguyen, M. \\nRanzato, M. Devin, and J. Dean. Multilingual acoustic'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 16}, page_content='17 \\nmodels using distributed deep neural networks. In \\nAcoustics, Speech and Signal Processing (ICASSP), \\n2013 IEEE International Conference on, pages 8619–\\n8623. \\nIEEE, \\n2013. \\nresearch.google.com/pubs/archive/40807.pdf. \\n[28] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. \\nJoseph, R. Katz, S. Shenker, and I. Stoica. Mesos: A \\nplatform for fine-grained resource sharing in the data \\ncenter. In Proceedings of the 8th USENIX Conference \\non Networked Systems Design and Implementation, \\nNSDI’11, pages 295–308, Berkeley, CA, USA, 2011. \\nUSENIX \\nAssociation. \\nwww.cs.berkeley.edu/˜alig/papers/mesos.pdf. \\n[29] G. E. Hinton. Learning distributed representations of \\nconcepts. In Proceedings of the Eighth Annual \\nConference of the Cognitive Science Society, pages 1–\\n12. \\nHillsdale, \\nNJ: \\nErlbaum, \\n1986. \\nwww.cogsci.ucsd.edu/˜ajyu/Teaching/Cogs202 \\nsp13/Readings/hinton86.pdf. \\n[30] G. E. Hinton, \\nL. Deng, D. Yu, \\nG. E. Dahl, \\nA. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 16}, page_content='NJ: \\nErlbaum, \\n1986. \\nwww.cogsci.ucsd.edu/˜ajyu/Teaching/Cogs202 \\nsp13/Readings/hinton86.pdf. \\n[30] G. E. Hinton, \\nL. Deng, D. Yu, \\nG. E. Dahl, \\nA. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, \\nP. Nguyen, T. N. Sainath, and B. Kingsbury. Deep \\nneural networks for acoustic modeling in speech \\nrecognition: The shared views of four research \\ngroups. IEEE Signal Process. Mag., 29(6):82– 97, \\n2012. \\nwww.cs.toronto.edu/˜gdahl/papers/ \\ndeepSpeechReviewSPM2012.pdf. \\n[31] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed. \\nZooKeeper: Wait-free coordination for internetscale \\nsystems. In Proceedings of the 2010 USENIX \\nConference on USENIX Annual Technical Conference, \\nUSENIXATC’10, pages 11–11, Berkeley, CA, USA, 2010. \\nUSENIX \\nAssociation. \\nwww.usenix.org/legacy/event/atc10/tech/full \\npapers/Hunt.pdf. \\n[32] S. Ioffe and C. Szegedy. Batch normalization: \\nAccelerating deep network training by reducing \\ninternal covariate shift. CoRR, abs/1502.03167, 2015. \\narxiv.org/abs/1502.03167.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 16}, page_content='papers/Hunt.pdf. \\n[32] S. Ioffe and C. Szegedy. Batch normalization: \\nAccelerating deep network training by reducing \\ninternal covariate shift. CoRR, abs/1502.03167, 2015. \\narxiv.org/abs/1502.03167. \\n[33] B. Jacob et al. gemmlowp: a small selfcontained low-\\nprecision \\nGEMM \\nlibrary, \\n2015. \\ngithub.com/google/gemmlowp. \\n[34] B. Jacob, G. Guennebaud, et al. Eigen library for linear \\nalgebra. eigen.tuxfamily.org. \\n[35] S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using \\nvery large target vocabulary for neural machine \\ntranslation. In Proceedings of the 53rd Annual Meeting \\nof the Association for Computational Linguistics and \\nthe 7th International Joint Conference on Natural \\nLanguage Processing (Volume 1: Long Papers), pages \\n1–10, Beijing, China, July 2015. Association for \\nComputational \\nLinguistics. \\nwww.aclweb.org/anthology/P15-1001. \\n[36] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. \\nGirshick, S. Guadarrama, and T. Darrell. Caffe: \\nConvolutional \\narchitecture \\nfor'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 16}, page_content='Linguistics. \\nwww.aclweb.org/anthology/P15-1001. \\n[36] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. \\nGirshick, S. Guadarrama, and T. Darrell. Caffe: \\nConvolutional \\narchitecture \\nfor \\nfast \\nfeature \\nembedding. In Proceedings of the ACM International \\nConference on Multimedia, pages 675–678. ACM, \\n2014. arxiv.org/pdf/1408.5093. \\n[37] M. I. Jordan. Serial order: A parallel distributed \\nprocessing approach. ICS report 8608, Institute for \\nCognitive \\nScience, \\nUCSD, \\nLa \\nJolla, \\n1986. \\ncseweb.ucsd.edu/˜gary/PAPERSUGGESTIONS/Jordan-\\nTR-8604.pdf. \\n[38] N. Jouppi. Google supercharges machine learning \\ntasks \\nwith \\nTPU \\ncustom \\nchip, \\n2016. \\ncloudplatform.googleblog.com/2016/05/Googlesupe\\nrcharges-machine-learning-tasks-with-\\ncustomchip.html. \\n[39] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer,´ and \\nY. Wu. Exploring the limits of language modeling. \\nCoRR, \\nabs/1602.02410, \\n2016. \\narxiv.org/abs/1602.02410. \\n[40] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 16}, page_content='Y. Wu. Exploring the limits of language modeling. \\nCoRR, \\nabs/1602.02410, \\n2016. \\narxiv.org/abs/1602.02410. \\n[40] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. \\nSukthankar, and L. Fei-Fei. Large-scale video \\nclassification with convolutional neural networks. In \\nComputer Vision and Pattern Recognition (CVPR), \\n2014 IEEE Conference on, pages 1725–1732. IEEE, \\n2014. research.google.com/pubs/archive/42455.pdf. \\n[41] A. Krizhevsky. One weird trick for parallelizing \\nconvolutional \\nneural \\nnetworks. \\narXiv \\npreprint \\narXiv:1404.5997, 2014. arxiv.org/abs/1404.5997. \\n[42] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet \\nclassification \\nwith \\ndeep \\nconvolutional \\nneural \\nnetworks. In Advances in Neural Information \\nProcessing \\nSystems, \\n2012. \\npapers.nips.cc/paper/4824imagenet-classification-\\nwith-deep-convolutionalneural-networks.pdf.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 17}, page_content='18 \\n[43] H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin. \\nExploring strategies for training deep neural networks. \\nJournal of Machine Learning Research, 10:1–40, Jan. \\n2009. \\ndeeplearning.cs.cmu.edu/pdfs/1111/jmlr10 \\nlarochelle.pdf. \\n[44] A. Lavin and S. Gray. Fast algorithms for convolutional \\nneural networks. CoRR, abs/1509.09308, 2015. \\narxiv.org/abs/1509.09308. \\n[45] Q. Le, M. Ranzato, R. Monga, M. Devin, G. Corrado, K. \\nChen, J. Dean, and A. Ng. Building highlevel features \\nusing large scale unsupervised learning. In ICML’2012, \\n2012. Google Research PDF. \\n[46] M. Li, D. G. Andersen, J. Park, A. J. Smola, A. Ahmed, \\nV. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su. Scaling \\ndistributed machine learning with the Parameter \\nServer. In 11th USENIX Symposium on Operating \\nSystems Design and Implementation (OSDI 14), pages \\n583–598, \\n2014. \\nwww.usenix.org/system/files/conference/osdi14/osd\\ni14paper-chilimbi.pdf. \\n[47] M. Li, T. Zhang, Y. Chen, and A. J. Smola.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 17}, page_content='Systems Design and Implementation (OSDI 14), pages \\n583–598, \\n2014. \\nwww.usenix.org/system/files/conference/osdi14/osd\\ni14paper-chilimbi.pdf. \\n[47] M. Li, T. Zhang, Y. Chen, and A. J. Smola. \\nEfficient mini-batch training for stochastic \\noptimization. In Proceedings of the 20th ACM \\nSIGKDD International Conference on Knowledge \\nDiscovery and Data Mining, KDD ’14, pages 661–\\n670, New York, NY, USA, 2014. ACM. \\nwww.cs.cmu.edu/˜muli/file/minibatch sgd.pdf. \\n[48] C. J. Maddison, A. Huang, I. Sutskever, and D. Silver. \\nMove evaluation in Go using deep convolutional \\nneural networks. arXiv preprint arXiv:1412.6564, \\n2014. arxiv.org/abs/1412.6564. \\n[49] F. McSherry, M. Isard, and D. G. Murray. Scalability! \\nBut at what COST? In Proceedings of the 15th USENIX \\nConference on Hot Topics in Operating Systems, \\nHOTOS’15, Berkeley, CA, USA, 2015. USENIX \\nAssociation. \\nwww.usenix.org/system/files/conference/hotos15/ \\nhotos15-paper-mcsherry.pdf.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 17}, page_content='Conference on Hot Topics in Operating Systems, \\nHOTOS’15, Berkeley, CA, USA, 2015. USENIX \\nAssociation. \\nwww.usenix.org/system/files/conference/hotos15/ \\nhotos15-paper-mcsherry.pdf. \\n[50] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient \\nestimation of word representations in vector space. In \\nInternational \\nConference \\non \\nLearning \\nRepresentations: \\nWorkshops \\nTrack, \\n2013. \\narxiv.org/abs/1301.3781. \\n[51] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. \\nVeness, M. G. Bellemare, A. Graves, M. Riedmiller, A. \\nK. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. \\nSadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, \\nS. Legg, and D. Hassabis. Human-level control through \\ndeep reinforcement learning. Nature, 518(7540):529–\\n533, 02 2015. dx.doi.org/10.1038/nature14236. \\n[52] P. Moritz, R. Nishihara, I. Stoica, and M. I. Jordan. \\nSparkNet: Training deep networks in Spark. In \\nInternational \\nConference \\non \\nLearning \\nRepresentations, 2016. arxiv.org/abs/1511.06051.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 17}, page_content='[52] P. Moritz, R. Nishihara, I. Stoica, and M. I. Jordan. \\nSparkNet: Training deep networks in Spark. In \\nInternational \\nConference \\non \\nLearning \\nRepresentations, 2016. arxiv.org/abs/1511.06051. \\n[53] Movidius Ltd. Movidius announces Deep Learning \\nAccelerator and Fathom software framework, 2016. \\nwww.movidius.com/news/movidius-announcesdeep-\\nlearning-accelerator-and-fathom-\\nsoftwareframework. \\n[54] D. G. Murray, F. McSherry, R. Isaacs, M. Isard, P. \\nBarham, and M. Abadi. Naiad: a timely dataflow \\nsystem. In Proceedings of the Twenty-Fourth ACM \\nSymposium on Operating Systems Principles, pages \\n439–455. ACM, 2013. Microsoft Research PDF. \\n[55] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. \\nFearon, A. De Maria, V. Panneershelvam, M. \\nSuleyman, C. Beattie, S. Petersen, et al. Massively \\nparallel methods for deep reinforcement learning. \\narXiv preprint arXiv:1507.04296, 2015. \\narxiv.org/abs/1507.04296. \\n[56] Nervana \\nSystems. neon, \\n2016. \\ngithub.com/NervanaSystems/neon.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 17}, page_content='parallel methods for deep reinforcement learning. \\narXiv preprint arXiv:1507.04296, 2015. \\narxiv.org/abs/1507.04296. \\n[56] Nervana \\nSystems. neon, \\n2016. \\ngithub.com/NervanaSystems/neon. \\n[57] NVIDIA Corporation. NCCL: Optimized primitives for \\ncollective \\nmulti-gpu \\ncommunication, \\n2016. \\ngithub.com/NVIDIA/nccl. \\n[58] K. Ovtcharov, O. Ruwase, J.-Y. Kim, J. Fowers, K. \\nStrauss, and E. Chung. Toward accelerating deep \\nlearning at scale using specialized logic. In Hot Chips: \\nA Symposium on High Performance Chips. HOTCHIPS, \\nAugust 2015. re- \\nsearch.microsoft.com/apps/pubs/default.aspx?id=246506. \\n[59] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty \\nof training recurrent neural networks. In ICML (3), \\nvolume 28 of JMLR'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 18}, page_content='19 \\nProceedings, pages 1310–1318. JMLR.org, 2013. \\nwww.jmlr.org/proceedings/papers/v28/pascanu13.p\\ndf. \\n[60] K. Powell. Nvidia \\ndevtech blog \\npost. \\nblogs.nvidia.com/blog/2015/03/17/digits-devbox/. \\n[61] J. Ragan-Kelley, C. Barnes, A. Adams, S. Paris, F. \\nDurand, and S. Amarasinghe. Halide: A language and \\ncompiler for optimizing parallelism, locality, and \\nrecomputation in image processing pipelines. ACM \\nSIGPLAN \\nNotices, \\n48(6):519– \\n530, \\n2013. \\npeople.csail.mit.edu/fredo/tmp/Halide5min.pdf. \\n[62] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-\\nfree approach to parallelizing stochastic gradient \\ndescent. In Advances in Neural Information Processing \\nSystems, \\npages \\n693–701, \\n2011. \\npapers.nips.cc/paper/4390-hogwild-a-lockfree-\\napproach-to-parallelizing-stochastic-gradientdescent. \\n[63] C. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. \\nFetterly. Dandelion: a compiler and runtime for \\nheterogeneous systems. In Proceedings of the Twenty-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 18}, page_content='[63] C. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. \\nFetterly. Dandelion: a compiler and runtime for \\nheterogeneous systems. In Proceedings of the Twenty-\\nFourth ACM Symposium on Operating Systems \\nPrinciples, pages 49–68. ACM, 2013. \\nresearch-\\nsrv.microsoft.com/pubs/201110/sosp13dandelion-\\nfinal.pdf. \\n[64] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. \\nLearning representations by backpropagating errors. \\nCognitive modeling, 5:3, 1988. \\nwww.cs.toronto.edu/ hinton/absps/naturebp.pdf. \\n[65] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, \\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, \\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual \\nRecognition Challenge. International Journal of \\nComputer Vision (IJCV), 115(3):211–252, 2015. \\narxiv.org/abs/1409.0575. \\n[66] A. Smola and S. Narayanamurthy. An architecture for \\nparallel topic models. Proc. \\nVLDB \\nEndow., 3(1-2):703–710, Sept. \\n2010. \\nvldb.org/pvldb/vldb2010/papers/R63.pdf.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 18}, page_content='arxiv.org/abs/1409.0575. \\n[66] A. Smola and S. Narayanamurthy. An architecture for \\nparallel topic models. Proc. \\nVLDB \\nEndow., 3(1-2):703–710, Sept. \\n2010. \\nvldb.org/pvldb/vldb2010/papers/R63.pdf. \\n[67] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. \\nOn the importance of initialization and momentum in \\ndeep learning. In Proceedings of the 30th International \\nConference on Machine Learning (ICML-13), pages \\n1139–1147. \\nJMLR \\nWorkshop \\nand \\nConference \\nProceedings, \\n2013. \\njmlr.org/proceedings/papers/v28/sutskever13.pdf. \\n[68] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to \\nsequence learning with neural networks. In NIPS, \\n2014. \\npapers.nips.cc/paper/5346-sequenceto-\\nsequence-learning-with-neural. \\n[69] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. \\nAnguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. \\nGoing deeper with convolutions. In CVPR’2015, 2015. \\narxiv.org/abs/1409.4842. \\n[70] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 18}, page_content='Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. \\nGoing deeper with convolutions. In CVPR’2015, 2015. \\narxiv.org/abs/1409.4842. \\n[70] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. \\nWojna. Rethinking the inception architecture for \\ncomputer vision. CoRR, abs/1512.00567, 2015. \\narxiv.org/abs/1512.00567. \\n[71] C. tao Chu, S. K. Kim, Y. an Lin, Y. Yu, G. Bradski, K. \\nOlukotun, and A. Y. Ng. Map-reduce for machine \\nlearning on multicore. In B. Scholkopf, J. C.¨ Platt, and \\nT. Hoffman, editors, Advances in Neural Information \\nProcessing Systems 19, pages 281–288. MIT Press, \\n2007. \\npapers.nips.cc/paper/3150-mapreduce-for-\\nmachine-learning-on-multicore.pdf. \\n[72] A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, \\nE. \\nTune, \\nand \\nJ. \\nWilkes. \\nLarge-scale \\ncluster \\nmanagement at Google with Borg. In Proceedings of \\nthe Tenth European Conference on Computer Systems, \\npage \\n18. \\nACM, \\n2015. \\nresearch.google.com/pubs/archive/43438.pdf.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 18}, page_content='Large-scale \\ncluster \\nmanagement at Google with Borg. In Proceedings of \\nthe Tenth European Conference on Computer Systems, \\npage \\n18. \\nACM, \\n2015. \\nresearch.google.com/pubs/archive/43438.pdf. \\n[73] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and \\nG. Hinton. Grammar as a foreign language. Technical \\nreport, \\narXiv:1412.7449, \\n2014. \\narxiv.org/abs/1412.7449. \\n[74] Y. Yu, M. Isard, D. Fetterly, M. Budiu, U. Erlingsson, P. \\nK. Gunda, and J. Currey. DryadLINQ: A system for \\ngeneral-purpose distributed dataparallel computing \\nusing a high-level language. In Proceedings of the 8th \\nUSENIX Conference on Operating Systems Design and \\nImplementation, OSDI’08, pages 1–14, Berkeley, CA, \\nUSA, \\n2008. \\nUSENIX \\nAssociation. \\nwww.usenix.org/legacy/event/osdi08/tech/full \\npapers/yu y/yu y.pdf. \\n[75] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. \\nMcCauley, M. J. Franklin, S. Shenker, and I. Stoica.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2022-09-29T18:02:02+05:00', 'source': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'file_path': '..\\\\data\\\\pdf\\\\RPaper.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'Nabeel Saad', 'subject': '', 'keywords': '', 'moddate': '2022-09-29T18:02:02+05:00', 'trapped': '', 'modDate': \"D:20220929180202+05'00'\", 'creationDate': \"D:20220929180202+05'00'\", 'page': 19}, page_content='20 \\nResilient \\ndistributed \\ndatasets: \\nA \\nfault-tolerant \\nabstraction for in-memory cluster computing. In \\nProceedings of the 9th USENIX conference on \\nNetworked Systems Design and Implementation. \\nUSENIX Association, 2012. \\nwww.usenix.org/system/files/conference/nsdi12/nsd\\ni12final138.pdf. \\n[76] M. D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, \\nQ. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean, and \\nG. E. Hinton. On rectified linear units for speech \\nprocessing. \\nIn \\nICASSP, \\n2013. \\nresearch.google.com/pubs/archive/40811.pdf. \\n[77] Dhaval Sahija, “Critical review of machine learning \\nintegration with augmented reality fordiscrete \\nmanufacturing” \\n2021, \\n10.2015/IJIRMF.2455.0620/202112017'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 0}, page_content='Scikit-Learn Made Easy: API Fast Guide\\nMohamad Yamen AL Mohamad\\nyamenmohamad@tabrizu.ac.ir\\nMay 29, 2025\\nAbstract\\nThis document provides a comprehensive reference guide to the core modules of scikit-learn,\\nthe premier machine learning library for Python. Each section introduces a module, explains\\nits purpose, and details key APIs with itemizes, use cases, and features. Visual diagrams\\nillustrate data flow and relationships between components, while practical code examples\\ndemonstrate real-world applications. The guide covers major functionalities including super-\\nvised learning (classification, regression), unsupervised learning (clustering, dimensionality\\nreduction), model selection, preprocessing, and pipeline construction. It is intended for data\\nscientists, machine learning practitioners, and developers who need a structured overview\\nof scikit-learn’s capabilities. By combining clear explanations with practical examples, this'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 0}, page_content='scientists, machine learning practitioners, and developers who need a structured overview\\nof scikit-learn’s capabilities. By combining clear explanations with practical examples, this\\ndocument helps users understand and implement effective machine learning workflows using\\nscikit-learn’s consistent API design. Keywords: Scikit-learn, Machine Learning, Python,\\nClassification, Regression, Clustering, Dimensionality Reduction, Model Selection, Prepro-\\ncessing, Pipelines, Feature Extraction, Cross-Validation, Hyperparameter Tuning, Super-\\nvised Learning, Unsupervised Learning\\n1'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 1}, page_content='1\\nOverview\\nScikit-learn [1, 2] is a comprehensive Python library for machine learning that provides simple\\nand eﬀicient tools for data mining and data analysis. Built on NumPy, SciPy, and matplotlib, it\\nfeatures various classification, regression, clustering algorithms, and includes utilities for model\\nevaluation, data preprocessing, and pipeline construction.\\nThis document serves as a structured reference to scikit-learn’s core modules:\\n• Clear definitions of each module’s purpose\\n• Detailed explanations of key APIs and functions\\n• Visual diagrams illustrating concepts and workflows\\n• Practical code examples demonstrating real-world usage\\nScikit-learn’s consistent API design enables practitioners to:\\n– Quickly implement and compare different algorithms\\n– Build end-to-end machine learning pipelines\\n– Evaluate models using robust validation techniques\\n– Preprocess data eﬀiciently\\n2\\nSupervised Learning'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 1}, page_content='– Build end-to-end machine learning pipelines\\n– Evaluate models using robust validation techniques\\n– Preprocess data eﬀiciently\\n2\\nSupervised Learning\\nDefinition 2.1. The sklearn package provides numerous algorithms for supervised learning,\\nwhere the goal is to predict target variables based on input features. These include both classi-\\nfication (predicting discrete labels) and regression (predicting continuous values) tasks.\\nLabeled Data\\nTrain/Test Split\\nClassifier/Regressor\\nEvaluation Metrics\\nFigure 1: Supervised Learning Workflow\\n2'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 2}, page_content='2.1\\nKey APIs\\n2.1.1\\nClassification\\n- LogisticRegression:\\n• Purpose: Linear model for binary and multiclass classification\\n• Use Case: When probabilities of class membership are needed\\n• Features: L1/L2 regularization, multiclass support\\n- SVC (Support Vector Classification):\\n• Purpose: Kernel-based classification\\n• Use Case: Complex decision boundaries in high-dimensional spaces\\n• Features: Multiple kernel options (linear, poly, rbf, sigmoid)\\n- RandomForestClassifier:\\n• Purpose: Ensemble of decision trees\\n• Use Case: Robust classification with feature importance\\n• Features: Handles missing values, parallel training\\nExample 2.1.\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import\\ntrain_test_split\\nfrom sklearn.metrics import accuracy_score\\n# Load dataset\\niris = load_iris()\\nX, y = iris.data, iris.target\\n# Split into train/test sets\\nX_train , X_test, y_train , y_test = train_test_split\\n(X, y, test_size=0.2)'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 2}, page_content='# Load dataset\\niris = load_iris()\\nX, y = iris.data, iris.target\\n# Split into train/test sets\\nX_train , X_test, y_train , y_test = train_test_split\\n(X, y, test_size=0.2)\\n# Train model\\nclf = RandomForestClassifier(n_estimators=100)\\nclf.fit(X_train , y_train)\\n# Evaluate\\ny_pred = clf.predict(X_test)\\nprint(f\"Accuracy: {accuracy_score(y_test , y_pred)\\n:.2f}\")\\n2.1.2\\nRegression\\n- LinearRegression:\\n• Purpose: Ordinary least squares regression\\n• Use Case: Linear relationships between features and target\\n• Features: Fast training, interpretable coeﬀicients\\n3'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 3}, page_content='- Ridge:\\n• Purpose: L2-regularized linear regression\\n• Use Case: When features are correlated\\n• Features: Reduces overfitting through regularization\\n- SVR (Support Vector Regression):\\n• Purpose: Kernel-based regression\\n• Use Case: Non-linear relationships\\n• Features: Robust to outliers with proper kernel choice\\nExample 2.2.\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.datasets import make_regression\\n# Generate synthetic regression data\\nX, y = make_regression(n_features=10, noise=0.1)\\n# Train model\\nridge = Ridge(alpha=1.0)\\nridge.fit(X, y)\\n# Predict and evaluate\\nscore = ridge.score(X, y)\\nprint(f\"R^2 Score: {score:.2f}\")\\n3\\nUnsupervised Learning\\nDefinition 3.1. The sklearn package provides algorithms for unsupervised learning tasks where\\nthe data has no labels. These include clustering (grouping similar data points) and dimension-\\nality reduction (reducing the number of features while preserving structure).\\nUnlabeled Data\\nPreprocessing\\nClustering/Dimensionality\\nReduction'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 3}, page_content='ality reduction (reducing the number of features while preserving structure).\\nUnlabeled Data\\nPreprocessing\\nClustering/Dimensionality\\nReduction\\nLabels/Reduced Features\\nFigure 2: Unsupervised Learning Workflow\\n4'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 4}, page_content='3.1\\nKey APIs\\n3.1.1\\nClustering\\n- KMeans:\\n• Purpose: Partition data into k clusters\\n• Use Case: When number of clusters is known\\n• Features: Scalable, simple interpretation\\n- DBSCAN:\\n• Purpose: Density-based clustering\\n• Use Case: Clusters of arbitrary shape\\n• Features: Handles noise, no need to specify cluster count\\n- AgglomerativeClustering:\\n• Purpose: Hierarchical clustering\\n• Use Case: When hierarchy is important\\n• Features: Dendrogram visualization possible\\nExample 3.1.\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.datasets import make_blobs\\nimport matplotlib.pyplot as plt\\n# Generate synthetic data\\nX, _ = make_blobs(n_samples=300, centers=4,\\nrandom_state=42)\\n# Cluster data\\nkmeans = KMeans(n_clusters=4)\\nkmeans.fit(X)\\nlabels = kmeans.labels_\\n# Visualize\\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\\'\\nviridis\\')\\nplt.title(\"KMeans Clustering\")\\nplt.show()\\n5'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 5}, page_content='3.1.2\\nDimensionality Reduction\\n- PCA (Principal Component Analysis):\\n• Purpose: Linear dimensionality reduction\\n• Use Case: Feature extraction, visualization\\n• Features: Preserves variance, orthogonal components\\n- t-SNE:\\n• Purpose: Non-linear dimensionality reduction\\n• Use Case: Visualization of high-dimensional data\\n• Features: Preserves local structure\\n- TruncatedSVD:\\n• Purpose: Dimensionality reduction for sparse matrices\\n• Use Case: Text data, recommendation systems\\n• Features: Works with sparse input\\nExample 3.2.\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.datasets import load_digits\\n# Load digit images\\ndigits = load_digits()\\nX = digits.data\\n# Apply PCA\\npca = PCA(n_components=2)\\nX_pca = pca.fit_transform(X)\\n# Plot\\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.\\ntarget, cmap=\\'tab10\\', alpha=0.6)\\nplt.title(\"Digits Projected onto First 2 Principal\\nComponents\")\\nplt.xlabel(\"PC1\")\\nplt.ylabel(\"PC2\")\\nplt.colorbar(label=\"Digit Class\")\\nplt.show()\\n6'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 6}, page_content='4\\nModel Selection\\nDefinition 4.1. The sklearn.model_selection module provides tools for evaluating models\\nand selecting hyperparameters, including cross-validation strategies and hyperparameter search\\nmethods.\\nData\\nCV Splitter\\nParameter Search\\nBest Model\\nFigure 3: Model Selection Workflow\\n4.1\\nKey APIs\\n- train_test_split:\\n• Purpose: Split data into train and test sets\\n• Use Case: Simple model evaluation\\n• Features: Stratification option for classification\\n- KFold:\\n• Purpose: K-fold cross-validation\\n• Use Case: Robust model evaluation\\n• Features: Shuffle option, stratification\\n- GridSearchCV:\\n• Purpose: Exhaustive hyperparameter search\\n• Use Case: When parameter space is small\\n• Features: Parallel computation, refitting\\n- RandomizedSearchCV:\\n• Purpose: Randomized hyperparameter search\\n• Use Case: Large parameter spaces\\n• Features: More eﬀicient than grid search\\n7'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 7}, page_content='Example 4.1.\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import SVC\\nfrom sklearn.datasets import load_iris\\n# Load data\\niris = load_iris()\\nX, y = iris.data, iris.target\\n# Define parameter grid\\nparam_grid = {\\'C\\': [0.1, 1, 10], \\'kernel\\': [\\'linear\\n\\', \\'rbf\\']}\\n# Perform grid search\\ngrid_search = GridSearchCV(SVC(), param_grid , cv=5)\\ngrid_search.fit(X, y)\\n# Output results\\nprint(f\"Best parameters: {grid_search.best_params_}\\n\")\\nprint(f\"Best score: {grid_search.best_score_:.2f}\")\\n8'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 8}, page_content='5\\nPreprocessing\\nDefinition 5.1. The sklearn.preprocessing module provides tools for transforming raw data\\ninto formats suitable for machine learning, including scaling, normalization, encoding categorical\\nvariables, and feature extraction.\\nRaw Data\\nScaler\\nEncoder\\nProcessed Data\\nFigure 4: Preprocessing Workflow\\n5.1\\nKey APIs\\n- StandardScaler:\\n• Purpose: Standardize features by removing mean and scaling to unit variance\\n• Use Case: When features have different scales\\n• Features: Preserves outliers\\n- MinMaxScaler:\\n• Purpose: Scale features to a given range (default [0, 1])\\n• Use Case: When bounded features are required\\n• Features: Sensitive to outliers\\n- OneHotEncoder:\\n• Purpose: Convert categorical features to binary indicators\\n• Use Case: When categories have no ordinal relationship\\n• Features: Handles unknown categories\\n- LabelEncoder:\\n• Purpose: Encode target labels with value between 0 and n_classes-1\\n• Use Case: Preparing classification targets\\n• Features: Simple transformation\\n9'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 9}, page_content=\"Example 5.1.\\nfrom sklearn.preprocessing import StandardScaler ,\\nOneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\nimport pandas as pd\\n# Create sample data\\ndata = pd.DataFrame({\\n'age': [25, 30, 35],\\n'income': [50000, 60000, 70000],\\n'gender': ['M', 'F', 'M']\\n})\\n# Define preprocessing\\npreprocessor = ColumnTransformer(\\ntransformers=[\\n('num', StandardScaler(), ['age', 'income']),\\n('cat', OneHotEncoder(), ['gender'])\\n])\\n# Apply transformations\\nprocessed = preprocessor.fit_transform(data)\\nprint(processed)\\n10\"),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 10}, page_content=\"6\\nPipelines\\nDefinition 6.1. The sklearn.pipeline module provides utilities to chain multiple processing\\nsteps together, ensuring proper data flow and preventing data leakage during cross-validation.\\nRaw Data\\nPreprocessing\\nModel\\nPredictions\\nFigure 5: Pipeline Workflow\\n6.1\\nKey APIs\\n- Pipeline:\\n• Purpose: Chain multiple estimators into one\\n• Use Case: Ensuring proper data flow\\n• Features: Single interface for all steps\\nExample 6.1.\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import\\ntrain_test_split\\nfrom sklearn.datasets import load_breast_cancer\\n# Load data\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train , X_test, y_train , y_test = train_test_split\\n(X, y, test_size=0.2)\\n# Create pipeline\\npipe = Pipeline([\\n('scaler', StandardScaler()),\\n('clf', LogisticRegression())\\n])\\n# Fit and evaluate\\npipe.fit(X_train , y_train)\"),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 10}, page_content='(X, y, test_size=0.2)\\n# Create pipeline\\npipe = Pipeline([\\n(\\'scaler\\', StandardScaler()),\\n(\\'clf\\', LogisticRegression())\\n])\\n# Fit and evaluate\\npipe.fit(X_train , y_train)\\nprint(f\"Test Accuracy: {pipe.score(X_test , y_test)\\n:.2f}\")\\n11'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-29T13:50:42+03:30', 'source': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciket-learn.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250529135042+03'30'\", 'page': 11}, page_content='References\\n[1] Scikit-learn Development Team. Scikit-learn Documentation. https://scikit-learn.org/\\nstable/, 2025.\\n[2] Scikit-learn\\nContributors.\\nScikit-learn\\nGitHub\\nRepository.\\nhttps://github.com/\\nscikit-learn/scikit-learn, 2025.\\n12'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 0}, page_content='seaborn: statistical data visualization\\nMichael L. Waskom1\\n1 Center for Neural Science, New York University\\nDOI: 10.21105/joss.03021\\nSoftware\\n• Review\\n• Repository\\n• Archive\\nEditor: Lorena Pantano\\nReviewers:\\n• @dangeles\\n• @Sara-ShiHo\\nSubmitted: 29 January 2021\\nPublished: 06 April 2021\\nLicense\\nAuthors of papers retain\\ncopyright and release the work\\nunder a Creative Commons\\nAttribution 4.0 International\\nLicense (CC BY 4.0).\\nSummary\\nseaborn is a library for making statistical graphics in Python. It provides a high-level interface\\nto matplotlib and integrates closely with pandas data structures. Functions in the seaborn\\nlibrary expose a declarative, dataset-oriented API that makes it easy to translate questions\\nabout data into graphics that can answer them. When given a dataset and a specification\\nof the plot to make, seaborn automatically maps the data values to visual attributes such\\nas color, size, or style, internally computes statistical transformations, and decorates the plot'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 0}, page_content='of the plot to make, seaborn automatically maps the data values to visual attributes such\\nas color, size, or style, internally computes statistical transformations, and decorates the plot\\nwith informative axis labels and a legend. Many seaborn functions can generate figures with\\nmultiple panels that elicit comparisons between conditional subsets of data or across different\\npairings of variables in a dataset. seaborn is designed to be useful throughout the lifecycle of\\na scientific project. By producing complete graphics from a single function call with minimal\\narguments, seaborn facilitates rapid prototyping and exploratory data analysis.\\nAnd by\\noffering extensive options for customization, along with exposing the underlying matplotlib\\nobjects, it can be used to create polished, publication-quality figures.\\nStatement of need\\nData visualization is an indispensable part of the scientific process. Effective visualizations'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 0}, page_content='objects, it can be used to create polished, publication-quality figures.\\nStatement of need\\nData visualization is an indispensable part of the scientific process. Effective visualizations\\nwill allow a scientist both to understand their own data and to communicate their insights to\\nothers (Tukey, 1977). These goals can be furthered by tools for specifying a graph that provide\\na good balance between efficiency and flexibility. Within the scientific Python ecosystem, the\\nmatplotlib (Hunter, 2007) project is very well established, having been under continuous\\ndevelopment for nearly two decades. It is highly flexible, offering fine-grained control over the\\nplacement and visual appearance of objects in a plot. It can be used interactively through GUI\\napplications, and it can output graphics to a wide range of static formats. Yet its relatively\\nlow-level API can make some common tasks cumbersome to perform. For example, creating'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 0}, page_content='applications, and it can output graphics to a wide range of static formats. Yet its relatively\\nlow-level API can make some common tasks cumbersome to perform. For example, creating\\na scatter plot where the marker size represents a numeric variable and the marker shape\\nrepresents a categorical variable requires one to transform the size values to graphical units\\nand to loop over the categorical levels, separately invoking a plotting function for each marker\\ntype.\\nThe seaborn library offers an interface to matplotlib that permits rapid data exploration\\nand prototyping of visualizations while retaining much of the flexibility and stability that are\\nnecessary to produce publication-quality graphics. It is domain-general and can be used to\\nvisualize a wide range of datasets that are well-represented within a tabular format.\\nExample\\nThe following example demonstrates the creation of a figure with seaborn. The example'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 0}, page_content='visualize a wide range of datasets that are well-represented within a tabular format.\\nExample\\nThe following example demonstrates the creation of a figure with seaborn. The example\\nmakes use of one of the built-in datasets that are provided for documentation and generation of\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n1'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 1}, page_content='reproducible bug reports. It illustrates several of the features described in the Overview section,\\nincluding the declarative API, semantic mappings, faceting across subplots, aggregation with\\nerror bars, and visual theme control.\\nimport seaborn as sns\\nsns.set_theme(context=\"paper\")\\nfmri = sns.load_dataset(\"fmri\")\\ng = sns.relplot(\\ndata=fmri, kind=\"line\",\\nx=\"timepoint\", y=\"signal\",\\nhue=\"event\", style=\"event\", col=\"region\",\\nheight=3.5, aspect=.8,\\n)\\ng.savefig(\"paper_demo.pdf\")\\n0\\n5\\n10\\n15\\ntimepoint\\n0.1\\n0.0\\n0.1\\n0.2\\n0.3\\nsignal\\nregion = parietal\\n0\\n5\\n10\\n15\\ntimepoint\\nregion = frontal\\nevent\\nstim\\ncue\\nFigure 1: An example seaborn figure demonstrating some of its key features.\\nThe image was\\ngenerated using seaborn v0.11.1.\\nOverview\\nUsers interface with seaborn through a collection of plotting functions that share a common\\nAPI for plot specification and offer many more specific options for customization.\\nThese'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 1}, page_content='Overview\\nUsers interface with seaborn through a collection of plotting functions that share a common\\nAPI for plot specification and offer many more specific options for customization.\\nThese\\nfunctions range from basic plot types such as scatter and line plots to functions that apply\\nvarious transformations and abstractions, such as histogram binning, kernel density estimation,\\nand regression model fitting. Functions in seaborn are classified as either “axes-level” or\\n“figure-level.” Axes-level functions behave like most plotting functions in the matplotlib.\\npyplot namespace. By default, they hook into the state machine that tracks a “current”\\nfigure and add a layer to it, but they can also accept a matplotlib axes object to control\\nwhere the plot is drawn, similar to using the matplotlib “object-oriented” interface. Figure-\\nlevel functions create their own figure when invoked, allowing them to “facet” the dataset'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 1}, page_content='where the plot is drawn, similar to using the matplotlib “object-oriented” interface. Figure-\\nlevel functions create their own figure when invoked, allowing them to “facet” the dataset\\nby creating multiple conditional subplots, along with adding conveniences such as putting\\nthe legend outside the space of the plot by default. Each figure-level function corresponds\\nto several axes-level functions that serve similar purposes, with a single parameter selecting\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n2'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 2}, page_content='the kind of plot to make. For example, the displot function can produce several different\\nrepresentations of a distribution, including a histogram, kernel density estimate, or empirical\\ncumulative distribution function. The figure-level functions make use of a seaborn class that\\ncontrols the layout of the figure, mediating between the axes-level functions and matplotlib.\\nThese classes are part of the public API and can be used directly for advanced applications.\\nOne of the key features in seaborn is that variables in a dataset can be automatically\\n“mapped” to visual attributes of the graph. These transformations are referred to as “se-\\nmantic” mappings because they endow the attributes with meaning vis a vis the dataset. By\\nfreeing the user from manually specifying the transformations – which often requires looping\\nand multiple function invocations when using matplotlib directly – seaborn allows rapid'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 2}, page_content='freeing the user from manually specifying the transformations – which often requires looping\\nand multiple function invocations when using matplotlib directly – seaborn allows rapid\\nexploration of multidimensional relationships. To further aid efficiency, the default parameters\\nof the mappings are opinionated. For example, when mapping the color of the elements in a\\nplot, seaborn infers whether to use a qualitative or quantitative mapping based on whether\\nthe input data are categorical or numeric. This behavior can be further configured or even\\noverridden by setting additional parameters of each plotting function.\\nSeveral seaborn functions also apply statistical transformations to the input data before\\nplotting, ranging from estimating the mean or median to fitting a general linear model. When\\ndata are transformed in this way, seaborn automatically computes and shows error bars to\\nprovide a visual cue about the uncertainty of the estimate. Unlike many graphical libraries,'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 2}, page_content='data are transformed in this way, seaborn automatically computes and shows error bars to\\nprovide a visual cue about the uncertainty of the estimate. Unlike many graphical libraries,\\nseaborn shows 95% confidence interval error bars by default, rather than standard errors. The\\nconfidence intervals are computed with a bootstrap algorithm, allowing them to generalize over\\nmany different statistics, and the default level allows the user to perform “inference by eye”\\n(Cumming & Finch, 2005). Historically, error bar specification has been relatively limited, but\\na forthcoming release (v0.12) will introduce a new configuration system that makes it possible\\nto show nonparametric percentile intervals and scaled analytic estimates of standard error or\\nstandard deviation statistics.\\nseaborn aims to be flexible about the format of its input data. The most convenient usage\\npattern provides a pandas (McKinney, 2010) dataframe with variables encoded in a long-'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 2}, page_content='seaborn aims to be flexible about the format of its input data. The most convenient usage\\npattern provides a pandas (McKinney, 2010) dataframe with variables encoded in a long-\\nform or “tidy” (Wickham, 2014) format. With this format, columns in the dataframe can\\nbe explicitly assigned to roles in the plot, such as specifying the x and y positions of a\\nscatterplot along with size and shape semantics. Long-form data supports efficient exploration\\nand prototyping because variables can be assigned different roles in the plot without modifying\\nanything about the original dataset.\\nBut most seaborn functions can also consume and\\nvisualize “wide-form” data, typically producing similar output to how the analogous matplot\\nlib function would interpret a 2D array (e.g., producing a boxplot where each box represents a\\ncolumn in the dataframe) while making use of the index and column names to label the graph.\\nUsing the label information in a pandas object can help make plots that are interpretable'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 2}, page_content='column in the dataframe) while making use of the index and column names to label the graph.\\nUsing the label information in a pandas object can help make plots that are interpretable\\nwithout further tweaking – reducing the chance of interpretive errors – but seaborn also\\naccepts data from a variety of more basic formats, including numpy (Harris et al., 2020) arrays\\nand simple Python collection types.\\nseaborn also offers multiple built-in themes that users can select to modify the visual appear-\\nance of their graphs. The themes make use of the matplotlib rcParams system, meaning\\nthat they will take effect for any figure created using matplotlib, not just those made by\\nseaborn. The themes are defined by two disjoint sets of parameters that separately control\\nthe style of the figure and the scaling of its elements (such as line widths and font sizes). This\\nseparation makes it easy to generate multiple versions of a figure that are scaled for different'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 2}, page_content='the style of the figure and the scaling of its elements (such as line widths and font sizes). This\\nseparation makes it easy to generate multiple versions of a figure that are scaled for different\\ncontexts, such as written reports and slide presentations. The theming system can also be\\nused to set a default color palette. As color is particularly important in data visualization and\\nno single set of defaults is universally appropriate, every plotting function makes it easy to\\nchoose an alternate categorical palette or continuous gradient mapping that is well-suited for\\nthe particular dataset and plot type. The seaborn documentation contains a tutorial on the\\nuse of color in data visualization to help users make this important decision.\\nseaborn does not aim to completely encapsulate or replace matplotlib.\\nMany useful\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n3'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 3}, page_content='graphs can be created through the seaborn interface, but more advanced applications –\\nsuch as defining composite figures with multiple arbitrary plot types – will require importing\\nand using matplotlib as well.\\nEven when calling only seaborn functions, deeper cus-\\ntomization of the plot appearance is achieved by specifying parameters that are passed-\\nthrough to the underlying matplotlib functions, and tweaks to the default axis limits,\\nticks, and labels are made by calling methods on the matplotlib object that axes-level\\nseaborn functions return.\\nThis approach is distinct from other statistical graphing sys-\\ntems, such as ggplot2 (Wickham, 2016).\\nWhile seaborn offers some similar features\\nand, in some cases, uses similar terminology to ggplot2, it does not implement the for-\\nmal Grammar of Graphics and cannot be used to produce arbitrary visualizations. Rather, its\\naim is to facilitate rapid exploration and prototyping through named functions and opinion-'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 3}, page_content='mal Grammar of Graphics and cannot be used to produce arbitrary visualizations. Rather, its\\naim is to facilitate rapid exploration and prototyping through named functions and opinion-\\nated defaults while allowing the user to leverage the considerable flexibility of matplotlib\\nto create more domain-specific graphics and to polish figures for publication.\\nAn exam-\\nple of a successful use of this approach to produce reproducible figures can be found at\\nhttps://github.com/WagnerLabPapers/Waskom_PNAS_2017 (Waskom & Wagner, 2017).\\nAcknowledgements\\nM.L.W. has been supported by the National Science Foundation IGERT program (0801700)\\nand by the Simons Foundation as a Junior Fellow in the Simons Society of Fellows (527794).\\nMany others have helped improve seaborn by asking questions, reporting bugs, and con-\\ntributing code; thank you to this community.\\nReferences\\nCumming, G., & Finch, S. (2005). Inference by eye: confidence intervals and how to read'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 3}, page_content='tributing code; thank you to this community.\\nReferences\\nCumming, G., & Finch, S. (2005). Inference by eye: confidence intervals and how to read\\npictures of data. The American Psychologist, 60(2), 170–180. https://doi.org/10.1037/\\n0003-066X.60.2.170\\nHarris, C. R., Millman, K. J., Walt, S. J. van der, Gommers, R., Virtanen, P., Cournapeau,\\nD., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., Kerkwijk,\\nM. H. van, Brett, M., Haldane, A., R’ıo, J. F. del, Wiebe, M., Peterson, P., … Oliphant,\\nT. E. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https:\\n//doi.org/10.1038/s41586-020-2649-2\\nHunter, J. D. (2007). Matplotlib: A 2D graphics environment. Computing in Science &\\nEngineering, 9(3), 90–95. https://doi.org/10.1109/MCSE.2007.55\\nMcKinney, W. (2010). Data structures for statistical computing in python. In S. van der Walt\\n& J. Millman (Eds.), Proceedings of the 9th Python in Science Conference (pp. 51–56).'),\n",
       " Document(metadata={'producer': 'XeTeX 0.99998', 'creator': 'LaTeX with hyperref package', 'creationdate': '2021-04-06T11:58:03+00:00', 'source': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'file_path': '..\\\\data\\\\pdf\\\\seaborn.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': 'seaborn: statistical data visualization', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20210406115803-00'00'\", 'page': 3}, page_content='McKinney, W. (2010). Data structures for statistical computing in python. In S. van der Walt\\n& J. Millman (Eds.), Proceedings of the 9th Python in Science Conference (pp. 51–56).\\nhttps://doi.org/10.25080/Majora-92bf1922-00a\\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley. ISBN: 978-0201076165\\nWaskom, M. L., & Wagner, A. D. (2017). Distributed representation of context by intrinsic\\nsubnetworks in prefrontal cortex. Proceedings of the National Academy of Sciences, 2030–\\n2035. https://doi.org/10.1073/pnas.1615269114\\nWickham, H. (2014). Tidy data.\\nJournal of Statistical Software, Articles, 59(10), 1–23.\\nhttps://doi.org/10.18637/jss.v059.i10\\nWickham, H. (2016).\\nggplot2:\\nElegant graphics for data analysis.\\nSpringer-Verlag.\\nISBN: 978-3-319-24277-4\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n4'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Jian Tao\\njtao@tamu.edu\\nSpring 2020 HPRC Short Course  \\n03/27/2020\\nIntroduction to Deep Learning \\nwith TensorFlow'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Schedule\\n●Part I. Deep Learning (70 mins)\\n●Break (10 mins)\\n●Part II. Intro to TensorFlow (70 mins)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='GitHub Repository for the Webinars\\nhttps://github.com/jtao/dswebinar'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Jupyter Notebook and JupyterLab\\nJupyter Notebook\\nJupyterLab'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Google Colaboratory'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Google Colaboratory\\nSearch GitHub user: jtao/dswebinar'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Part I. Deep Learning\\nDeep Learning\\nby Ian Goodfellow, Yoshua Bengio, and Aaron Courville\\nhttp://www.deeplearningbook.org/\\nAnimation of Neutron Networks\\nby Grant Sanderson\\nhttps://www.3blue1brown.com/'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Relationship of AI, ML, and DL\\nArtificial Intelligence\\n \\nMachine Learning\\n \\nDeep Learning\\n●Artificial Intelligence (AI)  \\nis anything about \\nman-made intelligence \\nexhibited by machines.\\n●Machine Learning (ML) is \\nan approach to achieve AI.\\n●Deep Learning (DL) is one \\ntechnique to implement \\nML.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Machine Learning\\nTraditional Modeling\\nMachine Learning (Supervised Learning)\\nSample \\nData\\nExpected \\nOutput\\nComputer\\nModel\\nData\\nScientific \\nModel\\nComputer\\nPrediction\\nModel\\nData\\nComputer\\nPrediction'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Types of ML Algorithms\\n●\\nSupervised Learning\\n○\\ntrained with labeled data; \\nincluding regression and \\nclassification problems\\n●\\nUnsupervised Learning\\n○\\ntrained with unlabeled data; \\nclustering and association rule \\nlearning problems.\\n●\\nReinforcement Learning\\n○\\nno training data; stochastic \\nMarkov decision process; robotics \\nand self-driving cars.\\nSupervised Learning\\nReinforcement Learning\\nUnsupervised Learning\\nMachine Learning'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Supervised Learning\\nWhen both input variables - X and output variables - Y are known, one can \\napproximate the mapping function from  X to Y.\\nTraining Data\\nML Algorithm\\nModel\\nTest Data\\nStep 1: Training\\nStep 2: Testing'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Unsupervised Learning\\nWhen only input variables - X are known and the training data is neither \\nclassified nor labeled. It is usually used for clustering problems.\\nData\\nClass 1\\nClass 2\\nClass 3'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Reinforcement Learning\\nWhen the input variables are only available via interacting with the \\nenvironment, reinforcement learning can be used to train an \"agent\".\\n(Image Credit: Wikipedia.org)\\n(Image Credit: deeplearning4j.org)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Why Deep Learning?\\n●Limitations of traditional machine learning algorithms\\n○not good at handling high dimensional data.\\n○difficult to do feature extraction and object recognition.\\n●Advantages of deep learning\\n○DL is computationally expensive, but it is capable of \\nhandling high dimensional data.\\n○feature extraction is done automatically.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='What is Deep Learning?\\nDeep learning is a class of machine learning algorithms that:\\n●use a cascade of multiple layers of nonlinear processing units \\nfor feature extraction and transformation. Each successive \\nlayer uses the output from the previous layer as input.\\n●learn in supervised (e.g., classification) and/or unsupervised \\n(e.g., pattern analysis) manners.\\n●learn multiple levels of representations that correspond to \\ndifferent levels of abstraction; the levels form a hierarchy of \\nconcepts.\\n(Source: Wikipedia)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Artificial Neural Network\\n(Image Credit: Wikipedia)\\nInput\\nOutput\\nHidden Layers'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Inputs and Outputs \\n256 X 256 \\nMatrix\\n4-Element Vector \\nDL model\\n1\\n2\\n3\\n4\\n5\\n6\\nA\\nC\\nT\\nG\\nM\\nF\\nWith deep learning, we are searching for a surjective \\n(or onto) function f from a set X to a set Y. \\nX\\nY'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='18 \\nLearning Principle\\nx\\n1\\nx\\n2\\nx\\nn\\n…..\\n-\\nError:\\nOutput/Prediction\\nTarget Output\\nDataset\\n= 5'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='19 \\nLearning Principle\\nx\\n1\\nx\\n2\\nx\\nn\\n…..\\n-\\nError:\\nOutput/Prediction\\nTarget Output\\n= 15'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='20 \\nLearning Principle\\nx\\n1\\nx\\n2\\nx\\nn\\n…..\\n-\\nError:\\nOutput/Prediction\\nTarget Output\\n= 2.5'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='Supervised Deep Learning with Neural Networks\\nX3\\nX2\\nX1\\nY3\\nInput\\nOutput\\nHidden Layers\\nW1\\nW2\\nW3\\nFrom one layer to the next\\nf is the activation function,\\nWi is the weight, and bi is \\nthe bias.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Training - Minimizing the Loss \\nX3\\nX2\\nX1\\nY2\\nInput\\nOutput\\nW3, b3\\nThe loss function with regard to weights \\nand biases can be defined as\\nW2, b2\\nW1, b1\\nL\\nThe weight update is computed by moving \\na step to the opposite direction of the cost \\ngradient. \\nIterate until L stops decreasing.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Convolution in 2D\\n(Image Credit: Applied Deep Learning | Arden Dertat)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='Convolution Kernel \\n(Image Credit: Applied Deep Learning | Arden Dertat)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='Convolution on Image\\nImage Credit: Deep Learning Methods for Vision | CVPR 2012 Tutorial'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Activation Functions\\nImage Credit: towardsdatascience.com'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 26}, page_content='Introducing Non Linearity (ReLU)\\nImage Credit: Deep Learning Methods for Vision | CVPR 2012 Tutorial'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 27}, page_content='Max Pooling \\n(Image Credit: Applied Deep Learning | Arden Dertat)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 28}, page_content='Pooling - Max-Pooling and Sum-Pooling\\nImage Credit: Deep Learning Methods for Vision | CVPR 2012 Tutorial'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 29}, page_content='CNN Implementation - Drop Out\\n(Image Credit: Applied Deep Learning | Arden Dertat)\\nDropout is used to prevent overfitting. A neuron is temporarily \\n“dropped” or disabled with probability P during training.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 30}, page_content='CNN Implementation - Data Augmentation (DA)\\n(Image Credit: Applied Deep Learning | Arden Dertat)\\nDA helps to popular  \\nartificial training \\ninstances from the \\nexisting train data sets.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 31}, page_content='Convolutional Neural Networks\\nA convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward \\nartificial neural networks that explicitly assumes that the inputs are images, which allows \\nus to encode certain properties into the architecture.\\n(Image Credit: https://becominghuman.ai)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 32}, page_content='Deep Learning for Facial Recognition \\n(Image Credit: www.edureka.co)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 33}, page_content='MNIST - Introduction\\n●\\nMNIST (Mixed National \\nInstitute of Standards and \\nTechnology) is a database for \\nhandwritten digits, distributed \\nby Yann Lecun.\\n●\\n60,000 examples, and a test \\nset of 10,000 examples.\\n●\\n28x28 pixels each.\\n●\\nWidely used for research and \\neducational purposes.\\n(Image Credit: Wikipedia)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 34}, page_content='MNIST - CNN Visualization\\n(Image Credit: http://scs.ryerson.ca/~aharley/vis/)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 35}, page_content='Part II. Introduction to TensorFlow\\n36\\nTensorFlow Official Website\\nhttp://www.tensorflow.org'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 36}, page_content='A Brief History of TensorFlow\\nTensorFlow is an end-to-end FOSS (free and open source software) \\nlibrary for dataflow, differentiable programming. TensorFlow is one of \\nthe most popular program frameworks for building machine learning \\napplications.\\n●\\nGoogle Brain built DistBelief in 2011 for internal usage.\\n●\\nTensorFlow 1.0.0 was released on Feb 11, 2017\\n●\\nTensorFlow 2.0 was released in Jan 2018.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 37}, page_content='TensorFlow, Keras, and PyTorch\\nKeras is a high-level \\nneural networks API, \\nwritten in Python and \\ncapable of running on \\ntop of TensorFlow, \\nCNTK, or Theano. It \\nwas developed with a \\nfocus on enabling fast \\nexperimentation.\\nTensorFlow is an \\nend-to-end open \\nsource platform for \\nmachine learning. It \\nhas a comprehensive, \\nflexible ecosystem to \\nbuild and deploy ML \\npowered applications.\\nPyTorch is an open \\nsource machine \\nlearning framework \\nthat accelerates the \\npath from research \\nprototyping to \\nproduction \\ndeployment.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 38}, page_content='Google Trends for Popular ML Frameworks\\n(Image Credit: https://trends.google.com/)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 39}, page_content='Programming Environment\\n(Image Credit: tensorflow.org)\\nIn TF 2.0, tf.keras is the \\nrecommended \\nhigh-level API.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 40}, page_content='(Image Credit: Plumber Game by Mobiloids)\\nA Connected Pipeline for the Flow of Tensors'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 41}, page_content='What is a Tensor in TensorFlow?\\n●\\nTensorFlow uses a tensor \\ndata structure to represent all \\ndata. A TensorFlow tensor as \\nan n-dimensional array or list. \\nA tensor has a static type, a \\nrank, and a shape.\\nName\\nRank\\nTensor\\nScalar\\n0\\n[5]\\nVector\\n1\\n[1 2 3]\\nMatrix\\n2\\n[[1 2 3 4],\\n[5 6 7 8]]\\nTensor\\n3\\n...'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 42}, page_content='TensorFlow Data Types \\nBasic TensorFlow data types include:\\n●\\nint[8|16|32|64], float[16|32|64], double\\n●\\nbool \\n●\\nstring\\nwith tf.cast(), the data types of variables \\ncould be converted.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 43}, page_content='Hello World with TensorFlow\\nimport tensorflow as tf\\nv = tf.constant(\"Hello World!\")\\ntf.print(v)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 44}, page_content='TensorFlow Constants\\nimport tensorflow as tf\\nx = tf.constant(1, tf.int32)\\nzeros = tf.zeros([2, 3], tf.int32)\\nones = tf.ones([2, 3], tf.int32)\\ny = x *(zeros + ones + ones)\\ntf.print(y)\\nTensorFlow provides several operations to generate constant tensor.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 45}, page_content='TensorFlow Variables\\nTensorFlow variables can represent shared, persistent state manipulated by \\nyour program. Weights and biases are usually stored in variables.\\nimport tensorflow as tf\\nW = tf.Variable(tf.random.normal([2,2], stddev=0.1), \\nname = \"W\")\\nb = tf.Variable(tf.zeros(shape=(2)), name=\"b\")'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 46}, page_content='Machine Learning Workflow with tf.keras\\nStep 1\\nPrepare Train Data\\nThe preprocessed data set needs \\nto be shuﬄed and splitted into \\ntraining and testing data.\\n  \\nStep 2\\nDeﬁne Model\\nA model could be deﬁned with \\ntf.keras Sequential model for a \\nlinear stack of layers or tf.keras \\nfunctional API for complex \\nnetwork.\\n  \\nStep 3\\nTraining Conﬁguration\\nThe conﬁguration of the training \\nprocess requires the \\nspeciﬁcation of an optimizer, a \\nloss function, and a list of \\nmetrics.\\n  \\nStep 4\\nTrain Model\\nThe training begins by calling the \\nﬁt function. The number of \\nepochs and batch size need to be \\nset. The measurement metrics \\nneed to be evaluated.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 47}, page_content='tf.keras Built-in Datasets\\n●\\ntf.keras provides many popular reference datasets that could be used \\nfor demonstrating and testing deep neural network models. To name a \\nfew,\\n○\\nBoston Housing (regression)\\n○\\nCIFAR100 (classification of 100 image labels)\\n○\\nMNIST (classification of 10 digits)\\n○\\nFashion-MNIST (classification of 10 fashion categories)\\n○\\nReuters News (multiclass text classification)\\n●\\nThe built-in datasets could be easily read in for training purpose. E.g.,\\nfrom tensorflow.keras.datasets import boston_housing\\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 48}, page_content='Prepare Datasets for tf.keras\\nIn order to train a deep neural network model with \\nKeras, the input data sets needs to be cleaned, \\nbalanced, transformed, scaled, and splitted.\\n●\\nBalance the classes. Unbalanced classes will \\ninterfere with training.\\n●\\nTransform the categorical variables into \\none-hot encoded variables. \\n●\\nExtract the X (variables) and y (targets) values \\nfor the training and testing datasets.\\n●\\nScale/normalize the variables.\\n●\\nShuffle and split the dataset into training and \\ntesting datasets\\nDog\\nCat\\nHorse\\n1\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\nDog\\nCat\\nHorse\\n1\\n2\\n3\\nOne-hot encoding\\nNumerical encoding'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 49}, page_content=\"Create a tf.keras Model\\n●\\nLayers are the fundamental \\nbuilding blocks of tf.keras \\nmodels. \\n●\\nThe Sequential model is a \\nlinear stack of layers.\\n●\\nA Sequential model can be \\ncreated with a list of layer \\ninstances to the constructor or \\nadded with the .add() method.\\n●\\nThe input shape/dimension of \\nthe first layer need to be set.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, \\nActivation\\nmodel = Sequential([\\n    Dense(64, activation='relu', input_dim=20),\\n    Dense(10, activation='softmax')\\n])\\nInput\\nOutput\\nHidden Layers\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 50}, page_content='Compile a tf.keras Model\\nThe compile method of a Keras model configures the learning \\nprocess before the model is trained. The following 3 arguments need \\nto be set (the optimizer and loss function are required).\\n●\\nAn optimizer: Adam, AdaGrad, SGD, RMSprop, etc.\\n●\\nA loss function: mean_squared_error, mean_absolute_error, \\nmean_squared_logarithmic_error, categorical_crossentropy, \\nkullback_leibler_divergence, etc.\\n●\\nA list of measurement metrics: accuracy, binary_accuracy, \\ncategorical_accuracy, etc.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 51}, page_content='Train and Evaluate a tf.keras Model\\nModel: \"sequential_1\"\\n_______________________________________________\\nLayer (type)                 Output Shape              Param #   \\n=============================================\\ndense_11 (Dense)             (None, 64)                1344      \\n_______________________________________________\\ndense_12 (Dense)             (None, 10)                650       \\n=============================================\\nTotal params: 1,994\\nTrainable params: 1,994\\nNon-trainable params: 0\\n_______________________________________________\\nNone\\ntf.keras is trained on NumPy arrays of input \\ndata and labels. The training is done with the \\n●\\nfit() function of the model class. In the fit \\nfunction, the following two \\nhyperparameters can be set:\\n○\\nnumber of epochs\\n○\\nbatch size\\n●\\nevaluate() function returns the loss value \\n& metrics values for the model in test \\nmode.\\n●\\nsummary() function prints out the \\nnetwork architecture.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 52}, page_content='Make Predictions and More\\nAfter the model is trained, \\n●\\npredict() function of the model class could be used to \\ngenerate output predictions for the input samples.\\n●\\nget_weights() function returns a list of all weight tensors in \\nthe model, as Numpy arrays.\\n●\\nto_json() returns a representation of the model as a JSON \\nstring. Note that the representation does not include the \\nweights, only the architecture. \\n●\\nsave_weights(filepath) saves the weights of the model as a \\nHDF5 file.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 53}, page_content='Hands-on Session #1\\nGetting Started with TensorFlow'),\n",
       " Document(metadata={'producer': '', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'file_path': '..\\\\data\\\\pdf\\\\tensorflow.pdf', 'total_pages': 55, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 54}, page_content='Hands-on Session #2\\nClassify Handwritten Digits with \\nTensorFlow'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 0}, page_content='Natural Language to Python Source Code using \\nTransformers  \\n \\nMeet Shah \\nInformation Technology \\nSardar Patel Institute of Technology \\nMumbai, India \\nmeet8june@gmail.com \\n \\nRajat Shenoy \\nInformation Technology \\nSardar Patel Institute of Technology \\nMumbai, India \\nrajatshenoy@gmail.com \\n \\nRadha Shankarmani \\nInformation Technology \\nSardar Patel Institute of Technology \\nMumbai, India \\nradha_shankarmani@spit.ac.in\\nAbstract—Writing code using natural language is a very exciting \\napplication of Neural Machine Translation. To achieve a small \\npart of such an application, in this paper, we try to generate python \\nsource code snippets from natural English language descriptions \\nusing the Django dataset. We trained the self-attention based \\ntransformer architecture on the snippets from the dataset. We \\nachieved a BLEU score of 64.29 \\nKeywords—Transformers, tokenizers, Django, English, Python \\n \\nI. INTRODUCTION \\nProgramming is a vast field of engineering. More and more'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 0}, page_content='achieved a BLEU score of 64.29 \\nKeywords—Transformers, tokenizers, Django, English, Python \\n \\nI. INTRODUCTION \\nProgramming is a vast field of engineering. More and more \\npeople are joining this profession and the skill is in high \\ndemand. But most of the programmers face this problem that \\nthe field is too fast and there is too much adaptation required \\nif we want to stay in this field for a long time. Writing source \\ncode in different languages is very tough for programmers. \\nThey need to first refer to the documentation of those \\nprogramming languages, understand the syntax and then \\nwrite the source code for their program. \\nWe strongly believe that programming should be based on \\nthe application of logic and problem-solving skills more as \\ncompared to having the knowledge of a particular \\nprogramming language. That is why we propose this solution \\nwhere we translate natural language (English) to a \\nprogramming \\nlanguage \\n(Python). \\nThis \\nwill \\nallow'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 0}, page_content='programming language. That is why we propose this solution \\nwhere we translate natural language (English) to a \\nprogramming \\nlanguage \\n(Python). \\nThis \\nwill \\nallow \\nprogrammers to write programs in natural language and not \\nworry about the syntax in various languages and the silly \\nsyntax errors that keep on occurring all the time.  \\nSemantic parsing is the problem of converting natural \\nlanguage constructs into logical constructs. One of the \\napplications of semantic parsing is machine translation. \\nMachine Translation uses software to convert one language \\nto another language. We are working on a small part of this \\nproblem by converting natural English language constructs \\ninto python programming language syntax. We will be using \\nthe Django dataset for this task. \\nIn the future, maybe when this proposed system is developed \\nenough, even non-programmers will be able to write \\nprograms with the help of just natural language if they'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 0}, page_content='In the future, maybe when this proposed system is developed \\nenough, even non-programmers will be able to write \\nprograms with the help of just natural language if they \\nunderstand the logic to solve the problem. This paper is in the \\nprimitive stages of the research topic and is subject to further \\nimprovement and research. \\n \\nII. LITERATURE REVIEW \\nMany papers have been published that follows a deep \\nlearning approach for natural language to source code \\nconversion. [1] and [2] use attention-based encoder-decoder \\narchitectures. They use one or more than one LSTM layers \\nwith appropriate attention mechanisms. \\nEncoder/decoder based transformers introduced in [3] are \\nnow replacing LSTM based architectures. Transformer based \\narchitectures \\nare \\nnow \\noutperforming \\nLSTM \\nbased \\narchitectures. State-of-the-art performance has been achieved \\nin machine translation tasks[4], language modelling tasks, \\nclassification tasks [5],  grammar correction tasks [6], \\nsummarization \\ntasks[7],'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 0}, page_content='in machine translation tasks[4], language modelling tasks, \\nclassification tasks [5],  grammar correction tasks [6], \\nsummarization \\ntasks[7], \\nentity \\nrecognition, \\ndialogue \\ngeneration tasks [8], and other NLP tasks as well using the \\nTransformer architecture. \\n \\nProgramming languages are of course not natural but many \\nNLP approaches are being applied to them. The programming \\nlanguages have grammar and syntax but with relatively lesser \\nvocabulary than natural languages. There also are \\nrelationships between the vocabulary. Therefore there are \\nmany opportunities to model these languages using NLP \\nmodels and tasks.  \\n \\nSuch NLP tasks applied to programming languages definitely \\nhave some uses in software development. Examples of which \\ninclude summarization/ translation of source code to generate \\ndocumentation or summaries[9][10], using natural language \\nquery for code search [11][12], patching  and detecting bugs \\nusing translation and error correction[13], code completion'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 0}, page_content=\"documentation or summaries[9][10], using natural language \\nquery for code search [11][12], patching  and detecting bugs \\nusing translation and error correction[13], code completion \\nusing language modelling or generation[14][15] \\n \\nWe will be using the Django Dataset [16] in this paper. The \\nDjango Dataset provides a dataset for annotated code \\nsnippets. For example, the dataset will contain the natural \\nlanguage intent ‘call the params.get method with string ' \\nKEY_PREFIX ' and an empty string as arguments, substitute \\nthe result for self._key_prefix.’ mapped to the python code \\n‘self . key_prefix = params . get ( 'KEY_PREFIX' , '' )’. The \\nDjango dataset contains 18805 such pairs of natural language \\nintents mapped to python code. We explore how \\nencoder/decoder based transformer architecture performs on \\nthese datasets.\"),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 1}, page_content='III. METHODOLOGY \\n \\nThe main methodology behind translating natural language to \\ncode is machine translation. The basic idea is that a \\nprogramming language is just like a normal natural language \\nwith much less vocabulary. \\n \\nThere exist mature solutions to translate one natural language \\nto another. Some products like Google Translate do it really \\nwell. If programming languages are treated like natural \\nlanguages and the machine translation solutions are applied \\nto translate from natural language to programming language, \\nwe will reach a good solution at some point in the future. \\n \\nSo for machine translation, the solutions are recurrent neural \\nnetworks (LSTMs) and Transformers. Most of the literature \\nwe reviewed used RNNs as a solution. But transformers are \\nfaster and more robust as compared to LSTMs and hence we \\nchose to use transformers as a solution. \\n \\nThe transformers cannot process English or python directly.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 1}, page_content='faster and more robust as compared to LSTMs and hence we \\nchose to use transformers as a solution. \\n \\nThe transformers cannot process English or python directly. \\nThey need to be converted to numerical form to be able to use \\nthem. That is where tokenizers come in handy and we have \\nbuilt our custom tokenizer based on BertTokenizer and we \\nhave built our custom transformer which inherits from the \\nTensorFlow model. The vocabulary generated from the \\ntokenizer is used to train the transformer. \\n \\nIV. IMPLEMENTATION \\n \\nA. User Interface \\n \\nThe implementation consists of 3 parts. The first one is the \\nuser interface. It is built in React. React is a pretty popular \\njavascript library used to build AJAX based frontend of a web \\napp. The frontend consists of 3 user input types. The user can \\ngive the input via a file in which they can type in multiple \\nlines of natural language intents. They can also type single \\nline text intents. They can give audio inputs by speaking in'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 1}, page_content='give the input via a file in which they can type in multiple \\nlines of natural language intents. They can also type single \\nline text intents. They can give audio inputs by speaking in \\nEnglish .They will get a corresponding python snippet as an \\noutput. We have also included a Code-editor for the user, \\nwhere a person can edit the predicted code as well and send \\nthe corrected code to us that we can see in Fig. 1 \\nFig. 1 – UI of our implementation \\nB. Server \\n \\nThe second part is the server side part. This part is made in \\nDjango. Django is a versatile server side python framework \\nthat has many tools for the convenience of web developers. \\nDjango consists of the basic routing of the web app and that \\nis where the saved ML model is used to translate the English \\ninput received from the client side to python and send it back \\nto the client. \\n \\nC. Model \\n \\n1) The Dataset \\n \\nThe third and the main part of the project is the model itself'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 1}, page_content='input received from the client side to python and send it back \\nto the client. \\n \\nC. Model \\n \\n1) The Dataset \\n \\nThe third and the main part of the project is the model itself \\nthat does all the work behind the scenes. The dataset used is \\nthe ase15-django-dataset [16]. This dataset consists of 18805 \\nrecords of English and corresponding python snippets. The \\ndata is distributed across code snippets of various libraries \\nlike Django, celery, JSON, etc. show in Fig. 2 \\n \\nFig. 2 – The Django Dataset [16] \\n \\n2)  Pre-processing \\n \\nThis dataset is split into train and test with train dataset \\nconsisting of 15000 records and test dataset consisting of the \\nremaining 3805 records. Some preprocessing is done on the \\ndata first and then it is converted into the TensorFlow dataset \\nformat for better processing with the TensorFlow library. \\nTensorflow is an open-source library in python by Google \\nand it is used for ML and DL operations. It has a vast number'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 1}, page_content='format for better processing with the TensorFlow library. \\nTensorflow is an open-source library in python by Google \\nand it is used for ML and DL operations. It has a vast number \\nof libraries built into it and is popular due to its versatility. \\nThe model training is done in a Google Colab GPU \\nenvironment. The TensorFlow version was 2.4.1.  \\n \\n3)    Tokenizer \\n \\nThen the bert_vocab_from_dataset library was loaded from \\ntensorflow_text for the purpose of vocabulary generation \\nfrom English and python. The vocabulary size for the bert \\nvocab was set to 4000. The vocabulary for python and \\nEnglish was created and stored in a text file. Two Bert \\ntokenizer models were trained using these txt files. One \\ncorresponds to English and the other corresponds to python.  \\n \\nA tokenizer is a tool that converts string input from the \\nvocabulary into a numerical format. Tokenization is \\nperformed on the input before feeding it to the transformer'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 1}, page_content='A tokenizer is a tool that converts string input from the \\nvocabulary into a numerical format. Tokenization is \\nperformed on the input before feeding it to the transformer \\nmodel so that the model can understand the input. Also, the \\nnumerical output received from the transformer model is \\nconverted to a string or a human-readable format by'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 2}, page_content='detokenizing using the tokenizer. Then a custom tokenizer is \\nbuilt and saved using TensorFlow’s save model function.  \\n \\n4)   Transformer \\nFig. 3: Transformer Architecture [3] \\n \\nThe tokenized pairs of English and python are divided into \\nbatches with a batch size of 64 and buffer size of 20000. Then \\npositional encoding is done. RNNs inherently know the \\nposition of a word in a sentence because it is a sequential \\nmodel. A single word input is accepted sequentially by RNN. \\nWhereas in transformers, the model itself does not have any \\nidea about the position of the word in the sentence because of \\nthe inherent nature of taking and processing all the words \\nsimultaneously. Hence, we need to add an extra piece of \\ninformation along with the word which contains details about \\nthe position of the word. This extra piece of information is \\nknown as positional encoding. Then we create a custom \\ntransformer model. \\n \\nThis transformer [3] consists of an encoder, a decoder and a'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 2}, page_content='known as positional encoding. Then we create a custom \\ntransformer model. \\n \\nThis transformer [3] consists of an encoder, a decoder and a \\nfinal linear layer. The encoder first performs input \\nembedding, then does positional encoding and then sends the \\ndata through multiple encoding layers. An encoding layer \\nconsists of a multi-head attention sublayer and a pointwise \\nfeed-forward network sublayer. The multi-head attention \\noperation splits a linear layer into heads, then performs scaled \\ndot-product attention, and then concatenates the heads into a \\nfinal linear layer. The decoder performs an output \\nembedding, then it performs positional encoding and then the \\ndata goes through multiple decoder layers. A decoder layer \\nfirst performs masked multi-head attention, then performs \\nmulti-head attention with padding mask and then the data \\ngoes through  pointwise feed-forward networks. We can see \\nthis architecture in Fig. 3. \\n \\nWe then set our hyper parameters. We have used the Adam'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 2}, page_content='goes through  pointwise feed-forward networks. We can see \\nthis architecture in Fig. 3. \\n \\nWe then set our hyper parameters. We have used the Adam \\nOptimizer with a customized learning rate scheduler in \\naccordance with the formula described in [3].  \\n \\nFig.4: Learning Rate Schedule \\n \\nWe then defined our loss function and our accuracy metrics. \\nWe used checkpointing while training our model. We had got \\naccess to a single Testa T4 GPU for training. We then train \\nthe model, running 50 epochs on the training dataset. After \\nthis, we evaluate the model, save the weights to use them in \\nthe server. \\n \\nV. RESULT \\nFig.5: Accuracy and Loss Metrics \\n \\nWe trained the transformer on 15000 pairs of intents and \\nsnippets. We obtained accuracy of 0.973 and a loss of about \\n0.0867 on the 50th epoch with each epoch taking on average  \\n \\nWe also calculated the BLEU Score of our model. To \\nevaluate the quality of machine translated natural language \\nfrom another language BLEU (bilingual evaluation'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 2}, page_content='We also calculated the BLEU Score of our model. To \\nevaluate the quality of machine translated natural language \\nfrom another language BLEU (bilingual evaluation \\nunderstudy) is used. It used to determine how close is the \\nmachine translated text compared to the professional human \\ntranslation. It is one of the most popular and inexpensive \\nmetrics. We have shown some of our BLEU scores and our \\naverage BLEU score in Fig. 6'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 3}, page_content='Fig.6: Our Average BLEU Score \\n \\nWe also tried giving new test cases to the model, some were \\nsuccessfully translated, some were not. The model struggles \\nwith variable names but fairly accurately predicts the syntax \\nof Python programming language. We can see in figure 7 \\nthat the model gave the correct prediction for the English \\nintent. But in figure 8 we can see that the model understood \\nthe syntax of classes but struggles with the variables a bit. \\n \\nFig.7: Correct prediction \\n \\nFig.8: Incorrect prediction \\nVI. CONCLUSION \\n \\nIn this paper, we presented a way to translate natural language \\n(English) to a programming language (python). We aimed to \\nsolve the “programming language barrier” in programming. \\nWe used the transformer model on the Django Dataset and \\nachieved a BLEU Score of 64.29. Our model fairly \\nunderstands the syntax of the Python Programming Language \\nbut struggles with variable names. \\n \\nThis topic is in primitive stages of research and needs more'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 3}, page_content='understands the syntax of the Python Programming Language \\nbut struggles with variable names. \\n \\nThis topic is in primitive stages of research and needs more \\nimprovement. In future when this technology is mature \\nenough, any person who does not know a programming \\nlanguage, will be able to write programs in that programming \\nlanguage.  \\n \\nVII. FUTURE SCOPE \\n \\nThe prototype we presented in this paper can be converted \\ninto a full system by adding functionalities like login, voice \\nauthentication, history of programs written, etc. Also, \\ncreating an extension for all major IDEs and text editors \\nwould be a great way for programmers to adapt to this \\ntechnology. By this, programmers will be able to easily write \\ncode and edit it if required in their favourite editor. The model \\ncan be improved by training the model on a dataset with more \\nnumber of records. Also experimentation can be done by \\nusing different tokenizers to improve the model even further. \\n \\n REFERENCES'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 3}, page_content='can be improved by training the model on a dataset with more \\nnumber of records. Also experimentation can be done by \\nusing different tokenizers to improve the model even further. \\n \\n REFERENCES \\n \\n[1] Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model \\nfor general-purpose code generation. arXiv preprint arXiv:1704.01696  \\n[2] Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract \\nsyntax networks for code generation and semantic parsing. arXiv \\npreprint arXiv:1704.07535 .  \\n[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion \\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. \\nAttention is all you need. In Advances in Neural Information \\nProcessing Systems. pages 5998–6008. \\n[4] \\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan \\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. \\nExploring the limits of transfer learning with a unified text-to-text \\ntransformer. arXiv preprint arXiv:1910.10683'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 3}, page_content='Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. \\nExploring the limits of transfer learning with a unified text-to-text \\ntransformer. arXiv preprint arXiv:1910.10683 \\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. \\n2018. Bert: Pre-training of deep bidirectional transformers for language \\nunderstanding. arXiv preprint arXiv:1810.04805 \\n[6] \\nChristopher Bryant, Mariano Felice, and Edward Briscoe. 2017. \\nAutomatic annotation and evaluation of error types for grammatical \\nerror correction. Association for Computational Linguistics. \\n[7] Yang Liu and Mirella Lapata. 2019. Text summarization with pre \\ntrained encoders. arXiv preprint arXiv:1908.08345 \\n[8] Paweł Budzianowski and Ivan Vulic. 2019. Hello, ´ it’s gpt-2–how can \\ni help you? towards the use of pre trained language models for task \\noriented dialogue systems. arXiv preprint arXiv:1907.05774.  \\n[9] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq:'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 3}, page_content='oriented dialogue systems. arXiv preprint arXiv:1907.05774.  \\n[9] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: \\nGenerating sequences from structured representations of code. arXiv \\npreprint arXiv:1808.01400.  \\n[10] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian \\nWu, and Philip S Yu. 2018. Improving automatic source code \\nsummarization via deep reinforcement learning. In Proceedings of the \\n33rd ACM/IEEE International Conference on Automated Software \\nEngineering, pages 397–407.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 4}, page_content='[11] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code \\nsearch. In Proceedings of the 40th International Conference on \\nSoftware Engineering, ICSE ’18, page 933–944, New York, NY, USA. \\nAssociation for Computing Machinery. \\n[12] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and \\nMarc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the \\nstate of semantic code search. arXiv preprint arXiv:1909.09436.  \\n[13] Juan Zhai, Xiangzhe Xu, Yu Shi, Minxue Pan, Shiqing Ma, Lei Xu, \\nWeifeng Zhang, Lin Tan, and Xiangyu Zhang. 2019. Cpc: \\nautomatically classifying and propagating natural language comments \\nvia program analysis. \\n[14] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. \\n2019. Pythia: Ai-assisted code completion system. In Proceedings of \\nthe 25th ACM SIGKDD International Conference on Knowledge \\nDiscovery & Data Mining, pages 2727–2735.  \\n[15] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2021-04-03T21:22:46+05:30', 'source': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Transformers.pdf', 'total_pages': 5, 'format': 'PDF 1.7', 'title': 'Microsoft Word - MP2_ResearchPaper.docx', 'author': 'Meet Shah', 'subject': '', 'keywords': '', 'moddate': '2021-04-03T21:22:46+05:30', 'trapped': '', 'modDate': \"D:20210403212246+05'30'\", 'creationDate': \"D:20210403212246+05'30'\", 'page': 4}, page_content='the 25th ACM SIGKDD International Conference on Knowledge \\nDiscovery & Data Mining, pages 2727–2735.  \\n[15] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel \\nSundaresan. 2020. Intellicode compose: Code generation using \\ntransformers. arXiv preprint arXiv:2005.08025.  \\n[16] Y. Oda et al., \"Learning to Generate Pseudo-Code from Source Code \\nUsing Statistical Machine Translation,\" 2015 30th IEEE/ACM \\nInternational Conference on Automated Software Engineering (ASE), \\nLincoln, NE, USA, 2015, pp. 574-584, doi: 10.1/109/ASE.2015.36. \\n.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = text_splitting(datas)\n",
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25fb5f",
   "metadata": {},
   "source": [
    "### Embedding Manager \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074a130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_manager:\n",
    "    def __init__(self, model_name:str=\"BAAI/bge-small-en-v1.5\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name_or_path=self.model_name)\n",
    "            print(f\"{self.model_name} loaded from Hugging_face\")\n",
    "        except Exception as e:\n",
    "            raise f\"Model {self.model_name} not Loaded: error - {e}\"\n",
    "    def generate(self, text:List[str])->np.ndarray:\n",
    "        if not self.model:\n",
    "            raise \"Model Not loaded\"\n",
    "        embeddings = self.model.encode(text)\n",
    "        print(f\"Embeddings {len(embeddings)} generated\")\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160da7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAAI/bge-small-en-v1.5 loaded from Hugging_face\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Embedding_manager at 0x217dae6bf50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_manager = Embedding_manager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd01bee",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8943d2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector_store initialized pdf_info\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorDB at 0x217907cda10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorDB:\n",
    "    def __init__(self, collection_name:str = \"pdf_info\",\n",
    "                 persist_directory:str = \"../vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self._initialize_store()\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            # Create file\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.Client(\n",
    "                Settings(\n",
    "                    persist_directory = self.persist_directory,\n",
    "                    anonymized_telemetry = False\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata={\"info\":\"PDF Embeddings\"}\n",
    "            )\n",
    "            \n",
    "            print(f\"Vector_store initialized {self.collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Store Not loaded {e}\")\n",
    "            self.client = None\n",
    "            raise\n",
    "    def add_docs(self, documents:List[Any], embeddings:np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Length of Documents and embeddings should be same\")\n",
    "        ids = []\n",
    "        document_text = []\n",
    "        metadatas = []\n",
    "        embedding_list = []\n",
    "        \n",
    "        for i, (document, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"Doc_{uuid.uuid4().hex[:10]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            metadata = dict(document.metadata)\n",
    "            metadata[\"id\"] = i\n",
    "            metadata[\"content\"] = len(document.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            document_text.append(document.page_content)\n",
    "            embedding_list.append(embedding.tolist())\n",
    "        \n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embedding_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=document_text\n",
    "            )\n",
    "            print(f\"successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding docs to vector store: {e}\")\n",
    "            raise\n",
    "vector_store = VectorDB()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169c5dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain\\nVasilios Mavroudis\\nAlan Turing Institute\\nvmavroudis@turing.ac.uk\\nAbstract. LangChain is a rapidly emerging framework that offers a ver-\\nsatile and modular approach to developing applications powered by large\\nlanguage models (LLMs). By leveraging LangChain, developers can sim-\\nplify complex stages of the application lifecycle—such as development,\\nproductionization, and deployment—making it easier to build scalable,\\nstateful, and contextually aware applications. It provides tools for han-\\ndling chat models, integrating retrieval-augmented generation (RAG),\\nand offering secure API interactions. With LangChain, rapid deployment\\nof sophisticated LLM solutions across diverse domains becomes feasible.\\nHowever, despite its strengths, LangChain’s emphasis on modularity and\\nintegration introduces complexities and potential security concerns that\\nwarrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,',\n",
       " 'warrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,\\nLangServe, and LangSmith. We explore how the framework facilitates the\\ndevelopment of LLM applications, discuss its applications across multi-\\nple domains, and critically evaluate its limitations in terms of usability,\\nsecurity, and scalability. By offering valuable insights into both the capa-\\nbilities and challenges of LangChain, this paper serves as a key resource\\nfor developers and researchers interested in leveraging LangChain for\\ninnovative and secure LLM-powered applications.\\nKeywords: LangChain · Large Language Models · LLM Applications ·\\nModular Framework\\nThe emergence of large language models (LLMs) such as OpenAI’s o1 [13],\\nGPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-',\n",
       " 'GPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-\\nlocked unprecedented capabilities in understanding and generating human-like\\ntext, enabling applications that range from intelligent conversational agents to\\nsophisticated data analysis tools. However, harnessing the full potential of LLMs\\nin real-world applications presents significant challenges. Developers must nav-\\nigate complexities related to model integration, state management, scalability,\\ncontextual awareness, and security.\\nLangChain has rapidly gained prominence as a powerful framework designed\\nto address these challenges in developing LLM-powered applications [2]. By\\nproviding a modular and flexible architecture, LangChain simplifies the com-\\nplexities inherent in working with LLMs, enabling developers to build scalable,',\n",
       " '2\\nVasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nable API deployment, and LangSmith for monitoring and evaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning',\n",
       " 'Despite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.',\n",
       " 'LangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-',\n",
       " 'functionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1\\nArchitecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific',\n",
       " 'LangChain\\n3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1. LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model',\n",
       " 'the top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPrompt Templates: Structured templates that standardize and format queries,\\nensuring consistency and precision in interactions with AI models. These tem-',\n",
       " 'Prompt Templates: Structured templates that standardize and format queries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.',\n",
       " '4\\nVasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.',\n",
       " 'Output Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks: Functions that log, monitor, and stream specific events within LangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1\\nChat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation',\n",
       " 'conversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2\\nRetrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy',\n",
       " 'LangChain\\n5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwith vector storage solutions like Chroma and Milvus for optimized searches [3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3\\nSecurity and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:',\n",
       " 'access to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4\\nIntegrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– Integration Packages: LangChain provides dedicated packages (e.g., langchain-',\n",
       " 'multi-modal data processing and AI tool integration [3]:\\n– Integration Packages: LangChain provides dedicated packages (e.g., langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– Custom Component Development: Developers can build custom plugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.',\n",
       " '6\\nVasilios Mavroudis\\n1.5\\nAdvanced Components\\nBeyond these core elements, LangChain offers advanced modules that support\\ncomplex workflows, API deployments, and performance monitoring. These com-\\nponents are elaborated in the following sections:\\n– LangGraph for Stateful Process Modeling: Explored in Section 3,\\nLangGraph enables developers to structure applications with nodes and\\nedges, allowing for complex branching and multi-agent workflows.\\n– LangServe for API Deployment: Detailed in Section 4, LangServe facil-\\nitates the deployment of LangChain applications as REST APIs, supporting\\nscalability in production [5].\\n– LangSmith for Monitoring and Evaluation: Discussed in Section 2,\\nLangSmith offers tools for real-time performance monitoring, error tracking,\\nand version control to optimize applications iteratively [4].\\n2\\nLangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-',\n",
       " '2\\nLangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-\\ning essential tools for building production-grade systems. Integrated with the\\nLangChain ecosystem, it enables users to trace, evaluate, and refine applications,\\nenhancing precision in complex environments. The platform addresses key chal-\\nlenges in observability, testing, and optimization, allowing developers to monitor\\nperformance, troubleshoot issues, and maintain high standards over time [9].\\n2.1\\nTracing\\nTracing is a central feature of LangSmith, providing detailed visibility into how\\napplications interact with LLMs and external data sources. For developers using\\nLangChain, LangSmith offers tracing without the need for direct SDK inte-\\ngration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-',\n",
       " 'gration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-\\nserve and analyze component behavior in real time. This capability is especially\\nuseful for debugging and identifying bottlenecks within complex workflows.\\nLangSmith supports multiple ways to log traces, including languages like\\nPython and TypeScript. Each trace logs every call made to an LLM, along with\\ninput parameters, function annotations, and generated outputs, making it easier\\nto identify where adjustments may be necessary. By using traceable wrappers\\nand decorators, developers can annotate functions or data pipelines, enabling\\nautomatic trace logging with minimal code alterations [9].\\n2.2\\nPerformance Testing\\nLangSmith’s evaluation tools enable developers to test and validate applica-\\ntions under real-world conditions. Evaluations require a defined dataset of test',\n",
       " 'LangChain\\n7\\ncases, including inputs and expected outputs. Using these datasets, developers\\ncan conduct performance tests and assess how well their models meet expected\\noutcomes—an essential step for applications where accuracy and reliability are\\ncrucial. LangSmith supports custom evaluators, allowing developers to specify\\nscoring functions based on specific needs. For instance, an evaluator may mea-\\nsure the exact match between outputs and expected answers, or use metrics\\nlike cosine similarity for open-ended tasks. By supporting both built-in and cus-\\ntom evaluators, LangSmith provides flexibility in performance measurement for\\ndeterministic outputs or nuanced language generation tasks [9].\\n2.3\\nDataset Management\\nDatasets are foundational to LangSmith’s evaluation system. Organizing test\\ncases into structured datasets enables methodical testing and validation. De-\\nvelopers can create datasets manually or import them from existing sources,',\n",
       " 'cases into structured datasets enables methodical testing and validation. De-\\nvelopers can create datasets manually or import them from existing sources,\\nallowing diverse testing scenarios that reflect real-world use cases. Datasets can\\ncontain structured or unstructured data evaluations, accommodating a variety of\\ntesting needs. LangSmith’s dataset version control allows developers to maintain\\nmultiple dataset versions as applications evolve. This feature is critical for ensur-\\ning consistency in evaluation, especially as application logic changes or models\\nare retrained, providing a robust foundation for testing and validation [9].\\n2.4\\nLangSmith Workflow\\nLangSmith integrates tracing, evaluation, and dataset management into a cohe-\\nsive framework, enabling developers to progress from debugging to optimization\\nin a structured manner. A typical LangSmith workflow includes the following\\nstages:\\n– Trace Logging: Developers activate tracing on application functions, pro-',\n",
       " 'in a structured manner. A typical LangSmith workflow includes the following\\nstages:\\n– Trace Logging: Developers activate tracing on application functions, pro-\\nviding insights into model-component interactions.\\n– Dataset Creation and Evaluation: Developers create datasets represent-\\ning different scenarios to conduct comprehensive testing.\\n– Evaluation and Iterative Optimization: Evaluation results indicate per-\\nformance areas for refinement, guiding iterative application improvements.\\n– Version Control and Historical Tracking: LangSmith logs all interac-\\ntions, dataset versions, and evaluation scores, allowing developers to assess\\nimprovements over time.\\n2.5\\nIntegration with LangChain and LangServe\\nLangSmith integrates seamlessly with LangChain and LangServe (Section 4) to\\nenhance the end-to-end LLM application development experience. For LangChain\\nusers, LangSmith can automatically log traces and integrate with existing work-',\n",
       " 'enhance the end-to-end LLM application development experience. For LangChain\\nusers, LangSmith can automatically log traces and integrate with existing work-\\nflows. Combined with LangServe, LangSmith provides robust observability for\\nAPI deployments, monitoring real-time usage patterns, tracking request laten-\\ncies, and identifying bottlenecks.',\n",
       " '8\\nVasilios Mavroudis\\n3\\nLangGraph\\nLangGraph is a low-level framework for building stateful, multi-actor appli-\\ncations with large language models (LLMs). It provides developers with fine-\\ngrained control over application flows, incorporating cycles, branching, and per-\\nsistence to support complex agent workflows. Inspired by frameworks such as\\nPregel [11] and Apache Beam [15], LangGraph enables advanced human-in-the-\\nloop applications and persistent state management, allowing for more reliable\\nand adaptable LLM-powered systems [7].\\n3.1\\nCore Features of LangGraph\\nCycles and Branching LangGraph distinguishes itself by supporting cycles\\nand branching in application workflows. This feature is particularly beneficial\\nfor agentic architectures that require iterative or conditional logic. By enabling\\ncycles within workflows, LangGraph provides a flexible structure that allows\\nnodes to execute repeatedly until a specified condition is met. This contrasts',\n",
       " 'cycles within workflows, LangGraph provides a flexible structure that allows\\nnodes to execute repeatedly until a specified condition is met. This contrasts\\nwith typical directed acyclic graph (DAG)-based architectures, which are limited\\nto single-pass execution without feedback loops [7].\\nPersistence and State Management One of LangGraph’s key innovations is\\nits built-in support for persistence, which enables state to be saved and accessed\\nthroughout the application’s lifecycle. This persistent state management is cru-\\ncial for applications that require continuity across sessions, such as customer ser-\\nvice agents or educational tools that need to recall previous interactions. Lang-\\nGraph’s persistence feature also facilitates advanced human-in-the-loop work-\\nflows, allowing agents to pause, receive human input, and resume operations\\nseamlessly.\\nLangGraph utilizes a stateful execution model where each node in the graph',\n",
       " 'flows, allowing agents to pause, receive human input, and resume operations\\nseamlessly.\\nLangGraph utilizes a stateful execution model where each node in the graph\\nupdates the application state as it processes input. For instance, in a multi-turn\\nconversation, the graph maintains a memory of all previous messages, which can\\nbe accessed by subsequent nodes to ensure coherent responses. This persistent\\nstate can also be saved externally using the LangGraph Platform [8], ensuring\\nrobust memory management across long-running sessions [7].\\nHuman-in-the-Loop and Streaming Support LangGraph offers built-in\\nsupport for human-in-the-loop interactions, which is essential for applications\\nthat require manual intervention or approval at certain stages. For example, a\\nhuman operator can review an agent’s planned actions and approve, reject, or\\nmodify them before the agent proceeds. This level of control makes LangGraph\\nsuitable for high-stakes domains like healthcare or legal advice, where accuracy',\n",
       " 'modify them before the agent proceeds. This level of control makes LangGraph\\nsuitable for high-stakes domains like healthcare or legal advice, where accuracy\\nand oversight are critical.\\nAdditionally, LangGraph supports streaming outputs from each node as they\\nare produced. This capability is especially useful for applications like chatbots',\n",
       " 'LangChain\\n9\\nor real-time monitoring systems, where immediate feedback improves user expe-\\nrience. Streaming can be implemented within any node in the graph, enabling\\nreal-time updates as actions are processed [7].\\n3.2\\nLangGraph Platform\\nThe LangGraph Platform [8] is an infrastructure solution that extends the\\nopen-source LangGraph framework for production deployments. It includes com-\\nponents like LangGraph Server (for API access), LangGraph SDKs (client li-\\nbraries), and LangGraph CLI (a command-line interface for deployment manage-\\nment). The platform is designed to handle complex agent workflows, supporting\\nlong-running agents, background processing, and task queuing to ensure reliable\\nperformance even under heavy loads. The LangGraph Platform also includes\\nfeatures such as:\\n– Background Execution: Allows agents to run asynchronously, handling\\nuser requests in parallel without blocking other tasks.\\n– Support for Long-Running Agents: Provides infrastructure for agents',\n",
       " '– Background Execution: Allows agents to run asynchronously, handling\\nuser requests in parallel without blocking other tasks.\\n– Support for Long-Running Agents: Provides infrastructure for agents\\nthat need to operate over extended periods, managing resource allocation\\nand monitoring agent health.\\n– Burst Handling and Task Queues: Uses queues to manage sudden in-\\ncreases in requests, ensuring that high-priority tasks are processed efficiently.\\n3.3\\nLangGraph Workflow\\nA typical LangGraph workflow begins by defining the state schema and nodes\\nrequired for the application. Each node represents an independent function, such\\nas calling an LLM, invoking a tool, or accessing external data. The developer sets\\nan entry point for graph execution and defines the transitions (edges) between\\nnodes, which can be conditional or sequential based on application requirements.\\n– Defining Nodes and State: Developers initialize nodes, such as an LLM',\n",
       " 'nodes, which can be conditional or sequential based on application requirements.\\n– Defining Nodes and State: Developers initialize nodes, such as an LLM\\nnode for responses or a tool node for external API calls, and specify the state\\nschema to manage conversation context.\\n– Setting Entry Points and Edges: Nodes are connected by edges, with\\nconditions determining the flow based on the application’s state.\\n– Compiling and Executing the Graph: Once nodes and edges are defined,\\nthe graph is compiled into a runnable format, enabling calls to functions such\\nas invoke() for execution and stream() for real-time updates.\\nLangGraph’s workflow design allows applications to cycle between nodes\\nbased on input conditions and dynamically update state, enabling applications\\nthat require complex interaction patterns.',\n",
       " '10\\nVasilios Mavroudis\\n3.4\\nIntegration with LangChain and LangSmith\\nLangGraph integrates seamlessly with LangChain and LangSmith, although\\nit operates independently if desired. For users within the LangChain ecosys-\\ntem, LangGraph can utilize LangSmith’s tracing capabilities to monitor each\\nnode’s performance and capture detailed logs of agent interactions. Addition-\\nally, LangChain tools and APIs can be incorporated as graph nodes, expanding\\nLangGraph’s functionality with LangChain’s extensive library of tools and con-\\nnectors [7].\\n4\\nLangServe\\nLangServe [5] is an integral component of the LangChain ecosystem, specifically\\ndesigned to facilitate the deployment of large language model (LLM) applications\\nas scalable REST APIs. With LangServe, developers can create production-\\ngrade APIs that allow external systems and users to interact with LangChain\\napplications. It enables LLM-powered applications to be served in real-time with',\n",
       " 'grade APIs that allow external systems and users to interact with LangChain\\napplications. It enables LLM-powered applications to be served in real-time with\\nrobust support for load balancing, monitoring, and scalability.\\n4.1\\nCore Features of LangServe\\nAPI Deployment and Management LangServe simplifies the process of\\nturning LangChain applications into APIs, making LLM models accessible for\\nvarious services and client applications. With LangServe, any LangChain work-\\nflow can be packaged and exposed via RESTful endpoints, enabling interactions\\nwith language models, data retrieval, and external tool integration. The API-\\ncentric design allows LangServe to support diverse use cases, from chatbots and\\nrecommendation systems to complex multi-agent interactions. It provides several\\ntools for managing API endpoints, such as configurable routing, request han-\\ndling, and response formatting, which make it easy to customize each endpoint',\n",
       " 'tools for managing API endpoints, such as configurable routing, request han-\\ndling, and response formatting, which make it easy to customize each endpoint\\nbased on application requirements. This flexibility allows developers to design\\nAPIs with specific functionalities, making LangServe suitable for applications of\\nvarying complexity [5].\\nScalability and Load Balancing LangServe includes built-in support for scal-\\nability, making it ideal for applications that experience high traffic volumes. It\\ncan handle multiple API requests simultaneously, ensuring consistent perfor-\\nmance by balancing the load across instances. This feature is critical for pro-\\nduction environments where maintaining low response times under heavy loads\\nis essential. To further enhance scalability, LangServe provides tools for setting\\nup auto-scaling, which dynamically adjusts the number of instances based on\\ndemand. This allows applications to handle traffic spikes without degradation',\n",
       " 'up auto-scaling, which dynamically adjusts the number of instances based on\\ndemand. This allows applications to handle traffic spikes without degradation\\nin performance, making LangServe suitable for real-world deployments with un-\\npredictable usage patterns [5].',\n",
       " 'LangChain\\n11\\nLatency and Error Management LangServe is designed to minimize latency,\\nensuring quick responses for each API request. It employs efficient request queu-\\ning and processing mechanisms to reduce waiting times. Additionally, LangServe\\nincludes error handling and retry logic, which helps maintain reliability by man-\\naging transient failures and minimizing downtime. This focus on latency and\\nerror resilience makes LangServe suitable for mission-critical applications that\\nrequire high availability. For instance, LangServe can automatically retry failed\\nrequests and log errors for further investigation, allowing developers to identify\\nissues promptly and maintain a stable API response rate [5].\\n4.2\\nLangServe Workflow\\nThe LangServe deployment workflow is straightforward, enabling developers to\\ngo from a LangChain application to a deployed API in a few steps. Here’s an\\noutline of a typical LangServe workflow:',\n",
       " 'The LangServe deployment workflow is straightforward, enabling developers to\\ngo from a LangChain application to a deployed API in a few steps. Here’s an\\noutline of a typical LangServe workflow:\\n1. Defining Endpoints: Developers define endpoints based on application re-\\nquirements, specifying which functions or models are exposed via API routes.\\nEach endpoint can have customized parameters, allowing for flexibility in\\nhow the API interacts with different components.\\n2. Configuring Request Handling and Routing: LangServe allows for fine-\\ngrained control over how requests are processed. Developers can set up rout-\\ning rules, parameter validation, and request parsing to tailor the API expe-\\nrience.\\n3. Setting Up Load Balancing and Scaling: For applications with high\\ntraffic, LangServe’s load balancing can be configured to distribute requests\\nacross multiple instances, ensuring consistent response times. Auto-scaling\\ncan also be set up to dynamically adjust resources based on demand.',\n",
       " 'across multiple instances, ensuring consistent response times. Auto-scaling\\ncan also be set up to dynamically adjust resources based on demand.\\n4. Monitoring and Error Tracking: LangServe integrates with monitoring\\ntools, including LangSmith, to provide real-time insights into API perfor-\\nmance, usage metrics, and error rates. This monitoring helps developers\\nmaintain optimal performance and quickly resolve issues as they arise.\\nLangServe’s streamlined workflow ensures that developers can deploy robust\\nAPIs with minimal overhead, making it a practical choice for scaling LLM ap-\\nplications in production environments.\\n4.3\\nIntegration with LangSmith and LangChain\\nLangServe integrates seamlessly with LangSmith, which offers observability fea-\\ntures like tracing, logging, and performance monitoring. Through LangSmith,\\nLangServe users can track metrics such as request frequency, latency, and er-\\nror rates. This integration provides a comprehensive view of API performance,',\n",
       " 'LangServe users can track metrics such as request frequency, latency, and er-\\nror rates. This integration provides a comprehensive view of API performance,\\nenabling developers to optimize applications based on real-time data.\\nAdditionally, LangServe’s integration with LangChain allows developers to\\nleverage LangChain’s extensive library of tools, models, and connectors as part',\n",
       " '12\\nVasilios Mavroudis\\nof the API. LangChain workflows, tools, and chains can be directly exposed\\nvia LangServe endpoints, providing flexible API interactions and enhancing the\\nfunctionality of LLM applications [5].\\n5\\nLimitations and Criticisms\\nLangChain provides a versatile framework for the development of applications\\npowered by large language models (LLMs). However, several limitations warrant\\nattention, especially in the domains of complexity and security.\\n5.1\\nComplexity\\nLangChain’s modular architecture, while designed to simplify LLM-based ap-\\nplication development, can paradoxically increase complexity. Effective use of\\nLangChain often requires a nuanced understanding of its distinct components,\\nsuch as LangGraph and LangSmith, as well as familiarity with its API ecosys-\\ntem. Consequently, developers may face a steep learning curve, particularly those\\naiming for rapid prototyping or deployment. The need for comprehensive knowl-',\n",
       " 'tem. Consequently, developers may face a steep learning curve, particularly those\\naiming for rapid prototyping or deployment. The need for comprehensive knowl-\\nedge of each module may present a barrier to new users, complicating onboarding\\nand initial implementation phases.\\n5.2\\nSecurity Concerns\\nGiven LangChain’s modular design and extensive reliance on external integra-\\ntions, security presents a notable challenge. Although LangChain incorporates a\\nrange of security measures, including fine-grained permission control and sand-\\nboxing, the complexity of securing LLM-based applications remains high, espe-\\ncially for applications managing sensitive data. Below, we outline several critical\\nsecurity risks associated with LangChain and explore strategies for risk mitiga-\\ntion.\\nRisks Associated with External Providers To enhance functionality, LangChain\\nintegrates with numerous external services, such as vector databases, API providers,',\n",
       " 'tion.\\nRisks Associated with External Providers To enhance functionality, LangChain\\nintegrates with numerous external services, such as vector databases, API providers,\\nand cloud storage platforms. However, these integrations expose applications to\\nsecurity vulnerabilities:\\n– Data Exposure: Accessing external resources can inadvertently expose sen-\\nsitive data to third-party providers, a risk particularly relevant for applica-\\ntions handling personal or confidential information. Without stringent data\\nencryption and access control mechanisms, the potential for data leaks or\\nunauthorized access increases.\\n– Third-Party Dependency: Reliance on third-party services introduces de-\\npendencies on their security protocols. Any compromise within a provider’s\\ninfrastructure could affect LangChain applications, resulting in data breaches\\nor service interruptions. This underscores the importance of thoroughly vet-\\nting providers and monitoring them for potential security issues.',\n",
       " 'LangChain\\n13\\nLangChain’s security model addresses many of these concerns, yet challenges\\npersist, particularly in sectors with rigorous compliance standards, such as fi-\\nnance and healthcare. Key areas for ongoing improvement include:\\n– Dynamic Permission Adjustment: Current permission settings in LangChain\\nare defined at deployment, but in dynamic applications, permissions may\\nneed to adapt based on user interactions. Implementing adaptive permis-\\nsions responsive to application state or user roles could enhance security.\\n– Advanced Encryption Standards: For applications processing highly\\nsensitive data, adopting advanced encryption practices—such as end-to-end\\nor field-level encryption—could bolster data security within even trusted\\nenvironments.\\n– Proactive Security Analytics: Integrating predictive analytics to pre-\\nemptively identify risks could further secure applications. Machine learning\\nmodels analyzing application logs could flag anomalous patterns indicative',\n",
       " 'emptively identify risks could further secure applications. Machine learning\\nmodels analyzing application logs could flag anomalous patterns indicative\\nof potential breaches or misuse.\\nIn summary, LangChain’s security framework includes robust features such\\nas granular permissions, sandboxing, and real-time monitoring. While these mea-\\nsures provide a solid foundation, the ongoing challenge of securing LLM-driven\\napplications—particularly those relying on external providers—demands contin-\\nued advancements in security practices.\\n6\\nConclusion\\nLangChain significantly advances the development of applications powered by\\nlarge language models (LLMs). Its modular framework—including components\\nlike LangGraph for stateful process modeling, LangServe for scalable API de-\\nployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-',\n",
       " 'ployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-\\nverse domains, including NLP, cybersecurity, healthcare, finance, and customer\\nservice.\\nWhile its versatility extends beyond NLP, allowing for applications in fields\\nlike cybersecurity (e.g., threat detection and automated incident response), the\\nframework’s emphasis on flexibility introduces complexities that may present a\\nlearning curve for developers new to LangChain. Additionally, reliance on exter-\\nnal integrations raises important security considerations, such as data exposure\\nand dependency vulnerabilities, which are critical in sensitive areas where data\\nintegrity and privacy are paramount.\\nIn summary, LangChain’s transformative potential lies in bridging the gap\\nbetween the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-',\n",
       " 'between the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-\\nments in usability and security, LangChain can continue to serve as a valuable\\ntool for developers seeking to leverage LLMs in building innovative and secure\\napplications. As industries increasingly adopt AI technologies, frameworks like\\nLangChain are poised to play a pivotal role in shaping the next generation of\\nintelligent, scalable, and secure solutions across various sectors.',\n",
       " '14\\nVasilios Mavroudis\\nReferences\\n1. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-\\nrencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\\nAnadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.\\n2. Harrison Chase.\\nLangChain, Oct 2022.\\nAvailable at https://github.com/\\nlangchain-ai/langchain.\\n3. LangChain, Inc. LangChain Documentation: Integration Providers. LangChain,\\nInc., San Francisco, CA, 2024. Available at https://python.langchain.com/docs/\\nintegrations/providers/.\\n4. LangChain, Inc.\\nLangChain Documentation: Key Concepts.\\nLangChain, Inc.,\\nSan Francisco, CA, 2024.\\nAvailable at https://python.langchain.com/docs/\\nconcepts/.\\n5. LangChain, Inc.\\nLangChain Documentation: LangServe.\\nLangChain, Inc.,\\nSan Francisco, CA, 2024.\\nAvailable at https://python.langchain.com/docs/\\nlangserve/.\\n6. LangChain, Inc. LangChain Documentation: Security Best Practices. LangChain,',\n",
       " 'LangChain, Inc.,\\nSan Francisco, CA, 2024.\\nAvailable at https://python.langchain.com/docs/\\nlangserve/.\\n6. LangChain, Inc. LangChain Documentation: Security Best Practices. LangChain,\\nInc., San Francisco, CA, 2024. Available at https://python.langchain.com/docs/\\nsecurity/.\\n7. LangChain, Inc. LangGraph: Building Language Agents as Graphs, 2024. Accessed:\\n2024-11-04.\\n8. LangChain, Inc. LangGraph Platform Documentation, 2024. Accessed: 2024-11-04.\\n9. LangChain, Inc. LangSmith: A Developer Platform for LLM Applications, 2024.\\nAccessed: 2024-11-04.\\n10. Patrick\\nLewis,\\nEthan\\nPerez,\\nAleksandra\\nPiktus,\\nFabio\\nPetroni,\\nVladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.\\nAdvances in Neural Information Processing Systems, 33:9459–9474, 2020.\\n11. Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan',\n",
       " 'Advances in Neural Information Processing Systems, 33:9459–9474, 2020.\\n11. Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan\\nHorn, Naty Leiser, and Grzegorz Czajkowski. Pregel: A System for Large-Scale\\nGraph Processing. In Proceedings of the 2010 ACM SIGMOD International Con-\\nference on Management of Data, pages 135–146, 2010.\\n12. OpenAI. Hello GPT-4O, 05 2024.\\n13. OpenAI. Introducing OpenAI O1-Preview, 09 2024.\\n14. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui\\nYu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican,\\net al. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint\\narXiv:2312.11805, 2023.\\n15. The Apache Software Foundation. Apache Beam: An Advanced Unified Program-\\nming Model, 2024. Accessed: 2024-11-04.\\n16. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal',\n",
       " 'ming Model, 2024. Accessed: 2024-11-04.\\n16. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv\\npreprint arXiv:2302.13971, 2023.',\n",
       " 'ASTRONOMICAL DATA ANALYSIS SOFTWARE AND SYSTEMS XIV\\nASP Conference Series, Vol. 347, 2005\\nP. L. Shopbell, M. C. Britton, and R. Ebert, eds.\\nmatplotlib – A Portable Python Plotting Package\\nPaul Barrett\\nSpace Telescope Science Institute\\nJohn Hunter\\nUniversity of Chicago\\nJ. Todd Miller, Jin-Chung Hsu, and Perry Greenﬁeld\\nSpace Telescope Science Institute\\nAbstract.\\nmatplotlib is a portable 2D plotting and imaging package aimed\\nprimarily at visualization of scientiﬁc, engineering, and ﬁnancial data.\\nmat-\\nplotlib can be used interactively from the Python shell, called from python\\nscripts, or embedded in a GUI application (GTK, Wx, Tk, Windows). Many\\npopular hardcopy outputs are supported including JPEG, PNG, PostScript and\\nSVG. Features include the creation of multiple axes and ﬁgures per page, inter-\\nactive navigation, many predeﬁned line styles and symbols, images, antialiasing,\\nalpha blending, date and ﬁnancial plots, W3C compliant font management and',\n",
       " 'active navigation, many predeﬁned line styles and symbols, images, antialiasing,\\nalpha blending, date and ﬁnancial plots, W3C compliant font management and\\nFreeType2 support, legends and tables, pseudocolor plots, mathematical text\\nand more. It works with both numarray and Numeric. The goals of the pack-\\nage, basic architecture, current features (illustrated with examples), and planned\\nenhancements will be described.\\n1.\\nIntroduction\\nmatplotlib is designed with the philosophy that you should be able to create\\nsimple plots with just a few commands, or just one!\\nIf you want to see a\\nhistogram of your data, you shouldn’t need to instantiate objects, call methods,\\nset properties, etc; it should just work.\\nThe initial goals of matplotlib were:\\n• Plots should be publication quality; particularly the text (antialiased, ro-\\ntated, etc.).\\n• PostScript output for inclusion with TEX documents.\\n• Embeddable in a graphical user interface for application development.\\n• Code should be understandable.',\n",
       " 'tated, etc.).\\n• PostScript output for inclusion with TEX documents.\\n• Embeddable in a graphical user interface for application development.\\n• Code should be understandable.\\n• Making plots should be easy.\\n• The software is Open Source, so it can be downloaded, used, and dis-\\ntributed freely.\\nmatplotlib can be used in a variety of settings. Most users are familiar with\\nthe command-line for interactively creating plots and images.\\nThis interface\\nprovides a simple pop-up window for displaying and manipulating the data.\\nHowever, the true power of matplotlib is the underlying plotting library, which\\n91',\n",
       " '92\\nBarrett et al.\\n\\x00\\x02\\x01\\x02\\x01\\x02\\x01\\n\\x00\\x02\\x01\\x02\\x03\\x02\\x01\\n\\x00\\x02\\x01\\x05\\x04\\x06\\x01\\n\\x00\\x02\\x01\\x02\\x07\\x02\\x01\\n\\x00\\x02\\x01\\x05\\x08\\x06\\x01\\n)\\n\\t\\nA\\n(',\n",
       " '\\x01\\n\\x03\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x04\\x02\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x07\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x08\\x02\\x0b\\r\\x0c\\x0e\\x00\\x02\\x0f\\n\\x00\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x03\\n\\x00\\x02\\x10\\n\\x03\\x05\\x0b\\r\\x0c\\x0e\\x00\\x02\\x03\\n\\x00\\x02\\x10\\n\\x04\\x02\\x0b\\x11\\x0c\\x12\\x00\\x02\\x03\\n\\x13\\x14\\n\\x15\\n\\x16\\n\\x17\\x19\\x18\\x1b\\x1a\\x1d\\x1c\\x1f\\x1e\\x06 !\\x17#\"%$\\'&\\x11(*),+.-0/.1*243\\r56\\x1c*78$:9*;\\nFigure 1.\\nA line plot of FITS binary table data containing 10k points.\\nis operating system independent and graphical user interface (GUI) agnostic. It\\ncan be used without a GUI as part of a web server to create plots and images in\\na variety of hardcopy outputs; or can be embedded in a larger application using\\none of several GUIs (e.g. GTK, Tk, or WXwindows) running on one of several\\nOSs (e.g. Windows, OS X, Solaris, and Linux).\\n2.\\nArchitecture\\nThe matplotlib code is conceptually divided into three parts:\\n• The matlab interface is the set of functions that allow a user to create\\nplots from the command line.\\n• The frontend or matplotlib API is the set of classes that do the heavy\\nlifting by creating and managing ﬁgures, text, lines, plots, etc. This is the\\nabstract interface that knows nothing about output.',\n",
       " 'lifting by creating and managing ﬁgures, text, lines, plots, etc. This is the\\nabstract interface that knows nothing about output.\\n• The backends are device dependent drawing devices or renderers that\\ntransform the frontend representation to hardcopy (JPEG, PNG, PDF,\\nPS, SVG, Paint, GD) or a display device (Agg, GTK/GTKAgg, TkAgg,\\nWX/WXAgg). Much of the critical rendering code is written in C/C++\\nand therefore provides very good performance.\\nAgg is the Anti-Grain Graphics library that enables writing vector graphics\\nto a buﬀer, which can then be block transfered (or BLTed) to the display de-\\nvice. This means that all interactive implementations based on Agg avoid the',\n",
       " 'matplotlib – A Portable Python Plotting Package\\n93\\n<\\x0e=?>\\n@BA\\nCED\\nF0G\\n@\\nCED\\nF0GIH\\nCJD\\nF\\x19K\\nA\\nCED\\nF\\n@\\x12L\\nM\\n=?N\\nG\\nK\\nM\\n=?N\\nG\\x12O\\nM\\n=\\nN\\nK\\nP\\nM\\n=?N\\n@\\n@\\nM\\n=?N\\n@IH\\nQSR\\nN\\nG\\x12P\\nQSR\\nN\\nK\\n@\\nQSR\\nN\\nK\\nH\\nT%U\\nT%V\\nT%W\\nT%X\\nY%Z\\nY\\\\[\\nY%T\\nFigure 2.\\nA ﬁnancial plot that uses the daily high, low, and closing values\\nof a stock price.\\ngraphical limitations of the GUI and render identical graphics regardless of the\\nGUI interface.\\n3.\\nPlotting\\nThe following Python session uses the matlab interface to create a quicklook\\nspectrum of FUSE data (see Figure 1).\\n> python\\nPython 2.3.3 (#1, Jan\\n5 2004, 16:22:13)}\\n[GCC 2.96 20000731 (Red Hat Linux 7.3 2.96-113)] on linux2\\nType \"help\", \"copyright\", \"credits\" or \"license\"\\nfor more information.\\n>>> import pyfits\\n>>> fits = pyfits.open(’fuse.fits’)\\n>>> wave = fits[1].data.field(’wave’)\\n>>> flux = fits[1].data.field(’flux’)\\n>>> from matplotlib.matlab import *\\n>>> ylim(0, 1.5e-12)\\n>>> xlim(985, 1085)\\n>>> xlabel(r’$\\\\lambda\\\\ (\\\\angstrom)$’)\\n<matplotlib.text.Text instance at 0x41118f8c>',\n",
       " '>>> flux = fits[1].data.field(’flux’)\\n>>> from matplotlib.matlab import *\\n>>> ylim(0, 1.5e-12)\\n>>> xlim(985, 1085)\\n>>> xlabel(r’$\\\\lambda\\\\ (\\\\angstrom)$’)\\n<matplotlib.text.Text instance at 0x41118f8c>\\n>>> ylabel(r’Flux’)',\n",
       " '94\\nBarrett et al.\\n]\\n^_]%]\\n`%].]\\na%]%]\\nb%]%]\\ncS]%]\\nd.egf.hjiEiEk\\n]\\n^_].]\\n`%].]\\na%].]\\nb%].]\\nc_].]\\nl\\nmn\\nop\\nq\\nq\\nr\\nsutwvyx\\\\twz|{~}jz\\n\\x7f\\x80z|{\\x19\\x81\\x1dtwvy\\x82jtw}\\x83\\x82\\x1b\\x84\\x85x\\x02\\x81\\nFigure 3.\\nA graphic of FITS image data. Note the labeled axes and title.\\n<matplotlib.text.Text instance at 0x4112108c>\\n>>> title(’FUSE LiF 1A spectrum of EG And’)\\n<matplotlib.text.Text instance at 0x4112420c>\\n>>> plot(wave, flux)\\nOther available plot types are: 2-D vector plots, high-low-close plots (see\\nFigure 2), histogram plots, log plots, pie charts and bar charts.\\n4.\\nImages\\nThe matlab interface has two functions for displaying image data: ﬁgimage,\\nwhich will preserve the size and shape of the image; and imshow, which will\\nresample the image to ﬁt the size of the ﬁgure (see Figure 3). Images can be\\nenhanced by annotations or graphical overlays.\\n5.\\nFeatures\\nKey features that make matplotlib easy to use are:\\n• Integrated support for numarray or Numeric – the Python multi-dimen-\\nsional array libraries.',\n",
       " '5.\\nFeatures\\nKey features that make matplotlib easy to use are:\\n• Integrated support for numarray or Numeric – the Python multi-dimen-\\nsional array libraries.\\n• The plot window contains a simple interactive GUI with support for pan-\\nand-zoom, history recall, and saving to hardcopy.',\n",
       " 'matplotlib – A Portable Python Plotting Package\\n95\\n• The command line interface is modeled after the easy to use MatLab in-\\nterface.\\n• Support for multiple plots and images per page.\\n• TrueType/FreeType fonts are available in the GD, Agg, Paint, and Post-\\nScript backends. SVG support is coming soon.\\n• Mathematical text ala TEX math mode is available whenever TrueType\\nfonts are available.\\n• Images are automatically resampled to the size of the ﬁgure.\\n• A fully object-oriented design to ease programming and development.\\n6.\\nEnchancements\\nEnhancements to matplotlib that are expected in the near future are:\\n• Contour plots which can be used for image overlays.\\n• The ability to handle general 2-D transforms, which are useful for map\\nprojections and world coordinate systems.\\nTo learn more about matplotlib and to download the latest version, go to\\nhttp://matplotlib.sourceforge.net.',\n",
       " 'Copyright: © the author(s), publisher and licensee Technoscience Academy. This is an open-access article distributed under the \\nterms of the Creative Commons Attribution Non-Commercial License, which permits unrestricted non-commercial use,\\ndistribution, and reproduction in any medium, provided the original work is properly cited \\n \\nInternational Journal of Scientific Research in Computer Science, Engineering and Information Technology \\nISSN : 2456-3307 (www.ijsrcseit.com) \\ndoi : https://doi.org/10.32628/CSEIT2173105 \\n \\n \\n \\n \\n \\n67 \\nStudy On Machine Learning Algorithms \\nPraba. R1, Darshan. G2, Roshanraj. K. T2, Surya Prakash. B2 \\n1Assistant Professor, Department of Information Technology, Dr.N.G.P. Arts and Science College, Coimbatore, \\nTamil Nadu, India \\n2UG Candidate, Department of Information Technology, Dr.N.G.P. Arts and Science College, Coimbatore, \\nTamil Nadu, India \\n \\n \\n \\nArticle Info \\nVolume  7, Issue 4 \\nPage Number: 67-72 \\nPublication Issue : \\nJuly-August-2021',\n",
       " 'Tamil Nadu, India \\n \\n \\n \\nArticle Info \\nVolume  7, Issue 4 \\nPage Number: 67-72 \\nPublication Issue : \\nJuly-August-2021 \\nArticle History \\nAccepted :  02 July 2021 \\nPublished : 08 July 2021 \\nABSTRACT \\n \\nVarious machine learning algorithms are described in this work. These \\nalgorithms are used for a variety of applications, including data mining, image \\nprocessing, predictive analytics, and so on. The fundamental benefit of \\nemploying machine learning is that once an algorithm learns what to do with \\ndata, it can complete tasks on its own. \\n \\nKeywords : Machine Learning, Algorithms \\n  \\n \\nI. INTRODUCTION \\n \\nMachine learning is used to teach machines how to \\nhandle the data more efficiently. We may be unable \\nto interpret the pattern or extract information from \\nthe data after examining it.In that case, we apply \\nmachine learning. With the abundance of datasets \\navailable, the demand for machine learning is in rise. \\nMany industries from medicine to military apply',\n",
       " 'machine learning. With the abundance of datasets \\navailable, the demand for machine learning is in rise. \\nMany industries from medicine to military apply \\nmachine learning to extract relevant information. \\n \\nThe phrase \"Machine Learning\" was coined by \\nArthur Samuel, a pioneer in the fields of artificial \\nintelligence and computer games. Machine learning, \\nhe stated, is a “field of research that enables \\ncomputers \\nto \\nlearn \\nwithout \\nbeing \\nexplicitly \\nprogrammed.” \\n \\nMachine Learning (ML) can be defined as the process \\nof automating and refining the learning process of \\ncomputers based on their experiences without the \\nneed for programming, i.e. without the use of \\nhumans. The process begins with providing high-\\nquality data, which is then used to train our machines \\n(computers) by creating machine learning models \\nbased on the data and other methods. The algorithms \\nwe use are determined by the type of data we have \\nand the task we are attempting to automate.',\n",
       " '(computers) by creating machine learning models \\nbased on the data and other methods. The algorithms \\nwe use are determined by the type of data we have \\nand the task we are attempting to automate. \\n \\nMachine learning (ML) is the study of computer \\nalgorithms that improve themselves over time as a \\nresult of experience and data. 1st It is consideredto be \\na component of artificial intelligence. Machine \\nlearning algorithms create a model based on sample',\n",
       " 'Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n68 \\ndata, referred to as \"training data,\" in order to make \\npredictions or judgments without being explicitly \\nprogrammed. \\n \\nMachine learning algorithms are utilized in a wide \\nrange of applications, including medicine, email \\nfiltering, and computer vision, where developing \\ntraditional algorithms to do the required tasks is \\ndifficult or impossible. \\n \\nHowever, not all machine learning is statistical \\nlearning. A subset of machine learning is strongly \\nrelated to computational statistics, which focuses on \\nmaking predictions using computers. The discipline \\nof machine learning benefits from the study of \\nmathematical optimization since it provides tools, \\ntheory, and application domains. Data mining is a \\nsimilar \\nbranch \\nof \\nresearch \\nthat \\nfocuses \\non \\nunsupervised learning for exploratory data analysis.',\n",
       " \"theory, and application domains. Data mining is a \\nsimilar \\nbranch \\nof \\nresearch \\nthat \\nfocuses \\non \\nunsupervised learning for exploratory data analysis. \\nMachine learning is also known as predictive \\nanalytics when it is used to solve business challenges. \\n \\nII. LITERATURE REVIEW \\n \\nMachine learning is the process of computers figuring \\nout how to do things without being specifically \\nprogrammed to do so. It entails computers learning \\nfrom data in order to do specific jobs. It is possible to \\nbuild algorithms that teach the computer how to \\nperform all steps required to solve the problem at \\nhand for basic jobs entrusted to computers; no \\nlearning is necessary on the computer's behalf. \\nManually creating the required algorithms for more \\ncomplicated tasks can be difficult for a human. In \\npractise, assisting the computer in developing its own \\nalgorithm rather than having human programmers \\nexplain each required step can prove to be more\",\n",
       " 'practise, assisting the computer in developing its own \\nalgorithm rather than having human programmers \\nexplain each required step can prove to be more \\nproductive. Machine learning is a discipline that uses \\na variety of ways to train computers how to complete \\ntasks for which no entirely suitable solution exists. \\nWhen there are a large number of possible replies, \\none strategy is to classify some of the correct \\nresponses as valid.  The computer can then utilize \\nthis as training data to refine the algorithm(s) it uses \\nto determine right answers. The MNIST dataset of \\nhandwritten digits, for example, has frequently been \\nused to train a system for the task of digital character \\nrecognition.  \\n \\nIII. TYPES OF LEARNING \\n \\nA. Supervised Learning \\n \\nSupervised learning algorithms create a mathematical \\nmodel of a set of data that includes both the inputs \\nand the outputs that are sought a The information is \\nreferred to as training data, and it consists of a',\n",
       " 'model of a set of data that includes both the inputs \\nand the outputs that are sought a The information is \\nreferred to as training data, and it consists of a \\ncollection of training instances. Each training \\nexample has one or more inputs and a supervisory \\nsignal as the desired output. Each training sample is \\nrepresented by an array or vector, sometimes referred \\nto as a feature vector, and the training data is \\nrepresented by a matrix in the mathematical model. \\nSupervised learning techniques develop a function \\nthat may be used to predict the output associated \\nwith fresh inputs by iteratively optimizing an \\nobjective function. Active learning, classification, and \\nregression are examples of supervised learning \\nalgorithms. When the outputs are limited to a small \\nset of values, classification techniques are employed, \\nand regression methods are used when the outputs \\ncan have any numerical value within a range.  An \\nincoming email, for example, would be the input to a',\n",
       " 'and regression methods are used when the outputs \\ncan have any numerical value within a range.  An \\nincoming email, for example, would be the input to a \\nclassification algorithm that filters emails, and the \\noutput would be the name of the folder to file the \\nemail in.',\n",
       " 'Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n69 \\n \\nA supervised learning model called a support-vector \\nmachine separates data into areas separated by a \\nlinear border. The black circles are separated from \\nthe white circles by a linear barrier. \\n \\nB.Unsupervised Learning \\nUnsupervised learning methods take a collection of \\ndata with only inputs and detect structure in it, such \\nas data point grouping or clustering. As a result, the \\nalgorithms learn from unlabeled, unclassified, and \\nuncategorized test data.  Unsupervised learning \\nalgorithms discover commonalities in the data and \\nreact depending on the existence or lack of such \\ncommonalities in each new piece of data, rather than \\nresponding to feedback. The field of density \\nestimation in statistics, such as calculating the \\nprobability density function, is a key application of',\n",
       " 'responding to feedback. The field of density \\nestimation in statistics, such as calculating the \\nprobability density function, is a key application of \\nunsupervised learning. Unsupervised learning, on the \\nother hand, comprises various domains that need \\nsummarising and explaining data aspects. \\n \\nCluster analysis divides a set of observations into \\nsubsets (called clusters) so that observations within \\nthe same cluster are comparable based on one or \\nmore predetermined criteria, while observations from \\ndifferent clusters are distinct. Different clustering \\napproaches make different assumptions about the \\nstructure \\nof \\nthe \\ndata, \\nwhich \\nis \\ncommonly \\ncharacterized \\nby \\nsome \\nsimilarity \\nmetric \\nand \\nevaluated, for example, by internal compactness, or \\nthe similarity between cluster members, and \\nseparation, or the difference between clusters. \\nEstimated density and graph connectedness are used \\nin other approaches. \\n \\n \\nC. Semi-supervised Learning',\n",
       " 'separation, or the difference between clusters. \\nEstimated density and graph connectedness are used \\nin other approaches. \\n \\n \\nC. Semi-supervised Learning \\nUnsupervised learning (without any labelled training \\ndata) and supervised learning (with labelled training \\ndata) are the two types of learning (with completely \\nlabelled training data).  Although some of the \\ntraining examples lack training labels, several \\nmachine-learning researchers have discovered that \\nunlabeled data, when combined with a modest \\namount of labelled data, can enhance learning \\naccuracy significantly. The training labels in weakly \\nsupervised learning are noisy, limited, or imprecise, \\nyet they are generally cheaper to obtain, resulting in \\nlarger effective training sets. \\n \\nD. Reinforcement learning \\nReinforcement learning is a branch of machine \\nlearning that studies how software agents should \\nbehave in a given environment in order to maximize \\nsome metric of cumulative reward. Game theory,',\n",
       " 'Reinforcement learning is a branch of machine \\nlearning that studies how software agents should \\nbehave in a given environment in order to maximize \\nsome metric of cumulative reward. Game theory, \\ncontrol theory, operations research, information \\ntheory, simulation-based optimization, multi-agent \\nsystems, swarm intelligence, statistics, and genetic \\nalgorithms are among the numerous disciplines that \\nstudy the field due to its generality. The environment \\nis generally represented as a Markov decision process \\nin machine learning (MDP). Dynamic programming \\ntechniques are used in many reinforcement learning \\nsystems. When exact mathematical models of the \\nMDP \\nare \\ninfeasible, \\nreinforcement \\nlearning \\nprocedures are applied. Reinforcement learning',\n",
       " \"Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n70 \\nalgorithms are employed in autonomous vehicles and \\nin teaching humans how to play a game.  \\n \\nE. Dimensionality Reduction \\nThe technique of lowering the number of random \\nvariables under consideration by generating a set of \\nprimary variables is known as dimensionality \\nreduction. In other words, it's a method of \\nminimising the size of your feature set, also known as \\nthe \\nnumber \\nof \\nfeatures. \\nThe \\nmajority \\nof \\ndimensionality reduction approaches fall into one of \\ntwo categories: feature deletion or extraction. \\nPrincipal component analysis is one of the most \\nwidely used methods for dimensionality reduction.  \\n \\nAnalyze the Principal Components (PCA) \\nPCA entails converting higher-dimensional data \\n(such as 3D) to a smaller space (eg. 2D). This results \\nin a decreased data dimension (2D rather than 3D),\",\n",
       " 'Analyze the Principal Components (PCA) \\nPCA entails converting higher-dimensional data \\n(such as 3D) to a smaller space (eg. 2D). This results \\nin a decreased data dimension (2D rather than 3D), \\nwhile preserving all of the original variables in the \\nmodel and not modifying the data. \\n \\nA supervised learning model called a support-vector \\nmachine separates data into areas separated by a \\nlinear border. The black circles are separated from \\nthe white circles by a linear barrier.  \\n \\nIV. MODELS \\n \\n1.Artificial neural networks \\nArtificial neural networks (ANNs), also known as \\nconnectionist systems, are computing systems that \\nare based on biological neural networks found in \\nanimal brains. Such systems \"learn\" to execute tasks \\nby considering examples, usually without any task-\\nspecific rules being coded. An artificial neural \\nnetwork (ANN) is a model built on a set of connected \\nunits or nodes known as \"artificial neurons,\" which',\n",
       " 'specific rules being coded. An artificial neural \\nnetwork (ANN) is a model built on a set of connected \\nunits or nodes known as \"artificial neurons,\" which \\nare roughly modelled after the neurons in a biological \\nbrain. Each link, like the synapses in a human brain, \\ncan send information, or a \"signal,\" from one artificial \\nneuron to the next.  \\n \\n2. Decision Tree \\nTo get from observations about an item (represented \\nin the branches) to conclusions about the item\\'s goal \\nvalue, decision tree learning employs a decision tree \\nas a predictive model (represented in the leaves). In \\nstatistics, data mining, and machine learning, it is one \\nof \\nthe \\npredictive \\nmodelling \\nmethodologies. \\nClassification trees are tree models in which the goal \\nvariable can take a discrete set of values; in these tree \\nstructures, leaves indicate class labels and branches \\nrepresent feature combinations that lead to those \\nclass labels.       \\n                  \\n \\n \\n3. Support-Vector Networks',\n",
       " 'structures, leaves indicate class labels and branches \\nrepresent feature combinations that lead to those \\nclass labels.       \\n                  \\n \\n \\n3. Support-Vector Networks  \\nSVMs, also known as support-vector networks, are a \\ngroup of similar supervised learning algorithms for',\n",
       " 'Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n71 \\nclassification and regression. An SVM training \\nmethod creates a model that predicts whether a new \\nexample falls into one of two categories given a set of \\ntraining examples that are individually labelled as \\nbelonging to one of two categories.  Although \\nmethods such as Platt scaling exist to employ SVM in \\na probabilistic classification environment, an SVM \\ntraining algorithm is a non-probabilistic, binary, \\nlinear classifier. SVMs may perform non-linear \\nclassification as well as linear classification by \\nimplicitly \\nmapping \\ntheir \\ninputs \\ninto \\nhigh-\\ndimensional feature spaces, which is known as the \\nkernel trick. \\n \\n \\n \\n4. Regression analysis \\nRegression analysis is a broad term that refers to a \\nnumber of statistical techniques for estimating the \\nrelationship between input variables and their',\n",
       " 'kernel trick. \\n \\n \\n \\n4. Regression analysis \\nRegression analysis is a broad term that refers to a \\nnumber of statistical techniques for estimating the \\nrelationship between input variables and their \\nassociated characteristics. Linear regression is the \\nmost frequent type, in which a single line is \\ngenerated to best match the available data using a \\nmathematical criterion such ordinary least squares.  \\n \\n \\n5. Bayesian network \\nA Bayesian network, also known as a belief network \\nor a directed acyclic graphical model, is a \\nprobabilistic graphical model that uses a directed \\nacyclic graph to describe a set of random variables \\nand their conditional independence (DAG).  A \\nBayesian network, for example, could be used to \\nillustrate \\nthe \\nprobability \\ncorrelations \\nbetween \\ndiseases and symptoms. The network may be used to \\ncalculate the chances of certain diseases being present \\nbased on symptoms. There are efficient algorithms \\nfor inference and learning. Dynamic Bayesian \\nnetworks',\n",
       " \"calculate the chances of certain diseases being present \\nbased on symptoms. There are efficient algorithms \\nfor inference and learning. Dynamic Bayesian \\nnetworks \\nare \\nBayesian \\nnetworks \\nthat \\nmodel \\nsequences of variables, such as speech signals or \\nprotein sequences.  \\n \\n \\n \\nV. CONCLUSION \\n \\nThis study examines a number of different machine \\nlearning algorithms. Today, everyone, intentionally \\nor unconsciously, employs machine learning. From \\nonline shopping for a recommended product to \\nuploading images on social networking sites, there's a \\nlot to do. This document provides an overview of the \\nmost widely used machine learning algorithms. \\n \\nVI. REFERENCES \\n \\n[1]. \\nR. Praba, “A Study on Data Science Basics with \\nPython Concepts”, Volume-4, Issue-14, ISSN: \\n2582-3930.\",\n",
       " 'Volume 7, Issue 4, July-August-2021 | http://ijsrcseit.com \\nPraba. R et al Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, July-August-2021, 7 (4) : 67-72 \\n \\n \\n \\n72 \\n[2]. \\nW. Richert, L. P. Coelho, “Building Machine \\nLearning \\nSystems \\nwith \\nPython”, \\nPackt \\nPublishing Ltd., ISBN 978-1-78216-140-0 \\n[3]. \\nM. \\nWelling, \\n“A \\nFirst \\nEncounter \\nwith \\nMachineLearning” \\n[4]. \\nM. Bowles, “Machine Learning in Python: \\nEssential Techniques for Predictive Analytics”, \\nJohn Wiley & Sons Inc., ISBN: 978-1-118-\\n96174-2 \\n[5]. \\nS.B. Kotsiantis, “Supervised Machine Learning: \\nA \\nReview \\nof \\nClassification \\nTechniques”, \\nInformatica 31 (2007) 249-268 \\n[6]. \\nL. Rokach, O. Maimon, “Top – Down \\nInduction of Decision Trees Classifiers – A \\nSurvey”, IEEE Transactions on Systems, \\n[7]. \\nD. Lowd, P. Domingos, “Naïve Bayes Models \\nfor Probability Estimation” \\n[8]. \\nhttps://webdocs.cs.ualberta.ca/~greiner/C- \\n651/Homework2_Fall2008.html \\n[9]. \\nD. Meyer, “Support Vector Machines – The',\n",
       " 'D. Lowd, P. Domingos, “Naïve Bayes Models \\nfor Probability Estimation” \\n[8]. \\nhttps://webdocs.cs.ualberta.ca/~greiner/C- \\n651/Homework2_Fall2008.html \\n[9]. \\nD. Meyer, “Support Vector Machines – The \\nInterface to libsvm in package e1071”, August \\n2015 \\n[10]. S. S. Shwartz, Y. Singer, N. Srebro, “Pegasos: \\nPrimal Estimated sub - Gradient Solver for \\nSVM”, \\nProceedings \\nof \\nthe \\n24th \\nInternationalConference on Machine Learning, \\nCorvallis, OR, 2007 \\n[11]. http://www.simplilearn.com/what-is-machine-\\nlearning-and-why-itmatters- article \\n[12]. P. Harrington, “Machine Learning in action”, \\nManning Publications Co., Shelter Island, New \\nYork, 2012 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCite this article as : \\n \\nPraba. R, Darshan. G, Roshanraj. K. T, Surya Prakash. \\nB, \"Study On Machine Learning Algorithms\", \\nInternational Journal of Scientific Research in \\nComputer Science, Engineering and Information \\nTechnology (IJSRCSEIT), ISSN : 2456-3307, Volume \\n7 Issue 4, pp. 67-72, July-August 2021. Available at',\n",
       " 'International Journal of Scientific Research in \\nComputer Science, Engineering and Information \\nTechnology (IJSRCSEIT), ISSN : 2456-3307, Volume \\n7 Issue 4, pp. 67-72, July-August 2021. Available at \\ndoi : https://doi.org/10.32628/CSEIT2173105         \\n  \\nJournal URL : https://ijsrcseit.com/CSEIT2173105',\n",
       " 'Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\u2002 |\\u2002 357\\nReview\\nArray programming with NumPy\\nCharles R. Harris1, K. Jarrod Millman2,3,4\\u2009✉, Stéfan J. van\\xa0der Walt2,4,5\\u2009✉, Ralf Gommers6\\u2009✉, \\nPauli Virtanen7,8, David Cournapeau9, Eric Wieser10, Julian Taylor11, Sebastian Berg4, \\nNathaniel J. Smith12, Robert Kern13, Matti Picus4, Stephan Hoyer14, Marten H. van Kerkwijk15, \\nMatthew Brett2,16, Allan Haldane17, Jaime Fernández del Río18, Mark Wiebe19,20,  \\nPearu Peterson6,21,22, Pierre Gérard-Marchant23,24, Kevin Sheppard25, Tyler Reddy26,  \\nWarren Weckesser4, Hameer Abbasi6, Christoph Gohlke27 & Travis E. Oliphant6\\nArray programming provides a powerful, compact and expressive syntax for \\naccessing, manipulating and operating on data in vectors, matrices and \\nhigher-dimensional arrays. NumPy is the primary array programming library for the \\nPython language. It has an essential role in research analysis pipelines in fields as',\n",
       " 'higher-dimensional arrays. NumPy is the primary array programming library for the \\nPython language. It has an essential role in research analysis pipelines in fields as \\ndiverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials \\nscience, engineering, finance and economics. For example, in astronomy, NumPy was \\nan important part of the software stack used in the discovery of gravitational waves1 \\nand in the first imaging of a black hole2. Here we review how a few fundamental array \\nconcepts lead to a simple and powerful programming paradigm for organizing, \\nexploring and analysing scientific data. NumPy is the foundation upon which the \\nscientific Python ecosystem is constructed. It is so pervasive that several projects, \\ntargeting audiences with specialized needs, have developed their own NumPy-like \\ninterfaces and array objects. Owing to its central position in the ecosystem, NumPy',\n",
       " 'targeting audiences with specialized needs, have developed their own NumPy-like \\ninterfaces and array objects. Owing to its central position in the ecosystem, NumPy \\nincreasingly acts as an interoperability layer between such array computation \\nlibraries and, together with its application programming interface (API), provides a \\nflexible framework to support the next decade of scientific and industrial analysis.\\nTwo Python array packages existed before NumPy. The Numeric pack-\\nage was developed in the mid-1990s and provided array objects and \\narray-aware functions in Python. It was written in C and linked to stand-\\nard fast implementations of linear algebra3,4. One of its earliest uses was \\nto steer C++ applications for inertial confinement fusion research at \\nLawrence Livermore National Laboratory5. To handle large astronomi-\\ncal images coming from the Hubble Space Telescope, a reimplementa-\\ntion of Numeric, called Numarray, added support for structured arrays,',\n",
       " 'cal images coming from the Hubble Space Telescope, a reimplementa-\\ntion of Numeric, called Numarray, added support for structured arrays, \\nflexible indexing, memory mapping, byte-order variants, more efficient \\nmemory use, flexible IEEE 754-standard error-handling capabilities, and \\nbetter type-casting rules6. Although Numarray was highly compatible \\nwith Numeric, the two packages had enough differences that it divided \\nthe community; however, in 2005 NumPy emerged as a ‘best of both \\nworlds’ unification7—combining the features of Numarray with the \\nsmall-array performance of Numeric and its rich C API.\\nNow, 15 years later, NumPy underpins almost every Python library \\nthat does scientific or numerical computation8–11, including SciPy12, \\nMatplotlib13, pandas14, scikit-learn15 and scikit-image16. NumPy is a \\ncommunity-developed, open-source library, which provides a mul-\\ntidimensional Python array object along with array-aware functions',\n",
       " 'community-developed, open-source library, which provides a mul-\\ntidimensional Python array object along with array-aware functions \\nthat operate on it. Because of its inherent simplicity, the NumPy array \\nis the de facto exchange format for array data in Python.\\nNumPy operates on in-memory arrays using the central processing \\nunit (CPU). To utilize modern, specialized storage and hardware, there \\nhas been a recent proliferation of Python array packages. Unlike with \\nthe Numarray–Numeric divide, it is now much harder for these new \\nlibraries to fracture the user community—given how much work is \\nalready built on top of NumPy. However, to provide the community with \\naccess to new and exploratory technologies, NumPy is transitioning \\ninto a central coordinating mechanism that specifies a well defined \\narray programming API and dispatches it, as appropriate, to special-\\nized array implementations.\\nNumPy arrays\\nThe NumPy array is a data structure that efficiently stores and accesses',\n",
       " 'array programming API and dispatches it, as appropriate, to special-\\nized array implementations.\\nNumPy arrays\\nThe NumPy array is a data structure that efficiently stores and accesses \\nmultidimensional arrays17 (also known as tensors), and enables a wide \\nvariety of scientific computation. It consists of a pointer to memory, \\nalong with metadata used to interpret the data stored there, notably \\n‘data type’, ‘shape’ and ‘strides’ (Fig.\\xa01a).\\nhttps://doi.org/10.1038/s41586-020-2649-2\\nReceived: 21 February 2020\\nAccepted: 17 June 2020\\nPublished online: 16 September 2020\\nOpen access\\n Check for updates\\n1Independent researcher, Logan, UT, USA. 2Brain Imaging Center, University of California, Berkeley, Berkeley, CA, USA. 3Division of Biostatistics, University of California, Berkeley, Berkeley, CA, \\nUSA. 4Berkeley Institute for Data Science, University of California, Berkeley, Berkeley, CA, USA. 5Applied Mathematics, Stellenbosch University, Stellenbosch, South Africa. 6Quansight, Austin,',\n",
       " 'USA. 4Berkeley Institute for Data Science, University of California, Berkeley, Berkeley, CA, USA. 5Applied Mathematics, Stellenbosch University, Stellenbosch, South Africa. 6Quansight, Austin, \\nTX, USA. 7Department of Physics, University of Jyväskylä, Jyväskylä, Finland. 8Nanoscience Center, University of Jyväskylä, Jyväskylä, Finland. 9Mercari JP, Tokyo, Japan. 10Department of \\nEngineering, University of Cambridge, Cambridge, UK. 11Independent researcher, Karlsruhe, Germany. 12Independent researcher, Berkeley, CA, USA. 13Enthought, Austin, TX, USA. 14Google \\nResearch, Mountain View, CA, USA. 15Department of Astronomy and Astrophysics, University of Toronto, Toronto, Ontario, Canada. 16School of Psychology, University of Birmingham, \\nEdgbaston, Birmingham, UK. 17Department of Physics, Temple University, Philadelphia, PA, USA. 18Google, Zurich, Switzerland. 19Department of Physics and Astronomy, The University of',\n",
       " 'Edgbaston, Birmingham, UK. 17Department of Physics, Temple University, Philadelphia, PA, USA. 18Google, Zurich, Switzerland. 19Department of Physics and Astronomy, The University of \\nBritish Columbia, Vancouver, British Columbia, Canada. 20Amazon, Seattle, WA, USA. 21Independent researcher, Saue, Estonia. 22Department of Mechanics and Applied Mathematics, Institute \\nof Cybernetics at Tallinn Technical University, Tallinn, Estonia. 23Department of Biological and Agricultural Engineering, University of Georgia, Athens, GA, USA. 24France-IX Services, Paris, \\nFrance. 25Department of Economics, University of Oxford, Oxford, UK. 26CCS-7, Los Alamos National Laboratory, Los Alamos, NM, USA. 27Laboratory for Fluorescence Dynamics, Biomedical \\nEngineering Department, University of California, Irvine, Irvine, CA, USA. ✉e-mail: millman@berkeley.edu; stefanv@berkeley.edu; ralf.gommers@gmail.com',\n",
       " '358\\u2002 |\\u2002 Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\nReview\\nThe data type describes the nature of elements stored in an array. \\nAn array has a single data type, and each element of an array occupies \\nthe same number of bytes in memory. Examples of data types include \\nreal and complex numbers (of lower and higher precision), strings, \\ntimestamps and pointers to Python objects.\\nThe shape of an array determines the number of elements along \\neach axis, and the number of axes is the dimensionality of the array. \\nFor example, a vector of numbers can be stored as a one-dimensional \\narray of shape N, whereas colour videos are four-dimensional arrays \\nof shape (T,\\xa0M,\\xa0N,\\xa03).\\nStrides are necessary to interpret computer memory, which stores \\nelements linearly, as multidimensional arrays. They describe the num-\\nber of bytes to move forward in memory to jump from row to row, col-\\numn to column, and so forth. Consider, for example, a two-dimensional',\n",
       " 'ber of bytes to move forward in memory to jump from row to row, col-\\numn to column, and so forth. Consider, for example, a two-dimensional \\narray of floating-point numbers with shape (4,\\xa03), where each element \\noccupies 8\\xa0bytes in memory. To move between consecutive columns, \\nwe need to jump forward 8\\xa0bytes in memory, and to access the next row, \\n3\\xa0×\\xa08\\xa0=\\xa024\\xa0bytes. The strides of that array are therefore (24,\\xa08). NumPy \\ncan store arrays in either C or Fortran memory order, iterating first over \\neither rows or columns. This allows external libraries written in those \\nlanguages to access NumPy array data in memory directly.\\nUsers interact with NumPy arrays using ‘indexing’ (to access sub-\\narrays or individual elements), ‘operators’ (for example, +, − and × \\nfor vectorized operations and @ for matrix multiplication), as well \\nas ‘array-aware functions’; together, these provide an easily readable, \\nexpressive, high-level API for array programming while NumPy deals',\n",
       " 'as ‘array-aware functions’; together, these provide an easily readable, \\nexpressive, high-level API for array programming while NumPy deals \\nwith the underlying mechanics of making operations fast.\\nIndexing an array returns single elements, subarrays or elements \\nthat satisfy a specific condition (Fig.\\xa01b). Arrays can even be indexed \\nusing other arrays (Fig.\\xa01c). Wherever possible, indexing that retrieves a \\nsubarray returns a ‘view’ on the original array such that data are shared \\nbetween the two arrays. This provides a powerful way to operate on \\nsubsets of array data while limiting memory usage.\\nTo complement the array syntax, NumPy includes functions that \\nperform vectorized calculations on arrays, including arithmetic, \\nstatistics and trigonometry (Fig.\\xa01d). Vectorization—operating on \\nentire arrays rather than their individual elements—is essential to array \\nprogramming. This means that operations that would take many tens',\n",
       " 'entire arrays rather than their individual elements—is essential to array \\nprogramming. This means that operations that would take many tens \\nof lines to express in languages such as C can often be implemented as \\na single, clear Python expression. This results in concise code and frees \\nusers to focus on the details of their analysis, while NumPy handles \\nlooping over array elements near-optimally—for example, taking \\nstrides into consideration to best utilize the computer’s fast cache \\nmemory.\\nWhen performing a vectorized operation (such as addition) on two \\narrays with the same shape, it is clear what should happen. Through \\n‘broadcasting’ NumPy allows the dimensions to differ, and produces \\nresults that appeal to intuition. A trivial example is the addition of a \\nscalar value to an array, but broadcasting also generalizes to more com-\\nplex examples such as scaling each column of an array or generating \\na grid of coordinates. In broadcasting, one or both arrays are virtually',\n",
       " 'plex examples such as scaling each column of an array or generating \\na grid of coordinates. In broadcasting, one or both arrays are virtually \\nduplicated (that is, without copying any data in memory), so that the \\nshapes of the operands match (Fig.\\xa01d). Broadcasting is also applied \\nwhen an array is indexed using arrays of indices (Fig.\\xa01c).\\nOther array-aware functions, such as sum, mean and maximum, \\nperform element-by-element ‘reductions’, aggregating results across \\none, multiple or all axes of a single array. For example, summing an \\nn-dimensional array over d axes results in an array of dimension n\\xa0−\\xa0d \\n(Fig.\\xa01f).\\nNumPy also includes array-aware functions for creating, reshaping, \\nconcatenating and padding arrays; searching, sorting and counting \\ndata; and reading and writing files. It provides extensive support for \\ngenerating pseudorandom numbers, includes an assortment of prob-\\nability distributions, and performs accelerated linear algebra, using',\n",
       " 'generating pseudorandom numbers, includes an assortment of prob-\\nability distributions, and performs accelerated linear algebra, using \\none of several backends such as OpenBLAS18,19 or Intel MKL optimized \\nfor the CPUs at hand (see Supplementary Methods for more details).\\nAltogether, the combination of a simple in-memory array repre-\\nsentation, a syntax that closely mimics mathematics, and a variety \\nof array-aware utility functions forms a productive and powerfully \\nexpressive array programming language.\\nIn [1]: import numpy as np\\nIn [2]: x = np.arange(12)\\nIn [3]: x = x.reshape(4, 3)\\nIn [4]: x\\nOut[4]:\\narray([[ 0,  1,  2],\\n       [ 3,  4,  5],\\n       [ 6,  7,  8],\\n       [ 9, 10, 11]])\\nIn [5]: np.mean(x, axis=0)\\nOut[5]: array([4.5, 5.5, 6.5])\\nIn [6]: x = x - np.mean(x, axis=0)\\nIn [7]: x\\nOut[7]:\\narray([[-4.5, -4.5, -4.5],\\n       [-1.5, -1.5, -1.5],\\n       [ 1.5,  1.5,  1.5],\\n       [ 4.5,  4.5,  4.5]])\\na Data structure\\ng Example\\nx =\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10 11\\ndata\\ndata type\\nshape',\n",
       " 'Out[7]:\\narray([[-4.5, -4.5, -4.5],\\n       [-1.5, -1.5, -1.5],\\n       [ 1.5,  1.5,  1.5],\\n       [ 4.5,  4.5,  4.5]])\\na Data structure\\ng Example\\nx =\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10 11\\ndata\\ndata type\\nshape\\nstrides\\n8-byte integer\\n(4, 3)\\n(24, 8)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n8\\n9 10 11\\n8 bytes\\nper element\\n3 × 8 = 24 bytes\\nto jump one\\nrow down\\nb Indexing (view)\\n10 11\\n9\\nx[:,1:] →\\nwith slices\\n1\\n2\\n4 5\\n7 8\\n0\\n3\\n6\\nx[:,::2]→\\nwith slices\\nwith steps\\n0\\n2\\n3\\n5\\n6\\n8\\n9\\n11\\n0 1\\n2\\n3 4\\n5\\n6\\n7 8\\n9 10\\n10 11\\nSlices are start:end:step,\\nany of which can be left blank\\nd Vectorization\\n+\\n→\\n0\\n1\\n3\\n4\\n6\\n7\\n9 10\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n4\\n5\\n7\\n8\\n10 11\\ne Broadcasting\\n×\\n3\\n6\\n0\\n9\\n1\\n2\\n→\\n0\\n0\\n3\\n6\\n6 12\\n9 18\\nf Reduction\\n0\\n1\\n3\\n4\\n6\\n7\\n9 10\\n2\\n5\\n8\\n11\\n3\\n12\\n21\\n30\\nsum\\naxis 1\\n18 22 26\\nsum\\naxis 0\\n66\\nsum\\naxis (0,1)\\nc Indexing (copy)\\n4\\n3\\n7\\n6\\nwith arrays\\nwith broadcasting\\n→\\nx\\n→\\n,\\n2\\n1\\n1\\n0\\nx\\n,\\n1 1\\n2 2\\n1 0\\n1 0\\nx \\nwith arrays\\nx[0,1],x[1,2]\\n1\\n5\\n→\\n→\\n0\\n1\\n1\\n2\\n,\\nx[x > 9]\\nwith masks\\n10 11\\n→\\n→5\\nwith scalars\\nx[1,2]',\n",
       " 'c Indexing (copy)\\n4\\n3\\n7\\n6\\nwith arrays\\nwith broadcasting\\n→\\nx\\n→\\n,\\n2\\n1\\n1\\n0\\nx\\n,\\n1 1\\n2 2\\n1 0\\n1 0\\nx \\nwith arrays\\nx[0,1],x[1,2]\\n1\\n5\\n→\\n→\\n0\\n1\\n1\\n2\\n,\\nx[x > 9]\\nwith masks\\n10 11\\n→\\n→5\\nwith scalars\\nx[1,2] \\nFig. 1 | The NumPy array incorporates several fundamental array concepts. \\na, The NumPy array data structure and its associated metadata fields.  \\nb, Indexing an array with slices and steps. These operations return a ‘view’ of \\nthe original data. c, Indexing an array with masks, scalar coordinates or other \\narrays, so that it returns a ‘copy’ of the original data. In the bottom example, an \\narray is indexed with other arrays; this broadcasts the indexing arguments \\nbefore performing the lookup. d, Vectorization efficiently applies operations \\nto groups of elements. e, Broadcasting in the multiplication of two-dimensional \\narrays. f, Reduction operations act along one or more axes. In this example,  \\nan array is summed along select axes to produce a vector, or along two axes',\n",
       " 'arrays. f, Reduction operations act along one or more axes. In this example,  \\nan array is summed along select axes to produce a vector, or along two axes \\nconsecutively to produce a scalar. g, Example NumPy code, illustrating some of \\nthese concepts.',\n",
       " 'Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\u2002 |\\u2002 359\\nScientific Python ecosystem\\nPython is an open-source, general-purpose interpreted programming \\nlanguage well suited to standard programming tasks such as cleaning \\ndata, interacting with web resources and parsing text. Adding fast array \\noperations and linear algebra enables scientists to do all their work \\nwithin a single programming language—one that has the advantage of \\nbeing famously easy to learn and teach, as witnessed by its adoption \\nas a primary learning language in many universities.\\nEven though NumPy is not part of Python’s standard library, it ben-\\nefits from a good relationship with the Python developers. Over the \\nyears, the Python language has added new features and special syntax \\nso that NumPy would have a more succinct and easier-to-read array \\nnotation. However, because it is not part of the standard library, NumPy \\nis able to dictate its own release policies and development patterns.',\n",
       " 'notation. However, because it is not part of the standard library, NumPy \\nis able to dictate its own release policies and development patterns.\\nSciPy and Matplotlib are tightly coupled with NumPy in terms of his-\\ntory, development and use. SciPy provides fundamental algorithms for \\nscientific computing, including mathematical, scientific and engineer-\\ning routines. Matplotlib generates publication-ready figures and visu-\\nalizations. The combination of NumPy, SciPy and Matplotlib, together \\nwith an advanced interactive environment such as IPython20 or Jupy-\\nter21, provides a solid foundation for array programming in Python. The \\nscientific Python ecosystem (Fig.\\xa02) builds on top of this foundation to \\nprovide several, widely used technique-specific libraries15,16,22, that in \\nturn underlie numerous domain-specific projects23–28. NumPy, at the \\nbase of the ecosystem of array-aware libraries, sets documentation \\nstandards, provides array testing infrastructure and adds build sup-',\n",
       " 'base of the ecosystem of array-aware libraries, sets documentation \\nstandards, provides array testing infrastructure and adds build sup-\\nport for Fortran and other compilers.\\nMany research groups have designed large, complex scientific librar-\\nies that add application-specific functionality to the ecosystem. For \\nexample, the eht-imaging library29, developed by the Event Horizon \\nTelescope collaboration for radio interferometry imaging, analysis \\nand simulation, relies on many lower-level components of the scientific \\nPython ecosystem. In particular, the EHT collaboration used this library \\nfor the first imaging of a black hole. Within eht-imaging, NumPy arrays \\nare used to store and manipulate numerical data at every step in the \\nprocessing chain: from raw data through calibration and image recon-\\nstruction. SciPy supplies tools for general image-processing tasks such \\nas filtering and image alignment, and scikit-image, an image-processing',\n",
       " 'struction. SciPy supplies tools for general image-processing tasks such \\nas filtering and image alignment, and scikit-image, an image-processing \\nlibrary that extends SciPy, provides higher-level functionality such \\nas edge filters and Hough transforms. The ‘scipy.optimize’ module \\nperforms mathematical optimization. NetworkX22, a package for com-\\nplex network analysis, is used to verify image comparison consistency. \\nAstropy23,24 handles standard astronomical file formats and computes \\ntime–coordinate transformations. Matplotlib is used to visualize data \\nand to generate the final image of the black hole.\\nThe interactive environment created by the array\\xa0programming foun-\\ndation and the surrounding ecosystem of tools—inside of IPython or \\nJupyter—is ideally suited to exploratory data analysis. Users can fluidly \\ninspect, manipulate and visualize their data, and rapidly iterate to refine \\nprogramming statements. These statements are then stitched together',\n",
       " 'inspect, manipulate and visualize their data, and rapidly iterate to refine \\nprogramming statements. These statements are then stitched together \\ninto imperative or functional programs, or notebooks containing both \\ncomputation and narrative. Scientific computing beyond exploratory \\nwork is often done in a text editor or an integrated development envi-\\nronment (IDE) such as Spyder. This rich and productive environment \\nhas made Python popular for scientific research.\\nTo complement this facility for exploratory work and rapid proto-\\ntyping, NumPy has developed a culture of using time-tested software \\nengineering practices to improve collaboration and reduce error30. This \\nculture is not only adopted by leaders in the project but also enthusi-\\nastically taught to newcomers. The NumPy team was early to adopt \\ndistributed revision control and code review to improve collaboration \\ncantera\\nChemistry\\nBiopython\\nBiology\\nAstropy\\nAstronomy\\nsimpeg\\nGeophysics\\nNLTK\\nLinguistics\\nQuantEcon\\nEconomics',\n",
       " 'distributed revision control and code review to improve collaboration \\ncantera\\nChemistry\\nBiopython\\nBiology\\nAstropy\\nAstronomy\\nsimpeg\\nGeophysics\\nNLTK\\nLinguistics\\nQuantEcon\\nEconomics\\nSciPy\\nAlgorithms\\nMatplotlib\\nPlots\\nscikit-learn\\nMachine learning\\nNetworkX\\nNetwork analysis\\npandas, statsmodels\\nStatistics\\nscikit-image\\nImage processing\\nPsychoPy\\nkhmer\\nQiime2\\nFiPy\\ndeepchem\\nlibrosa\\nPyWavelets\\nSunPy\\nQuTiP\\nyt\\nnibabel\\nyellowbrick\\nmne-python \\nscikit-HEP\\neht-imaging\\nMDAnalysis\\niris\\ncesium\\nPyChrono\\nFoundation\\nApplication-speciﬁc\\nDomain-speciﬁc\\nTechnique-speciﬁc\\nArray Protocols\\nNumPy API\\nPython\\nLanguage\\nIPython / Jupyter\\nInteractive environments\\nNumPy\\nArrays\\nNew array implementations\\nFig. 2 | NumPy is the base of the scientific Python ecosystem. Essential libraries and projects that depend on NumPy’s API gain access to new array \\nimplementations that support NumPy’s array protocols (Fig.\\xa03).',\n",
       " '360\\u2002 |\\u2002 Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\nReview\\non code, and continuous testing that runs an extensive battery of auto-\\nmated tests for every proposed change to NumPy. The project also \\nhas comprehensive, high-quality documentation, integrated with the \\nsource code31–33.\\nThis culture of using best practices for producing reliable scientific \\nsoftware has been adopted by the ecosystem of libraries that build on \\nNumPy. For example, in a recent award given by the Royal Astronomi-\\ncal Society to Astropy, they state: “The Astropy Project has provided \\nhundreds of junior scientists with experience in professional-standard \\nsoftware development practices including use of version control, unit \\ntesting, code review and issue tracking procedures. This is a vital skill \\nset for modern researchers that is often missing from formal university \\neducation in physics or astronomy”34. Community members explicitly \\nwork to address this lack of formal education through courses and',\n",
       " 'education in physics or astronomy”34. Community members explicitly \\nwork to address this lack of formal education through courses and \\nworkshops35–37.\\nThe recent rapid growth of data science, machine learning and arti-\\nficial intelligence has further and dramatically boosted the scientific \\nuse of Python. Examples of its important applications, such as the \\neht-imaging library, now exist in almost every discipline in the natu-\\nral and social sciences. These tools have become the primary software \\nenvironment in many fields. NumPy and its ecosystem are commonly \\ntaught in university courses, boot camps and summer schools, and \\nare the focus of community conferences and workshops worldwide. \\nNumPy and its API have become truly ubiquitous.\\nArray proliferation and interoperability\\nNumPy provides in-memory, multidimensional, homogeneously typed \\n(that is, single-pointer and strided) arrays on CPUs. It runs on machines \\nranging from embedded devices to the world’s largest supercomputers,',\n",
       " '(that is, single-pointer and strided) arrays on CPUs. It runs on machines \\nranging from embedded devices to the world’s largest supercomputers, \\nwith performance approaching that of compiled languages. For most \\nits existence, NumPy addressed the vast majority of array computa-\\ntion use cases.\\nHowever, scientific datasets now routinely exceed the memory capac-\\nity of a single machine and may be stored on multiple machines or in \\nthe cloud. In addition, the recent need to accelerate deep-learning and \\nartificial intelligence applications has led to the emergence of special-\\nized accelerator hardware, including graphics processing units (GPUs), \\ntensor processing units (TPUs) and field-programmable gate arrays \\n(FPGAs). Owing to its in-memory data model, NumPy is currently unable \\nto directly utilize such storage and specialized hardware. However, \\nboth distributed data and also the parallel execution of GPUs, TPUs \\nand FPGAs map well to the paradigm of array programming: therefore',\n",
       " 'both distributed data and also the parallel execution of GPUs, TPUs \\nand FPGAs map well to the paradigm of array programming: therefore \\nleading to a gap between available modern hardware architectures and \\nthe tools necessary to leverage their computational power.\\nThe community’s efforts to fill this gap led to a proliferation of new \\narray implementations. For example, each deep-learning framework \\ncreated its own arrays; the PyTorch38, Tensorflow39, Apache MXNet40 \\nand JAX arrays all have the capability to run on CPUs and GPUs in a \\ndistributed fashion, using lazy evaluation to allow for additional per-\\nformance optimizations. SciPy and PyData/Sparse both provide sparse \\narrays, which typically contain few non-zero values and store only those \\nin memory for efficiency. In addition, there are projects that build on \\nNumPy arrays as data containers, and extend its capabilities. Distrib-\\nuted arrays are made possible that way by Dask, and labelled arrays—',\n",
       " \"NumPy arrays as data containers, and extend its capabilities. Distrib-\\nuted arrays are made possible that way by Dask, and labelled arrays—\\nreferring to dimensions of an array by name rather than by index for \\nclarity, compare x[:,\\xa01] versus x.loc[:,\\xa0'time']—by xarray41.\\nSuch libraries often mimic the NumPy API, because this lowers the \\nbarrier to entry for newcomers and provides the wider community with \\na stable array\\xa0programming interface. This, in turn, prevents disruptive \\nschisms such as the divergence between\\xa0Numeric and Numarray. But \\nexploring new ways of working with arrays is experimental by nature \\nand, in fact, several promising libraries (such as Theano and Caffe) have \\nalready ceased development. And each time that a user decides to try a \\nnew technology, they must change import statements and ensure that the \\nnew library implements all the parts of the NumPy API they currently use.\\nIdeally, operating on specialized arrays using NumPy functions or\",\n",
       " 'new library implements all the parts of the NumPy API they currently use.\\nIdeally, operating on specialized arrays using NumPy functions or \\nsemantics would simply work, so that users could write code once, \\nand would then benefit from switching between NumPy arrays, GPU \\narrays, distributed arrays and so forth as appropriate. To support array \\noperations between external array objects, NumPy therefore added \\nthe capability to act as a central coordination mechanism with a well \\nspecified API (Fig.\\xa02).\\nTo facilitate this interoperability, NumPy provides ‘protocols’ (or \\ncontracts of operation), that allow for specialized arrays to be passed to \\nNumPy functions (Fig.\\xa03). NumPy, in turn, dispatches operations to the \\noriginating library, as required. Over four hundred of the most popular \\nNumPy functions are supported. The protocols are implemented by \\nwidely used libraries such as Dask, CuPy, xarray and PyData/Sparse.',\n",
       " 'NumPy functions are supported. The protocols are implemented by \\nwidely used libraries such as Dask, CuPy, xarray and PyData/Sparse. \\nThanks to these developments, users can now, for example, scale their \\ncomputation from a single machine to distributed systems using Dask. \\nThe protocols also compose well, allowing users to redeploy NumPy \\ncode at scale on distributed, multi-GPU systems via, for instance, CuPy \\narrays embedded in Dask arrays. Using NumPy’s high-level API, users \\ncan leverage highly parallel code execution on multiple systems with \\nmillions of cores, all with minimal code changes42.\\nThese array protocols are now a key feature of NumPy, and are \\nexpected to only increase in importance. The NumPy developers—\\nmany of whom are authors of this Review—iteratively refine and add \\nprotocol designs to improve utility and simplify adoption.\\nOutput\\narrays\\nInput\\narrays\\nNumPy\\nAPI\\nnp.stack\\nnp.reshape\\nnp.transpose\\nnp.argmin\\nnp.mean\\nnp.std\\nnp.max\\nnp.cos\\nnp.arctan\\nnp.log\\nnp.cumsum',\n",
       " 'protocol designs to improve utility and simplify adoption.\\nOutput\\narrays\\nInput\\narrays\\nNumPy\\nAPI\\nnp.stack\\nnp.reshape\\nnp.transpose\\nnp.argmin\\nnp.mean\\nnp.std\\nnp.max\\nnp.cos\\nnp.arctan\\nnp.log\\nnp.cumsum\\nnp.diff\\n...\\nNumPy array protocols\\nIn [1]: import numpy as np\\nIn [2]: import dask.array as da\\nIn [3]: x = da.arange(12)\\nIn [4]: x = np.reshape(x, (4, 3))\\nIn [5]: x\\nOut[5]: dask.array<..., shape=(4, 3), ...>\\nIn [6]: np.mean(x, axis=0)\\nOut[6]: dask.array<..., shape=(3,), ...>\\nIn [7]: x = x - np.mean(x, axis=0)\\nIn [8]: x\\nOut[8]: dask.array<..., shape=(4, 3), ...>\\nArray\\nimplementation\\nNumPy\\nDask\\nCuPy\\nPyData/\\nSparse\\n...\\n...\\nDask\\nNumPy\\nCuPy\\nPyData\\nSparse\\n...\\nDask\\nNumPy\\nCuPy\\nPyData\\nSparse\\nFig. 3 | NumPy’s API and array protocols expose new arrays to the \\necosystem. In this example, NumPy’s ‘mean’ function is called on a Dask array. \\nThe call succeeds by dispatching to the appropriate library implementation (in \\nthis case, Dask) and results in a new Dask array. Compare this code to the',\n",
       " 'The call succeeds by dispatching to the appropriate library implementation (in \\nthis case, Dask) and results in a new Dask array. Compare this code to the \\nexample code in Fig.\\xa01g.',\n",
       " 'Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\u2002 |\\u2002 361\\nDiscussion\\nNumPy combines the expressive power of array programming, the \\nperformance of C, and the readability, usability and versatility of Python \\nin a mature, well tested, well documented and community-developed \\nlibrary. Libraries in the scientific Python ecosystem provide fast imple-\\nmentations of most important algorithms. Where extreme optimiza-\\ntion is warranted, compiled languages can be used, such as Cython43, \\nNumba44 and Pythran45; these languages extend Python and trans-\\nparently accelerate bottlenecks. Owing to NumPy’s simple memory \\nmodel, it is easy to write low-level, hand-optimized code, usually in C \\nor Fortran, to manipulate NumPy arrays and pass them back to Python. \\nFurthermore, using array protocols, it is possible to utilize the full \\nspectrum of specialized hardware acceleration with minimal changes \\nto existing code.\\nNumPy was initially developed by students, faculty and researchers',\n",
       " 'spectrum of specialized hardware acceleration with minimal changes \\nto existing code.\\nNumPy was initially developed by students, faculty and researchers \\nto provide an advanced, open-source array programming library for \\nPython, which was free to use and unencumbered by license servers and \\nsoftware protection dongles. There was a sense of building something \\nconsequential together for the benefit of many others. Participating \\nin such an endeavour, within a welcoming community of like-minded \\nindividuals, held a powerful attraction for many early contributors.\\nThese user–developers frequently had to write code from scratch \\nto solve their own or their colleagues’ problems—often in low-level \\nlanguages that preceded Python, such as Fortran46 and C. To them, \\nthe advantages of an interactive, high-level array library were evident. \\nThe design of this new tool was informed by other powerful interactive \\nprogramming languages for scientific computing such as Basis47–50,',\n",
       " 'The design of this new tool was informed by other powerful interactive \\nprogramming languages for scientific computing such as Basis47–50, \\nYorick51, R52 and APL53, as well as commercial languages and environ-\\nments such as IDL (Interactive Data Language) and MATLAB.\\nWhat began as an attempt to add an array object to Python became \\nthe foundation of a vibrant ecosystem of tools. Now, a large amount of \\nscientific work depends on NumPy being correct, fast and stable. It is \\nno longer a small community project, but core scientific infrastructure.\\nThe developer culture has matured: although initial development was \\nhighly informal, NumPy now has a roadmap and a process for propos-\\ning and discussing large changes. The project has formal governance \\nstructures and is fiscally sponsored by NumFOCUS, a nonprofit that \\npromotes open practices in research, data and scientific computing. \\nOver the past few years, the project attracted its first funded develop-',\n",
       " 'promotes open practices in research, data and scientific computing. \\nOver the past few years, the project attracted its first funded develop-\\nment, sponsored by the Moore and Sloan Foundations, and received \\nan award as part of the Chan Zuckerberg Initiative’s Essentials of Open \\nSource Software programme. With this funding, the project was (and \\nis) able to have sustained focus over multiple months to implement \\nsubstantial new features and improvements. That said, the develop-\\nment of NumPy still depends heavily on contributions made by gradu-\\nate students and researchers in their free time (see Supplementary \\nMethods for more details).\\nNumPy is no longer merely the foundational array library underlying \\nthe scientific Python ecosystem, but it has become the standard API for \\ntensor computation and a central coordinating mechanism between \\narray types and technologies in Python. Work continues to expand on \\nand improve these interoperability features.',\n",
       " 'tensor computation and a central coordinating mechanism between \\narray types and technologies in Python. Work continues to expand on \\nand improve these interoperability features.\\nOver the next decade, NumPy developers will face several challenges. \\nNew devices will be developed, and existing specialized hardware will \\nevolve to meet diminishing returns on Moore’s law. There will be more, \\nand a wider variety of, data science practitioners, a large proportion of \\nwhom will use NumPy. The scale of scientific data gathering will con-\\ntinue to increase, with the adoption of devices and instruments such \\nas light-sheet microscopes and the Large Synoptic Survey Telescope \\n(LSST)54. New generation languages, interpreters and compilers, such as \\nRust55, Julia56 and LLVM57, will create new concepts and data structures, \\nand determine their viability.\\nThrough the mechanisms described in this Review, NumPy is poised \\nto embrace such a changing landscape, and to continue playing a',\n",
       " 'and determine their viability.\\nThrough the mechanisms described in this Review, NumPy is poised \\nto embrace such a changing landscape, and to continue playing a \\nleading part in interactive scientific computation, although to do so \\nwill require sustained funding from government, academia and indus-\\ntry. But, importantly, for NumPy to meet the needs of the next decade \\nof data science, it will also need a new generation of graduate students \\nand community contributors to drive it forward.\\n1.\\t\\nAbbott, B. P. et\\xa0al. Observation of gravitational waves from a binary black hole merger. \\nPhys. Rev. Lett. 116, 061102 (2016).\\n2.\\t\\nChael, A. et\\xa0al. High-resolution linear polarimetric imaging for the Event Horizon \\nTelescope. Astrophys. J. 286, 11 (2016).\\n3.\\t\\nDubois, P. F., Hinsen, K. & Hugunin, J. Numerical Python. Comput. Phys. 10, 262–267 (1996).\\n4.\\t\\nAscher, D., Dubois, P. F., Hinsen, K., Hugunin, J. & Oliphant, T. E. An Open Source Project:',\n",
       " '3.\\t\\nDubois, P. F., Hinsen, K. & Hugunin, J. Numerical Python. Comput. Phys. 10, 262–267 (1996).\\n4.\\t\\nAscher, D., Dubois, P. F., Hinsen, K., Hugunin, J. & Oliphant, T. E. An Open Source Project: \\nNumerical Python (Lawrence Livermore National Laboratory, 2001).\\n5.\\t\\nYang, T.-Y., Furnish, G. & Dubois, P. F. Steering object-oriented scientific computations. In \\nProc. TOOLS USA 97. Intl Conf. Technology of Object Oriented Systems and Languages \\n(eds Ege,\\xa0R., Singh,\\xa0M.\\xa0& Meyer,\\xa0B.) 112–119 (IEEE, 1997).\\n6.\\t\\nGreenfield, P., Miller, J. T., Hsu, J. & White, R. L. numarray: a new scientific array package \\nfor Python. In PyCon DC 2003 http://citeseerx.ist.psu.edu/viewdoc/download?d\\noi=10.1.1.112.9899 (2003).\\n7.\\t\\nOliphant, T. E. Guide to NumPy 1st edn (Trelgol Publishing, 2006).\\n8.\\t\\nDubois, P. F. Python: batteries included. Comput. Sci. Eng. 9, 7–9 (2007).\\n9.\\t\\nOliphant, T. E. Python for scientific computing. Comput. Sci. Eng. 9, 10–20 (2007).\\n10.',\n",
       " '8.\\t\\nDubois, P. F. Python: batteries included. Comput. Sci. Eng. 9, 7–9 (2007).\\n9.\\t\\nOliphant, T. E. Python for scientific computing. Comput. Sci. Eng. 9, 10–20 (2007).\\n10.\\t\\nMillman, K. J. & Aivazis, M. Python for scientists and engineers. Comput. Sci. Eng. 13, 9–12 \\n(2011).\\n11.\\t\\nPérez, F., Granger, B. E. & Hunter, J. D. Python: an ecosystem for scientific computing. \\nComput. Sci. Eng. 13, 13–21 (2011).  \\nExplains why the scientific Python ecosystem is a highly productive environment for \\nresearch.\\n12.\\t\\nVirtanen, P. et\\xa0al. SciPy 1.0—fundamental algorithms for scientific computing in Python. \\nNat. Methods 17, 261–272 (2020); correction 17, 352 (2020).  \\nIntroduces the SciPy library and includes a more detailed history of NumPy and SciPy.\\n13.\\t\\nHunter, J. D. Matplotlib: a 2D graphics environment. Comput. Sci. Eng. 9, 90–95 (2007).\\n14.\\t\\nMcKinney, W. Data structures for statistical computing in Python. In Proc. 9th Python in',\n",
       " '13.\\t\\nHunter, J. D. Matplotlib: a 2D graphics environment. Comput. Sci. Eng. 9, 90–95 (2007).\\n14.\\t\\nMcKinney, W. Data structures for statistical computing in Python. In Proc. 9th Python in \\nScience Conf. (eds van\\xa0der\\xa0Walt,\\xa0S.\\xa0& Millman,\\xa0K.\\xa0J.) 56–61 (2010).\\n15.\\t\\nPedregosa, F. et\\xa0al. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, \\n2825–2830 (2011).\\n16.\\t\\nvan\\xa0der Walt, S. et\\xa0al. scikit-image: image processing in Python. PeerJ 2, e453 (2014).\\n17.\\t\\nvan\\xa0der Walt, S., Colbert, S. C. & Varoquaux, G. The NumPy array: a structure for efficient \\nnumerical computation. Comput. Sci. Eng. 13, 22–30 (2011).  \\nDiscusses the NumPy array data structure with a focus on how it enables efficient \\ncomputation.\\n18.\\t\\nWang, Q., Zhang, X., Zhang, Y. & Yi, Q. AUGEM: automatically generate high performance \\ndense linear algebra kernels on x86 CPUs. In SC’13: Proc. Intl Conf. High Performance \\nComputing, Networking, Storage and Analysis 25 (IEEE, 2013).\\n19.',\n",
       " 'dense linear algebra kernels on x86 CPUs. In SC’13: Proc. Intl Conf. High Performance \\nComputing, Networking, Storage and Analysis 25 (IEEE, 2013).\\n19.\\t\\nXianyi, Z., Qian, W. & Yunquan, Z. Model-driven level 3 BLAS performance optimization \\non Loongson 3A processor. In 2012 IEEE 18th Intl Conf. Parallel and Distributed Systems \\n684–691 (IEEE, 2012).\\n20.\\t Pérez, F. & Granger, B. E. IPython: a system for interactive scientific computing. Comput. \\nSci. Eng. 9, 21–29 (2007).\\n21.\\t\\nKluyver, T. et\\xa0al. Jupyter Notebooks—a publishing format for reproducible computational \\nworkflows. In Positioning and Power in Academic Publishing: Players, Agents and Agendas \\n(eds Loizides,\\xa0F.\\xa0& Schmidt,\\xa0B.) 87–90 (IOS Press, 2016).\\n22.\\t Hagberg, A. A., Schult, D. A. & Swart, P. J. Exploring network structure, dynamics, and \\nfunction using NetworkX. In Proc. 7th Python in Science Conf. (eds Varoquaux,\\xa0G., \\nVaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 11–15 (2008).',\n",
       " 'function using NetworkX. In Proc. 7th Python in Science Conf. (eds Varoquaux,\\xa0G., \\nVaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 11–15 (2008).\\n23.\\t Astropy Collaboration et\\xa0al. Astropy: a community Python package for astronomy. Astron. \\nAstrophys. 558, A33 (2013).\\n24.\\t Price-Whelan, A. M. et\\xa0al. The Astropy Project: building an open-science project and \\nstatus of the v2.0 core package. Astron. J. 156, 123 (2018).\\n25.\\t Cock, P. J. et\\xa0al. Biopython: freely available Python tools for computational molecular \\nbiology and bioinformatics. Bioinformatics 25, 1422–1423 (2009).\\n26.\\t Millman, K. J. & Brett, M. Analysis of functional magnetic resonance imaging in Python. \\nComput. Sci. Eng. 9, 52–55 (2007).\\n27.\\t\\nThe SunPy Community et\\xa0al. SunPy—Python for solar physics. Comput. Sci. Discov. 8, \\n014009 (2015).\\n28.\\t Hamman, J., Rocklin, M. & Abernathy, R. Pangeo: a big-data ecosystem for scalable Earth \\nsystem science. In EGU General Assembly Conf. Abstracts 12146 (2018).',\n",
       " '014009 (2015).\\n28.\\t Hamman, J., Rocklin, M. & Abernathy, R. Pangeo: a big-data ecosystem for scalable Earth \\nsystem science. In EGU General Assembly Conf. Abstracts 12146 (2018).\\n29.\\t Chael, A. A. et\\xa0al. ehtim: imaging, analysis, and simulation software for radio \\ninterferometry. Astrophysics Source Code Library https://ascl.net/1904.004 (2019).\\n30.\\t Millman, K. J. & Pérez, F. Developing open source scientific practice. In Implementing \\nReproducible Research (eds Stodden,\\xa0V., Leisch,\\xa0F.\\xa0& Peng,\\xa0R.\\xa0D.) 149–183 (CRC Press, 2014). \\nDescribes the software engineering practices embraced by the NumPy and SciPy \\ncommunities with a focus on how these practices improve research.\\n31.\\t\\nvan\\xa0der Walt, S. The SciPy Documentation Project (technical overview). In Proc. 7th Python \\nin Science Conf. (SciPy 2008) (eds Varoquaux,\\xa0G., Vaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 27–28 (2008).\\n32.\\t Harrington, J. The SciPy Documentation Project. In Proc. 7th Python in Science',\n",
       " 'in Science Conf. (SciPy 2008) (eds Varoquaux,\\xa0G., Vaught,\\xa0T.\\xa0& Millman,\\xa0K.\\xa0J.) 27–28 (2008).\\n32.\\t Harrington, J. The SciPy Documentation Project. In Proc. 7th Python in Science \\nConference (SciPy 2008) (eds Varoquaux,\\xa0G., Vaught,\\xa0T.\\xa0& Millman, K.\\xa0J.) 33–35 (2008).\\n33.\\t Harrington, J. & Goldsmith, D. Progress report: NumPy and SciPy documentation in 2009. \\nIn Proc. 8th Python in Science Conf. (SciPy 2009) (eds Varoquaux,\\xa0G., van\\xa0der\\xa0Walt,\\xa0S.\\xa0& \\nMillman,\\xa0K.\\xa0J.) 84–87 (2009).\\n34.\\t Royal Astronomical Society Report of the RAS ‘A’ Awards Committee 2020: Astropy \\nProject: 2020 Group Achievement Award (A) https://ras.ac.uk/sites/default/files/2020-01/\\nGroup%20Award%20-%20Astropy.pdf (2020).\\n35.\\t Wilson, G. Software carpentry: getting scientists to write better code by making them \\nmore productive. Comput. Sci. Eng. 8, 66–69 (2006).',\n",
       " '362\\u2002 |\\u2002 Nature\\u2002 |\\u2002 Vol 585\\u2002 |\\u2002 17 September 2020\\nReview\\n36.\\t Hannay, J. E. et\\xa0al. How do scientists develop and use scientific software? In Proc. 2009 \\nICSE Workshop on Software Engineering for Computational Science and Engineering 1–8 \\n(IEEE, 2009).\\n37.\\t\\nMillman, K. J., Brett, M., Barnowski, R. & Poline, J.-B. Teaching computational \\nreproducibility for neuroimaging. Front. Neurosci. 12, 727 (2018).\\n38.\\t Paszke, A. et\\xa0al. Pytorch: an imperative style, high-performance deep learning library. In \\nAdvances in Neural Information Processing Systems 32 (eds Wallach,\\xa0H.\\xa0et al.) 8024–8035 \\n(Neural Information Processing Systems, 2019).\\n39.\\t Abadi, M. et\\xa0al. TensorFlow: a system for large-scale machine learning. In OSDI’16: Proc. \\n12th USENIX Conf. Operating Systems Design and Implementation (chairs Keeton, K. & \\nRoscoe, T.) 265–283 (USENIX Association, 2016).\\n40.\\t Chen, T. et\\xa0al. MXNet: a flexible and efficient machine learning library for heterogeneous',\n",
       " 'Roscoe, T.) 265–283 (USENIX Association, 2016).\\n40.\\t Chen, T. et\\xa0al. MXNet: a flexible and efficient machine learning library for heterogeneous \\ndistributed systems. Preprint at http://www.arxiv.org/abs/1512.01274 (2015).\\n41.\\t\\nHoyer, S. & Hamman, J. xarray: N–D labeled arrays and datasets in Python. J. Open Res. \\nSoftw. 5, 10 (2017).\\n42.\\t Entschev, P. Distributed multi-GPU computing with Dask, CuPy and RAPIDS. In EuroPython \\n2019 https://ep2019.europython.eu/media/conference/slides/\\nfX8dJsD-distributed-multi-gpu-computing-with-dask-cupy-and-rapids.pdf (2019).\\n43.\\t Behnel, S. et\\xa0al. Cython: the best of both worlds. Comput. Sci. Eng. 13, 31–39 (2011).\\n44.\\t Lam, S. K., Pitrou, A. & Seibert, S. Numba: a LLVM-based Python JIT compiler. In Proc. \\nSecond Workshop on the LLVM Compiler Infrastructure in HPC, LLVM ’15 7:1–7:6 (ACM, 2015).\\n45.\\t Guelton, S. et\\xa0al. Pythran: enabling static optimization of scientific Python programs. \\nComput. Sci. Discov. 8, 014001 (2015).',\n",
       " '45.\\t Guelton, S. et\\xa0al. Pythran: enabling static optimization of scientific Python programs. \\nComput. Sci. Discov. 8, 014001 (2015).\\n46.\\t Dongarra, J., Golub, G. H., Grosse, E., Moler, C. & Moore, K. Netlib and NA-Net: building a \\nscientific computing community. IEEE Ann. Hist. Comput. 30, 30–41 (2008).\\n47.\\t\\nBarrett, K. A., Chiu, Y. H., Painter, J. F., Motteler, Z. C. & Dubois, P. F. Basis System, Part I: \\nRunning a Basis Program—A Tutorial for Beginners UCRL-MA-118543, Vol.\\xa01 (Lawrence \\nLivermore National Laboratory 1995).\\n48.\\t Dubois, P. F. & Motteler, Z. Basis System, Part II: Basis Language Reference Manual \\nUCRL-MA-118543, Vol.\\xa02 (Lawrence Livermore National Laboratory, 1995).\\n49.\\t Chiu, Y. H. & Dubois, P. F. Basis System, Part III: EZN User Manual UCRL-MA-118543, Vol.\\xa03 \\n(Lawrence Livermore National Laboratory, 1995).\\n50.\\t Chiu, Y. H. & Dubois, P. F. Basis System, Part IV: EZD User Manual UCRL-MA-118543, Vol.\\xa04 \\n(Lawrence Livermore National Laboratory, 1995).\\n51.',\n",
       " '(Lawrence Livermore National Laboratory, 1995).\\n50.\\t Chiu, Y. H. & Dubois, P. F. Basis System, Part IV: EZD User Manual UCRL-MA-118543, Vol.\\xa04 \\n(Lawrence Livermore National Laboratory, 1995).\\n51.\\t\\nMunro, D. H. & Dubois, P. F. Using the Yorick interpreted language. Comput. Phys. 9, \\n609–615 (1995).\\n52.\\t Ihaka, R. & Gentleman, R. R: a language for data analysis and graphics. J. Comput. Graph. \\nStat. 5, 299–314 (1996).\\n53.\\t Iverson, K. E. A programming language. In Proc. 1962 Spring Joint Computer Conf. \\n345–351 (1962).\\n54.\\t Jenness, T. et\\xa0al. LSST data management software development practices and tools. In \\nProc. SPIE 10707, Software and Cyberinfrastructure for Astronomy V 1070709 (SPIE and \\nInternational Society for Optics and Photonics, 2018).\\n55.\\t Matsakis, N. D. & Klock, F. S. The Rust language. Ada Letters 34, 103–104 (2014).\\n56.\\t Bezanson, J., Edelman, A., Karpinski, S. & Shah, V. B. Julia: a fresh approach to numerical \\ncomputing. SIAM Rev. 59, 65–98 (2017).\\n57.',\n",
       " '56.\\t Bezanson, J., Edelman, A., Karpinski, S. & Shah, V. B. Julia: a fresh approach to numerical \\ncomputing. SIAM Rev. 59, 65–98 (2017).\\n57.\\t\\nLattner, C. & Adve, V. LLVM: a compilation framework for lifelong program analysis and \\ntransformation. In Proc. 2004 Intl Symp. Code Generation and Optimization (CGO’04) \\n75–88 (IEEE, 2004).\\nAcknowledgements We thank R.\\xa0Barnowski, P.\\xa0Dubois, M.\\xa0Eickenberg, and P.\\xa0Greenfield, who \\nsuggested text and provided helpful feedback on the manuscript. K.J.M. and S.J.v.d.W. were \\nfunded in part by the Gordon and Betty Moore Foundation through grant GBMF3834 and by \\nthe Alfred P. Sloan Foundation through grant 2013-10-27 to the University of California, \\nBerkeley. S.J.v.d.W., S.B., M.P. and W.W. were funded in part by the Gordon and Betty Moore \\nFoundation through grant GBMF5447 and by the Alfred P. Sloan Foundation through grant \\nG-2017-9960 to the University of California, Berkeley.',\n",
       " 'Foundation through grant GBMF5447 and by the Alfred P. Sloan Foundation through grant \\nG-2017-9960 to the University of California, Berkeley.\\nAuthor contributions K.J.M. and S.J.v.d.W. composed the manuscript with input from \\nothers. S.B., R.G., K.S., W.W., M.B. and T.R. contributed text. All authors contributed \\nsubstantial code, documentation and/or expertise to the NumPy project. All authors \\nreviewed the manuscript.\\nCompeting interests The authors declare no competing interests.\\nAdditional information\\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\\n2649-2.\\nCorrespondence and requests for materials should be addressed to K.J.M., S.J.v.W. or R.G.\\nPeer review information Nature thanks Edouard Duchesnay,\\xa0Alan Edelman and the other, \\nanonymous, reviewer(s) for their contribution to the peer review of this work.\\nReprints and permissions information is available at http://www.nature.com/reprints.',\n",
       " 'anonymous, reviewer(s) for their contribution to the peer review of this work.\\nReprints and permissions information is available at http://www.nature.com/reprints.\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \\npublished maps and institutional affiliations.\\nOpen Access This article is licensed under a Creative Commons Attribution \\n4.0 International License, which permits use, sharing, adaptation, distribution \\nand reproduction in any medium or format, as long as you give appropriate \\ncredit to the original author(s) and the source, provide a link to the Creative Commons license, \\nand indicate if changes were made. The images or other third party material in this article are \\nincluded in the article’s Creative Commons license, unless indicated otherwise in a credit line \\nto the material. If material is not included in the article’s Creative Commons license and your',\n",
       " 'included in the article’s Creative Commons license, unless indicated otherwise in a credit line \\nto the material. If material is not included in the article’s Creative Commons license and your \\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \\nneed to obtain permission directly from the copyright holder. To view a copy of this license, \\nvisit http://creativecommons.org/licenses/by/4.0/.\\n© The Author(s) 2020',\n",
       " '1\\npandas: a Foundational Python Library for Data\\nAnalysis and Statistics\\nWes McKinney\\n!\\nAbstract—In this paper we will discuss pandas, a Python library of rich\\ndata structures and tools for working with structured data sets common to\\nstatistics, ﬁnance, social sciences, and many other ﬁelds. The library provides\\nintegrated, intuitive routines for performing common data manipulations and\\nanalysis on such data sets. It aims to be the foundational layer for the future of\\nstatistical computing in Python. It serves as a strong complement to the existing\\nscientiﬁc Python stack while implementing and improving upon the kinds of data\\nmanipulation tools found in other statistical programming languages such as\\nR. In addition to detailing its design and features of pandas, we will discuss\\nfuture avenues of work and growth opportunities for statistics and data analysis\\napplications in the Python language.\\nIntroduction\\nPython is being used increasingly in scientiﬁc applications',\n",
       " 'future avenues of work and growth opportunities for statistics and data analysis\\napplications in the Python language.\\nIntroduction\\nPython is being used increasingly in scientiﬁc applications\\ntraditionally dominated by [R], [MATLAB], [Stata], [SAS],\\nother commercial or open-source research environments. The\\nmaturity and stability of the fundamental numerical li-\\nbraries ([NumPy], [SciPy], and others), quality of documenta-\\ntion, and availability of “kitchen-sink” distributions ([EPD],\\n[Pythonxy]) have gone a long way toward making Python\\naccessible and convenient for a broad audience. Additionally\\n[matplotlib] integrated with [IPython] provides an interactive\\nresearch and development environment with data visualization\\nsuitable for most users. However, adoption of Python for\\napplied statistical modeling has been relatively slow compared\\nwith other areas of computational science.\\nOne major issue for would-be statistical Python program-',\n",
       " 'applied statistical modeling has been relatively slow compared\\nwith other areas of computational science.\\nOne major issue for would-be statistical Python program-\\nmers in the past has been the lack of libraries implementing\\nstandard models and a cohesive framework for specifying\\nmodels. However, in recent years there have been signiﬁcant\\nnew developments in econometrics ([StaM]), Bayesian statis-\\ntics ([PyMC]), and machine learning ([SciL]), among others\\nﬁelds. However, it is still difﬁcult for many statisticians to\\nchoose Python over R given the domain-speciﬁc nature of the\\nR language and breadth of well-vetted open-source libraries\\navailable to R users ([CRAN]). In spite of this obstacle, we\\nbelieve that the Python language and the libraries and tools\\ncurrently available can be leveraged to make Python a superior\\nenvironment for data analysis and statistical computing.\\nAnother issue preventing many from using Python in the',\n",
       " 'currently available can be leveraged to make Python a superior\\nenvironment for data analysis and statistical computing.\\nAnother issue preventing many from using Python in the\\npast for data analysis applications has been the lack of rich data\\nstructures with integrated handling of metadata. By metadata\\nwe mean labeling information about data points. For example,\\nCorresponding author can be contacted at: wesmckinn@gmail.com.\\nc○2011 Wes McKinney\\na table or spreadsheet of data will likely have labels for the\\ncolumns and possibly also the rows. Alternately, some columns\\nin a table might be used for grouping and aggregating data into\\na pivot or contingency table. In the case of a time series data\\nset, the row labels could be time stamps. It is often necessary\\nto have the labeling information available to allow many kinds\\nof data manipulations, such as merging data sets or performing\\nan aggregation or “group by” operation, to be expressed in an',\n",
       " 'to have the labeling information available to allow many kinds\\nof data manipulations, such as merging data sets or performing\\nan aggregation or “group by” operation, to be expressed in an\\nintuitive and concise way. Domain-speciﬁc database languages\\nlike SQL and statistical languages like R and SAS have a\\nwealth of such tools. Until relatively recently, Python had few\\ntools providing the same level of richness and expressiveness\\nfor working with labeled data sets.\\nThe pandas library, under development since 2008, is\\nintended to close the gap in the richness of available data\\nanalysis tools between Python, a general purpose systems\\nand scientiﬁc computing language, and the numerous domain-\\nspeciﬁc statistical computing platforms and database lan-\\nguages. We not only aim to provide equivalent functionality\\nbut also implement many features, such as automatic data\\nalignment and hierarchical indexing, which are not readily\\navailable in such a tightly integrated way in any other libraries',\n",
       " 'but also implement many features, such as automatic data\\nalignment and hierarchical indexing, which are not readily\\navailable in such a tightly integrated way in any other libraries\\nor computing environments to our knowledge. While initially\\ndeveloped for ﬁnancial data analysis applications, we hope that\\npandas will enable scientiﬁc Python to be a more attractive\\nand practical statistical computing environment for academic\\nand industry practitioners alike. The library’s name derives\\nfrom panel data, a common term for multidimensional data\\nsets encountered in statistics and econometrics.\\nWhile we offer a vignette of some of the main features of\\ninterest in pandas, this paper is by no means comprehensive.\\nFor more, we refer the interested reader to the online docu-\\nmentation at http://pandas.sf.net ([pandas]).\\nStructured data sets\\nStructured data sets commonly arrive in tabular format, i.e.\\nas a two-dimensional list of observations and names for the',\n",
       " 'mentation at http://pandas.sf.net ([pandas]).\\nStructured data sets\\nStructured data sets commonly arrive in tabular format, i.e.\\nas a two-dimensional list of observations and names for the\\nﬁelds of each observation. Usually an observation can be\\nuniquely identiﬁed by one or more values or labels. We show\\nan example data set for a pair of stocks over the course of\\nseveral days. The NumPy ndarray with structured dtype can\\nbe used to hold this data:\\n>>> data\\narray([(’GOOG’, ’2009-12-28’, 622.87, 1697900.0),',\n",
       " '2\\n(’GOOG’, ’2009-12-29’, 619.40, 1424800.0),\\n(’GOOG’, ’2009-12-30’, 622.73, 1465600.0),\\n(’GOOG’, ’2009-12-31’, 619.98, 1219800.0),\\n(’AAPL’, ’2009-12-28’, 211.61, 23003100.0),\\n(’AAPL’, ’2009-12-29’, 209.10, 15868400.0),\\n(’AAPL’, ’2009-12-30’, 211.64, 14696800.0),\\n(’AAPL’, ’2009-12-31’, 210.73, 12571000.0)],\\ndtype=[(’item’, ’|S4’), (’date’, ’|S10’),\\n(’price’, ’<f8’), (’volume’, ’<f8’)])\\n>>> data[’price’]\\narray([622.87, 619.4, 622.73, 619.98, 211.61, 209.1,\\n211.64, 210.73])\\nStructured (or record) NumPy arrays such as this can be\\neffective in many applications, but in our experience they do\\nnot provide the same level of ﬂexibility and ease of use as\\nother statistical environments. One major issue is that they do\\nnot integrate well with the rest of NumPy, which is mainly\\nintended for working with arrays of homogeneous dtype.\\nR provides the data.frame class which stores mixed-\\ntype data as a collection of independent columns. The core',\n",
       " 'intended for working with arrays of homogeneous dtype.\\nR provides the data.frame class which stores mixed-\\ntype data as a collection of independent columns. The core\\nR language and its 3rd-party libraries were built with the\\ndata.frame object in mind, so most operations on such\\na data set are very natural. A data.frame is also ﬂexible\\nin size, an important feature when assembling a collection of\\ndata. The following code fragment loads the data stored in the\\nCSV ﬁle data into the variable df and adds a new column\\nof boolean values:\\n> df <- read.csv(’data’)\\nitem\\ndate\\nprice\\nvolume\\n1 GOOG 2009-12-28 622.87\\n1697900\\n2 GOOG 2009-12-29 619.40\\n1424800\\n3 GOOG 2009-12-30 622.73\\n1465600\\n4 GOOG 2009-12-31 619.98\\n1219800\\n5 AAPL 2009-12-28 211.61 23003100\\n6 AAPL 2009-12-29 209.10 15868400\\n7 AAPL 2009-12-30 211.64 14696800\\n8 AAPL 2009-12-31 210.73 12571000\\n> df$ind <- df$item == \"GOOG\"\\n> df\\nitem\\ndate\\nprice\\nvolume\\nind\\n1 GOOG 2009-12-28 622.87\\n1697900\\nTRUE\\n2 GOOG 2009-12-29 619.40\\n1424800\\nTRUE',\n",
       " '8 AAPL 2009-12-31 210.73 12571000\\n> df$ind <- df$item == \"GOOG\"\\n> df\\nitem\\ndate\\nprice\\nvolume\\nind\\n1 GOOG 2009-12-28 622.87\\n1697900\\nTRUE\\n2 GOOG 2009-12-29 619.40\\n1424800\\nTRUE\\n3 GOOG 2009-12-30 622.73\\n1465600\\nTRUE\\n4 GOOG 2009-12-31 619.98\\n1219800\\nTRUE\\n5 AAPL 2009-12-28 211.61 23003100 FALSE\\n6 AAPL 2009-12-29 209.10 15868400 FALSE\\n7 AAPL 2009-12-30 211.64 14696800 FALSE\\n8 AAPL 2009-12-31 210.73 12571000 FALSE\\npandas provides a similarly-named DataFrame class\\nwhich implements much of the functionality of its R coun-\\nterpart, though with some important enhancements which we\\nwill discuss. Here we convert the structured array above into\\na pandas DataFrame object and similarly add the same\\ncolumn:\\n>>> from pandas import DataFrame\\n>>> data = DataFrame(data)\\n>>> data\\nitem\\ndate\\nprice\\nvolume\\n0\\nGOOG\\n2009-12-28\\n622.9\\n1.698e+06\\n1\\nGOOG\\n2009-12-29\\n619.4\\n1.425e+06\\n2\\nGOOG\\n2009-12-30\\n622.7\\n1.466e+06\\n3\\nGOOG\\n2009-12-31\\n620\\n1.22e+06\\n4\\nAAPL\\n2009-12-28\\n211.6\\n2.3e+07\\n5\\nAAPL\\n2009-12-29\\n209.1\\n1.587e+07\\n6\\nAAPL',\n",
       " '2009-12-28\\n622.9\\n1.698e+06\\n1\\nGOOG\\n2009-12-29\\n619.4\\n1.425e+06\\n2\\nGOOG\\n2009-12-30\\n622.7\\n1.466e+06\\n3\\nGOOG\\n2009-12-31\\n620\\n1.22e+06\\n4\\nAAPL\\n2009-12-28\\n211.6\\n2.3e+07\\n5\\nAAPL\\n2009-12-29\\n209.1\\n1.587e+07\\n6\\nAAPL\\n2009-12-30\\n211.6\\n1.47e+07\\n7\\nAAPL\\n2009-12-31\\n210.7\\n1.257e+07\\n>>> data[’ind’] = data[’item’] == ’GOOG’\\n>>> data\\nitem\\ndate\\nprice\\nvolume\\nind\\n0\\nGOOG\\n2009-12-28\\n622.9\\n1.698e+06\\nTrue\\n1\\nGOOG\\n2009-12-29\\n619.4\\n1.425e+06\\nTrue\\n2\\nGOOG\\n2009-12-30\\n622.7\\n1.466e+06\\nTrue\\n3\\nGOOG\\n2009-12-31\\n620\\n1.22e+06\\nTrue\\n4\\nAAPL\\n2009-12-28\\n211.6\\n2.3e+07\\nFalse\\n5\\nAAPL\\n2009-12-29\\n209.1\\n1.587e+07\\nFalse\\n6\\nAAPL\\n2009-12-30\\n211.6\\n1.47e+07\\nFalse\\n7\\nAAPL\\n2009-12-31\\n210.7\\n1.257e+07\\nFalse\\nThis data can be reshaped or “pivoted” on the date and\\nitem columns into a different form for future examples by\\nmeans of the DataFrame method pivot:\\n>>> del data[’ind’] # delete ind column\\n>>> data.pivot(’date’, ’item’)\\nprice\\nvolume\\nitem\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\ndate\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06',\n",
       " '>>> del data[’ind’] # delete ind column\\n>>> data.pivot(’date’, ’item’)\\nprice\\nvolume\\nitem\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\ndate\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\nThe result of the pivot operation has a hierarchical index\\nfor the columns. As we will show in a later section, this is\\na powerful and ﬂexible way of representing and manipulat-\\ning multidimensional data. Currently the pivot method of\\nDataFrame only supports pivoting on two columns to reshape\\nthe data, but could be augmented to consider more than just\\ntwo columns. By using hierarchical indexes, we can guarantee\\nthat the result will always be two-dimensional. Later in the\\npaper we will demonstrate the pivot_table function which\\ncan produce spreadsheet-style pivot table data summaries as\\nDataFrame objects with hierarchical rows and columns.\\nBeyond observational data, one will also frequently en-',\n",
       " 'can produce spreadsheet-style pivot table data summaries as\\nDataFrame objects with hierarchical rows and columns.\\nBeyond observational data, one will also frequently en-\\ncounter categorical data, which can be used to partition identi-\\nﬁers into broader groupings. For example, stock tickers might\\nbe categorized by their industry or country of incorporation.\\nHere we have created a DataFrame object cats storing\\ncountry and industry classiﬁcations for a group of stocks:\\n>>> cats\\ncountry\\nindustry\\nAAPL\\nUS\\nTECH\\nIBM\\nUS\\nTECH\\nSAP\\nDE\\nTECH\\nGOOG\\nUS\\nTECH\\nC\\nUS\\nFIN\\nSCGLY\\nFR\\nFIN\\nBAR\\nUK\\nFIN\\nDB\\nDE\\nFIN\\nVW\\nDE\\nAUTO\\nRNO\\nFR\\nAUTO\\nF\\nUS\\nAUTO\\nTM\\nJP\\nAUTO\\npandas data model\\nEach axis of a pandas data structure has an Index object\\nwhich stores labeling information about each tick along that\\naxis. The most general Index is simply a 1-dimensional\\nvector of labels (stored in a NumPy ndarray). It’s convenient\\nto think about the Index as an implementation of an ordered',\n",
       " 'axis. The most general Index is simply a 1-dimensional\\nvector of labels (stored in a NumPy ndarray). It’s convenient\\nto think about the Index as an implementation of an ordered\\nset. In the stock data above, the row index contains simply',\n",
       " '3\\nsequential observation numbers, while the column index con-\\ntains the column names. The labels are not required to be\\nsorted, though a subclass of Index could be implemented to\\nrequire sortedness and provide operations optimized for sorted\\ndata (e.g. time series data).\\nThe Index object is used for many purposes:\\n• Performing lookups to select subsets of slices of an object\\n• Providing fast data alignment routines for aligning one\\nobject with another\\n• Enabling intuitive slicing / selection to form new Index\\nobjects\\n• Forming unions and intersections of Index objects\\nHere are some examples of how the index is used internally:\\n>>> index = Index([’a’, ’b’, ’c’, ’d’, ’e’])\\n>>> ’c’ in index\\nTrue\\n>>> index.get_loc(’d’)\\n3\\n>>> index.slice_locs(’b’, ’d’)\\n(1, 4)\\n# for aligning data\\n>>> index.get_indexer([’c’, ’e’, ’f’])\\narray([ 2,\\n4, -1], dtype=int32)\\nThe basic Index uses a Python dict internally to map\\nlabels to their respective locations and implement these fea-',\n",
       " '>>> index.get_indexer([’c’, ’e’, ’f’])\\narray([ 2,\\n4, -1], dtype=int32)\\nThe basic Index uses a Python dict internally to map\\nlabels to their respective locations and implement these fea-\\ntures, though subclasses could take a more specialized and\\npotentially higher performance approach.\\nMultidimensional objects like DataFrame are not proper\\nsubclasses of NumPy’s ndarray nor do they use arrays\\nwith structured dtype. In recent releases of pandas there is a\\nnew internal data structure known as BlockManager which\\nmanipulates a collection of n-dimensional ndarray objects\\nwe refer to as blocks. Since DataFrame needs to be able to\\nstore mixed-type data in the columns, each of these internal\\nBlock objects contains the data for a set of columns all\\nhaving the same type. In the example from above, we can\\nexamine the BlockManager, though most users would never\\nneed to do this:\\n>>> data._data\\nBlockManager\\nItems: [item date price volume ind]\\nAxis 1: [0 1 2 3 4 5 6 7]',\n",
       " 'examine the BlockManager, though most users would never\\nneed to do this:\\n>>> data._data\\nBlockManager\\nItems: [item date price volume ind]\\nAxis 1: [0 1 2 3 4 5 6 7]\\nFloatBlock: [price volume], 2 x 8, dtype float64\\nObjectBlock: [item date], 2 x 8, dtype object\\nBoolBlock: [ind], 1 x 8, dtype bool\\nThe key importance of BlockManager is that many\\noperations, e.g. anything row-oriented (as opposed to column-\\noriented), especially in homogeneous DataFrame objects,\\nare signiﬁcantly faster when the data are all stored in a\\nsingle ndarray. However, as it is common to insert and\\ndelete columns, it would be wasteful to have a reallocate-\\ncopy step on each column insertion or deletion step. As\\na result, the BlockManager effectively provides a lazy\\nevaluation scheme where-in newly inserted columns are stored\\nin new Block objects. Later, either explicitly or when certain\\nmethods are called in DataFrame, blocks having the same\\ntype will be consolidated, i.e. combined together, to form a',\n",
       " 'in new Block objects. Later, either explicitly or when certain\\nmethods are called in DataFrame, blocks having the same\\ntype will be consolidated, i.e. combined together, to form a\\nsingle homogeneously-typed Block:\\n>>> data[’newcol’] = 1.\\n>>> data._data\\nBlockManager\\nItems: [item date price volume ind newcol]\\nAxis 1: [0 1 2 3 4 5 6 7]\\nFloatBlock: [price volume], 2 x 8\\nObjectBlock: [item date], 2 x 8\\nBoolBlock: [ind], 1 x 8\\nFloatBlock: [newcol], 1 x 8\\n>>> data.consolidate()._data\\nBlockManager\\nItems: [item date price volume ind newcol]\\nAxis 1: [0 1 2 3 4 5 6 7]\\nBoolBlock: [ind], 1 x 8\\nFloatBlock: [price volume newcol], 3 x 8\\nObjectBlock: [item date], 2 x 8\\nThe separation between the internal BlockManager ob-\\nject and the external, user-facing DataFrame gives the pan-\\ndas developers a signiﬁcant amount of freedom to modify the\\ninternal structure to achieve better performance and memory\\nusage.\\nLabel-based data access\\nWhile standard []-based indexing (using __getitem__',\n",
       " 'internal structure to achieve better performance and memory\\nusage.\\nLabel-based data access\\nWhile standard []-based indexing (using __getitem__\\nand __setitem__) is reserved for column access in\\nDataFrame, it is useful to be able to index both axes of\\na DataFrame in a matrix-like way using labels. We would\\nlike to be able to get or set data on any axis using one of the\\nfollowing:\\n• A list or array of labels or integers\\n• A slice, either with integers (e.g. 1:5) or labels (e.g.\\nlab1:lab2)\\n• A boolean vector\\n• A single label\\nTo avoid excessively overloading the []-related methods,\\nleading to ambiguous indexing semantics in some cases, we\\nhave implemented a special label-indexing attribute ix on all\\nof the pandas data structures. Thus, we can pass a tuple of\\nany of the above indexing objects to get or set values.\\n>>> df\\nA\\nB\\nC\\nD\\n2000-01-03 -0.2047\\n1.007\\n-0.5397 -0.7135\\n2000-01-04\\n0.4789 -1.296\\n0.477\\n-0.8312\\n2000-01-05 -0.5194\\n0.275\\n3.249\\n-2.37\\n2000-01-06 -0.5557\\n0.2289 -1.021\\n-1.861',\n",
       " '>>> df\\nA\\nB\\nC\\nD\\n2000-01-03 -0.2047\\n1.007\\n-0.5397 -0.7135\\n2000-01-04\\n0.4789 -1.296\\n0.477\\n-0.8312\\n2000-01-05 -0.5194\\n0.275\\n3.249\\n-2.37\\n2000-01-06 -0.5557\\n0.2289 -1.021\\n-1.861\\n2000-01-07\\n1.966\\n1.353\\n-0.5771 -0.8608\\n>>> df.ix[:2, [’D’, ’C’, ’A’]]\\nD\\nC\\nA\\n2000-01-03 -0.7135 -0.5397 -0.2047\\n2000-01-04 -0.8312\\n0.477\\n0.4789\\n>>> df.ix[-2:, ’B’:]\\nB\\nC\\nD\\n2000-01-06\\n0.2289 -1.021\\n-1.861\\n2000-01-07\\n1.353\\n-0.5771 -0.8608\\nSetting values also works as expected.\\n>>> date1, date2 = df.index[[1, 3]]\\n>>> df.ix[date1:date2, [’A’, ’C’]] = 0\\n>>> df\\nA\\nB\\nC\\nD\\n2000-01-03 -0.6856\\n0.1362\\n0.3996\\n1.585\\n2000-01-04\\n0\\n0.8863\\n0\\n1.907\\n2000-01-05\\n0\\n-1.351\\n0\\n0.104\\n2000-01-06\\n0\\n-0.8863\\n0\\n0.1741\\n2000-01-07 -0.05927 -1.013\\n0.9923 -0.4395',\n",
       " '4\\nData alignment\\nOperations between related, but differently-sized data sets can\\npose a problem as the user must ﬁrst ensure that the data points\\nare properly aligned. As an example, consider time series over\\ndifferent date ranges or economic data series over varying sets\\nof entities:\\n>>> s1\\n>>> s2\\nAAPL\\n0.044\\nAAPL\\n0.025\\nIBM\\n0.050\\nBAR\\n0.158\\nSAP\\n0.101\\nC\\n0.028\\nGOOG\\n0.113\\nDB\\n0.087\\nC\\n0.138\\nF\\n0.004\\nSCGLY\\n0.037\\nGOOG\\n0.154\\nBAR\\n0.200\\nIBM\\n0.034\\nDB\\n0.281\\nVW\\n0.040\\nOne might choose to explicitly align (or reindex) one of\\nthese 1D Series objects with the other before adding them,\\nusing the reindex method:\\n>>> s1.reindex(s2.index)\\nAAPL\\n0.0440877763224\\nBAR\\n0.199741007422\\nC\\n0.137747485628\\nDB\\n0.281070058049\\nF\\nNaN\\nGOOG\\n0.112861123629\\nIBM\\n0.0496445829129\\nHowever, we often ﬁnd it preferable to simply ignore the\\nstate of data alignment:\\n>>> s1 + s2\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nF\\nNaN\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\nSAP\\nNaN\\nSCGLY\\nNaN\\nVW\\nNaN',\n",
       " 'state of data alignment:\\n>>> s1 + s2\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nF\\nNaN\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\nSAP\\nNaN\\nSCGLY\\nNaN\\nVW\\nNaN\\nHere, the data have been automatically aligned based on\\ntheir labels and added together. The result object contains\\nthe union of the labels between the two objects so that no\\ninformation is lost. We will discuss the use of NaN (Not a\\nNumber) to represent missing data in the next section.\\nClearly, the user pays linear overhead whenever automatic\\ndata alignment occurs and we seek to minimize that overhead\\nto the extent possible. Reindexing can be avoided when\\nIndex objects are shared, which can be an effective strategy\\nin performance-sensitive applications. [Cython], a widely-\\nused tool for creating Python C extensions and interfacing\\nwith C/C++ code, has been utilized to speed up these core\\nalgorithms.\\nData alignment using DataFrame occurs automatically',\n",
       " 'used tool for creating Python C extensions and interfacing\\nwith C/C++ code, has been utilized to speed up these core\\nalgorithms.\\nData alignment using DataFrame occurs automatically\\non both the column and row labels. This deeply integrated\\ndata alignment differs from any other tools outside of Python\\nthat we are aware of. Similar to the above, if the columns\\nthemselves are different, the resulting object will contain the\\nunion of the columns:\\n>>> df\\n>>> df2\\nAAPL\\nGOOG\\nAAPL\\n2009-12-28\\n211.6\\n622.9\\n2009-12-28\\n2.3e+07\\n2009-12-29\\n209.1\\n619.4\\n2009-12-29\\n1.587e+07\\n2009-12-30\\n211.6\\n622.7\\n2009-12-30\\n1.47e+07\\n2009-12-31\\n210.7\\n620\\n>>> df / df2\\nAAPL\\nGOOG\\n2009-12-28\\n9.199e-06\\nNaN\\n2009-12-29\\n1.318e-05\\nNaN\\n2009-12-30\\n1.44e-05\\nNaN\\n2009-12-31\\nNaN\\nNaN\\nThis may seem like a simple feature, but in practice it grants\\nimmense freedom as there is no longer a need to sanitize\\ndata from an untrusted source. For example, if you loaded\\ntwo data sets from a database and the columns and rows,',\n",
       " 'immense freedom as there is no longer a need to sanitize\\ndata from an untrusted source. For example, if you loaded\\ntwo data sets from a database and the columns and rows,\\nthey can be added together, say, without having to do any\\nchecking whether the labels are aligned. Of course, after doing\\nan operation between two data sets, you can perform an ad\\nhoc cleaning of the results using such functions as fillna\\nand dropna:\\n>>> (df / df2).fillna(0)\\nAAPL\\nGOOG\\n2009-12-28\\n9.199e-06\\n0\\n2009-12-29\\n1.318e-05\\n0\\n2009-12-30\\n1.44e-05\\n0\\n2009-12-31\\n0\\n0\\n>>> (df / df2).dropna(axis=1, how=’all’)\\nAAPL\\n2009-12-28\\n9.199e-06\\n2009-12-29\\n1.318e-05\\n2009-12-30\\n1.44e-05\\n2009-12-31\\nNaN\\nHandling missing data\\nIt is common for a data set to have missing observations.\\nFor example, a group of related economic time series stored\\nin a DataFrame may start on different dates. Carrying\\nout calculations in the presence of missing data can lead\\nboth to complicated code and considerable performance loss.',\n",
       " 'in a DataFrame may start on different dates. Carrying\\nout calculations in the presence of missing data can lead\\nboth to complicated code and considerable performance loss.\\nWe chose to use NaN as opposed to using the NumPy\\nMaskedArray object for performance reasons (which are\\nbeyond the scope of this paper), as NaN propagates in ﬂoating-\\npoint operations in a natural way and can be easily detected\\nin algorithms. While this leads to good performance, it comes\\nwith drawbacks: namely that NaN cannot be used in integer-\\ntype arrays, and it is not an intuitive “null” value in object or\\nstring arrays (though it is used in these arrays regardless).\\nWe regard the use of NaN as an implementation detail and\\nattempt to provide the user with appropriate API functions for\\nperforming common operations on missing data points. From\\nthe above example, we can use the dropna method to drop\\nmissing data, or we could use fillna to replace missing data\\nwith a speciﬁc value:\\n>>> (s1 + s2).dropna()\\nAAPL',\n",
       " 'the above example, we can use the dropna method to drop\\nmissing data, or we could use fillna to replace missing data\\nwith a speciﬁc value:\\n>>> (s1 + s2).dropna()\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\n>>> (s1 + s2).fillna(0)\\nAAPL\\n0.0686791008184\\nBAR\\n0.358165479807',\n",
       " '5\\nC\\n0.16586702944\\nDB\\n0.367679872693\\nF\\n0.0\\nGOOG\\n0.26666583847\\nIBM\\n0.0833057542385\\nSAP\\n0.0\\nSCGLY\\n0.0\\nVW\\n0.0\\nThe reindex and fillna methods are equipped with\\na couple simple interpolation options to propagate values\\nforward and backward, which is especially useful for time\\nseries data:\\n>>> ts\\n>>> ts2\\n2000-01-03\\n0.03825\\n2000-01-03\\n0.03825\\n2000-01-04\\n-1.9884\\n2000-01-06\\n-0.0588\\n2000-01-05\\n0.73255\\n2000-01-11\\n0.04410\\n2000-01-06\\n-0.0588\\n2000-01-14\\n-0.1786\\n2000-01-07\\n-0.4767\\n2000-01-10\\n1.98008\\n2000-01-11\\n0.04410\\n>>> ts3 = ts + ts2\\n>>> ts3\\n>>> ts3.fillna(method=’ffill’)\\n2000-01-03\\n0.07649\\n2000-01-03\\n0.07649\\n2000-01-04\\nNaN\\n2000-01-04\\n0.07649\\n2000-01-05\\nNaN\\n2000-01-05\\n0.07649\\n2000-01-06\\n-0.1177\\n2000-01-06\\n-0.1177\\n2000-01-07\\nNaN\\n2000-01-07\\n-0.1177\\n2000-01-10\\nNaN\\n2000-01-10\\n-0.1177\\n2000-01-11\\n0.08821\\n2000-01-11\\n0.08821\\n2000-01-14\\nNaN\\n2000-01-14\\n0.08821\\nSeries and DataFrame also have explicit arithmetic\\nmethods with which a fill_value can be used to specify',\n",
       " '2000-01-10\\n-0.1177\\n2000-01-11\\n0.08821\\n2000-01-11\\n0.08821\\n2000-01-14\\nNaN\\n2000-01-14\\n0.08821\\nSeries and DataFrame also have explicit arithmetic\\nmethods with which a fill_value can be used to specify\\na treatment of missing data in the computation. An occasional\\nchoice is to treat missing values as 0 when adding two\\nSeries objects:\\n>>> ts.add(ts2, fill_value=0)\\n2000-01-03\\n0.0764931953608\\n2000-01-04\\n-1.98842046359\\n2000-01-05\\n0.732553684194\\n2000-01-06\\n-0.117727627078\\n2000-01-07\\n-0.476754320696\\n2000-01-10\\n1.9800873096\\n2000-01-11\\n0.0882102892097\\n2000-01-14\\n-0.178640361674\\nCommon ndarray methods have been rewritten to auto-\\nmatically exclude missing data from calculations:\\n>>> (s1 + s2).sum()\\n1.3103630754662747\\n>>> (s1 + s2).count()\\n6\\nSimilar to R’s is.na function, which detects NA (Not Avail-\\nable) values, pandas has special API functions isnull and\\nnotnull for determining the validity of a data point. These\\ncontrast with numpy.isnan in that they can be used with',\n",
       " 'able) values, pandas has special API functions isnull and\\nnotnull for determining the validity of a data point. These\\ncontrast with numpy.isnan in that they can be used with\\ndtypes other than float and also detect some other markers\\nfor “missing” occurring in the wild, such as the Python None\\nvalue.\\n>>> isnull(s1 + s2)\\nAAPL\\nFalse\\nBAR\\nFalse\\nC\\nFalse\\nDB\\nFalse\\nF\\nTrue\\nGOOG\\nFalse\\nIBM\\nFalse\\nSAP\\nTrue\\nSCGLY\\nTrue\\nVW\\nTrue\\nNote that R’s NA value is distinct from NaN. NumPy core\\ndevelopers are currently working on an NA value implementa-\\ntion that will hopefully suit the needs of libraries like pandas\\nin the future.\\nHierarchical Indexing\\nA relatively recent addition to pandas is the ability for an\\naxis to have a hierarchical index, known in the library as a\\nMultiIndex. Semantically, this means that each a location\\non a single axis can have multiple labels associated with it.\\n>>> hdf\\nA\\nB\\nC\\nfoo\\none\\n-0.9884\\n0.09406\\n1.263\\ntwo\\n1.29\\n0.08242 -0.05576\\nthree\\n0.5366\\n-0.4897\\n0.3694\\nbar\\none\\n-0.03457 -2.484',\n",
       " 'on a single axis can have multiple labels associated with it.\\n>>> hdf\\nA\\nB\\nC\\nfoo\\none\\n-0.9884\\n0.09406\\n1.263\\ntwo\\n1.29\\n0.08242 -0.05576\\nthree\\n0.5366\\n-0.4897\\n0.3694\\nbar\\none\\n-0.03457 -2.484\\n-0.2815\\ntwo\\n0.03071\\n0.1091\\n1.126\\nbaz\\ntwo\\n-0.9773\\n1.474\\n-0.06403\\nthree -1.283\\n0.7818\\n-1.071\\nqux\\none\\n0.4412\\n2.354\\n0.5838\\ntwo\\n0.2215\\n-0.7445\\n0.7585\\nthree\\n1.73\\n-0.965\\n-0.8457\\nHierarchical indexing can be viewed as a way to represent\\nhigher-dimensional data in a lower-dimensional data structure\\n(here, a 2D DataFrame). For example, we can select rows\\nfrom the above DataFrame by specifying only a label from\\nthe left-most level of the index:\\n>>> hdf.ix[’foo’]\\nA\\nB\\nC\\none\\n-0.9884\\n0.09406\\n1.263\\ntwo\\n1.29\\n0.08242 -0.05576\\nthree\\n0.5366\\n-0.4897\\n0.3694\\nOf course, if all of the levels are speciﬁed, we can select a\\nrow or column just as with a regular Index.\\n>>> hdf.ix[’foo’, ’three’]\\nA\\n0.5366\\nB\\n-0.4897\\nC\\n0.3694\\n# same result\\n>>> hdf.ix[’foo’].ix[’three’]\\nThe hierarchical index can be used with any axis. From the',\n",
       " '>>> hdf.ix[’foo’, ’three’]\\nA\\n0.5366\\nB\\n-0.4897\\nC\\n0.3694\\n# same result\\n>>> hdf.ix[’foo’].ix[’three’]\\nThe hierarchical index can be used with any axis. From the\\npivot example earlier in the paper we obtained:\\n>>> pivoted = data.pivot(’date’, ’item’)\\n>>> pivoted\\nprice\\nvolume\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\n>>> pivoted[’volume’]\\nAAPL\\nGOOG\\n2009-12-28\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n1.257e+07\\n1.22e+06\\nThere are several utility methods for manipulating a\\nMultiIndex such as swaplevel and sortlevel:',\n",
       " '6\\n>>> swapped = pivoted.swaplevel(0, 1, axis=1)\\n>>> swapped\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\nprice\\nprice\\nvolume\\nvolume\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\n>>> swapped[’AAPL’]\\nprice\\nvolume\\n2009-12-28\\n211.6\\n2.3e+07\\n2009-12-29\\n209.1\\n1.587e+07\\n2009-12-30\\n211.6\\n1.47e+07\\n2009-12-31\\n210.7\\n1.257e+07\\nHere is an example for sortlevel:\\n>>> pivoted.sortlevel(1, axis=1)\\nprice\\nvolume\\nprice\\nvolume\\nAAPL\\nAAPL\\nGOOG\\nGOOG\\n2009-12-28\\n211.6\\n2.3e+07\\n622.9\\n1.698e+06\\n2009-12-29\\n209.1\\n1.587e+07\\n619.4\\n1.425e+06\\n2009-12-30\\n211.6\\n1.47e+07\\n622.7\\n1.466e+06\\n2009-12-31\\n210.7\\n1.257e+07\\n620\\n1.22e+06\\nAdvanced pivoting and reshaping\\nClosely related to hierarchical indexing and the earlier pivoting\\nexample, we illustrate more advanced reshaping of data using\\nthe stack and unstack methods. stack reshapes by\\nremoving a level from the columns of a DataFrame object',\n",
       " 'example, we illustrate more advanced reshaping of data using\\nthe stack and unstack methods. stack reshapes by\\nremoving a level from the columns of a DataFrame object\\nand moving that level to the row labels, producing either a\\n1D Series or another DataFrame (if the columns were a\\nMultiIndex).\\n>>> df\\nAAPL\\nGOOG\\n2009-12-28\\n211.6\\n622.9\\n2009-12-29\\n209.1\\n619.4\\n2009-12-30\\n211.6\\n622.7\\n2009-12-31\\n210.7\\n620\\n>>> df.stack()\\n2009-12-28\\nAAPL\\n211.61\\nGOOG\\n622.87\\n2009-12-29\\nAAPL\\n209.1\\nGOOG\\n619.4\\n2009-12-30\\nAAPL\\n211.64\\nGOOG\\n622.73\\n2009-12-31\\nAAPL\\n210.73\\nGOOG\\n619.98\\n>>> pivoted\\nprice\\nvolume\\nAAPL\\nGOOG\\nAAPL\\nGOOG\\n2009-12-28\\n211.6\\n622.9\\n2.3e+07\\n1.698e+06\\n2009-12-29\\n209.1\\n619.4\\n1.587e+07\\n1.425e+06\\n2009-12-30\\n211.6\\n622.7\\n1.47e+07\\n1.466e+06\\n2009-12-31\\n210.7\\n620\\n1.257e+07\\n1.22e+06\\n>>> pivoted.stack()\\nprice\\nvolume\\n2009-12-28\\nAAPL\\n211.6\\n2.3e+07\\nGOOG\\n622.9\\n1.698e+06\\n2009-12-29\\nAAPL\\n209.1\\n1.587e+07\\nGOOG\\n619.4\\n1.425e+06\\n2009-12-30\\nAAPL\\n211.6\\n1.47e+07\\nGOOG\\n622.7\\n1.466e+06\\n2009-12-31\\nAAPL\\n210.7\\n1.257e+07\\nGOOG\\n620',\n",
       " '2009-12-28\\nAAPL\\n211.6\\n2.3e+07\\nGOOG\\n622.9\\n1.698e+06\\n2009-12-29\\nAAPL\\n209.1\\n1.587e+07\\nGOOG\\n619.4\\n1.425e+06\\n2009-12-30\\nAAPL\\n211.6\\n1.47e+07\\nGOOG\\n622.7\\n1.466e+06\\n2009-12-31\\nAAPL\\n210.7\\n1.257e+07\\nGOOG\\n620\\n1.22e+06\\nBy default, the innermost level is stacked. The level to stack\\ncan be speciﬁed explicitly:\\n>>> pivoted.stack(0)\\nAAPL\\nGOOG\\n2009-12-28\\nprice\\n211.6\\n622.9\\nvolume\\n2.3e+07\\n1.698e+06\\n2009-12-29\\nprice\\n209.1\\n619.4\\nvolume\\n1.587e+07\\n1.425e+06\\n2009-12-30\\nprice\\n211.6\\n622.7\\nvolume\\n1.47e+07\\n1.466e+06\\n2009-12-31\\nprice\\n210.7\\n620\\nvolume\\n1.257e+07\\n1.22e+06\\nThe unstack method is the inverse of stack:\\n>>> df.stack()\\n>>> df.stack().unstack()\\n2009-12-28\\nAAPL\\n211.61\\nAAPL\\nGOOG\\nGOOG\\n622.87\\n2009-12-28\\n211.6\\n622.9\\n2009-12-29\\nAAPL\\n209.1\\n2009-12-29\\n209.1\\n619.4\\nGOOG\\n619.4\\n2009-12-30\\n211.6\\n622.7\\n2009-12-30\\nAAPL\\n211.64\\n2009-12-31\\n210.7\\n620\\nGOOG\\n622.73\\n2009-12-31\\nAAPL\\n210.73\\nGOOG\\n619.98\\nThese reshaping methods can be combined with built-in\\nDataFrame and Series method to select or aggregate data',\n",
       " 'AAPL\\n211.64\\n2009-12-31\\n210.7\\n620\\nGOOG\\n622.73\\n2009-12-31\\nAAPL\\n210.73\\nGOOG\\n619.98\\nThese reshaping methods can be combined with built-in\\nDataFrame and Series method to select or aggregate data\\nat a level. Here we take the maximum among AAPL and GOOG\\nfor each date / ﬁeld pair:\\n>>> pivoted.stack(0)\\nAAPL\\nGOOG\\n2009-12-28\\nprice\\n211.6\\n622.9\\nvolume\\n2.3e+07\\n1.698e+06\\n2009-12-29\\nprice\\n209.1\\n619.4\\nvolume\\n1.587e+07\\n1.425e+06\\n2009-12-30\\nprice\\n211.6\\n622.7\\nvolume\\n1.47e+07\\n1.466e+06\\n2009-12-31\\nprice\\n210.7\\n620\\nvolume\\n1.257e+07\\n1.22e+06\\n>>> pivoted.stack(0).max(1).unstack()\\nprice\\nvolume\\n2009-12-28\\n622.9\\n2.3e+07\\n2009-12-29\\n619.4\\n1.587e+07\\n2009-12-30\\n622.7\\n1.47e+07\\n2009-12-31\\n620\\n1.257e+07\\nThese kinds of aggregations are closely related to “group\\nby” operations which we discuss in the next section.\\nGroup By: grouping and aggregating data\\nA very common operation in SQL-like languages and gen-\\nerally in statistical data analysis is to group data by some',\n",
       " 'Group By: grouping and aggregating data\\nA very common operation in SQL-like languages and gen-\\nerally in statistical data analysis is to group data by some\\nidentiﬁers and perform either an aggregation or transformation\\nof the data. For example, suppose we had a simple data set\\nlike this:\\n>>> df\\nA\\nB\\nC\\nD\\n0\\nfoo\\none\\n-1.834\\n1.903\\n1\\nbar\\none\\n1.772\\n-0.7472\\n2\\nfoo\\ntwo\\n-0.67\\n-0.309\\n3\\nbar\\nthree\\n0.04931\\n0.3939\\n4\\nfoo\\ntwo\\n-0.5215\\n1.861\\n5\\nbar\\ntwo\\n-3.202\\n0.9365\\n6\\nfoo\\none\\n0.7927\\n1.256\\n7\\nfoo\\nthree\\n0.1461\\n-2.655\\nWe could compute group means using the A column like\\nso:\\n>>> df.groupby(’A’).mean()\\nC\\nD\\nbar -0.4602\\n0.1944',\n",
       " '7\\nfoo -0.4173\\n0.4112\\nThe object returned by groupby is a special intermediate\\nobject with a lot of nice features. For example, you can use\\nit to iterate through the portions of the data set corresponding\\nto each group:\\n>>> for key, group in df.groupby(’A’):\\n...\\nprint key\\n...\\nprint group\\nbar\\nA\\nB\\nC\\nD\\n1\\nbar\\none\\n1.772\\n-0.7472\\n3\\nbar\\nthree\\n0.04931\\n0.3939\\n5\\nbar\\ntwo\\n-3.202\\n0.9365\\nfoo\\nA\\nB\\nC\\nD\\n0\\nfoo\\none\\n-1.834\\n1.903\\n2\\nfoo\\ntwo\\n-0.67\\n-0.309\\n4\\nfoo\\ntwo\\n-0.5215\\n1.861\\n6\\nfoo\\none\\n0.7927\\n1.256\\n7\\nfoo\\nthree\\n0.1461 -2.65\\nGrouping by multiple columns is also possible:\\ndf.groupby([’A’, ’B’]).mean()\\nC\\nD\\nbar\\none\\n1.772\\n-0.7472\\nthree\\n0.04931\\n0.3939\\ntwo\\n-3.202\\n0.9365\\nfoo\\none\\n-0.5205\\n1.579\\nthree\\n0.1461\\n-2.655\\ntwo\\n-0.5958\\n0.7762\\nThe default result of a multi-key groupby aggregation\\nis a hierarchical index. This can be disabled when calling\\ngroupby which may be useful in some settings:\\ndf.groupby([’A’, ’B’], as_index=False).mean()\\nA\\nB\\nC\\nD\\n0\\nbar\\none\\n1.772\\n-0.7472\\n1\\nbar\\nthree\\n0.04931\\n0.3939\\n2\\nbar\\ntwo\\n-3.202\\n0.9365\\n3',\n",
       " 'groupby which may be useful in some settings:\\ndf.groupby([’A’, ’B’], as_index=False).mean()\\nA\\nB\\nC\\nD\\n0\\nbar\\none\\n1.772\\n-0.7472\\n1\\nbar\\nthree\\n0.04931\\n0.3939\\n2\\nbar\\ntwo\\n-3.202\\n0.9365\\n3\\nfoo\\none\\n-0.5205\\n1.579\\n4\\nfoo\\nthree\\n0.1461\\n-2.655\\n5\\nfoo\\ntwo\\n-0.5958\\n0.7762\\nIn a completely general setting, groupby operations are\\nabout mapping axis labels to buckets. In the above examples,\\nwhen we pass column names we are simply establishing a cor-\\nrespondence between the row labels and the group identiﬁers.\\nThere are other ways to do this; the most general is to pass a\\nPython function (for single-key) or list of functions (for multi-\\nkey) which will be invoked on each each label, producing a\\ngroup speciﬁcation:\\n>>> dat\\nA\\nB\\nC\\nD\\n2000-01-03\\n0.6371\\n0.672\\n0.9173\\n1.674\\n2000-01-04 -0.8178 -1.865\\n-0.23\\n0.5411\\n2000-01-05\\n0.314\\n0.2931 -0.6444 -0.9973\\n2000-01-06\\n1.913\\n-0.5867\\n0.273\\n0.4631\\n2000-01-07\\n1.308\\n0.426\\n-1.306\\n0.04358\\n>>> mapping\\n{’A’: ’Group 1’, ’B’: ’Group 2’,\\n’C’: ’Group 1’, ’D’: ’Group 2’}',\n",
       " '0.5411\\n2000-01-05\\n0.314\\n0.2931 -0.6444 -0.9973\\n2000-01-06\\n1.913\\n-0.5867\\n0.273\\n0.4631\\n2000-01-07\\n1.308\\n0.426\\n-1.306\\n0.04358\\n>>> mapping\\n{’A’: ’Group 1’, ’B’: ’Group 2’,\\n’C’: ’Group 1’, ’D’: ’Group 2’}\\n>>> for name, group in dat.groupby(mapping.get,\\n...\\naxis=1):\\n...\\nprint name; print group\\nGroup 1\\nA\\nC\\n2000-01-03\\n0.6371\\n0.9173\\n2000-01-04 -0.8178 -0.23\\n2000-01-05\\n0.314\\n-0.6444\\n2000-01-06\\n1.913\\n0.273\\n2000-01-07\\n1.308\\n-1.306\\nGroup 2\\nB\\nD\\n2000-01-03\\n0.672\\n1.674\\n2000-01-04 -1.865\\n0.5411\\n2000-01-05\\n0.2931 -0.9973\\n2000-01-06 -0.5867\\n0.4631\\n2000-01-07\\n0.426\\n0.04358\\nSome creativity with grouping functions will enable the\\nuser to perform quite sophisticated operations. The object re-\\nturned by groupby can either iterate, aggregate (with an\\narbitrary function), transform (compute a modiﬁed same-\\nsize version of each data group), or do a general apply-by-\\ngroup. While we do not have space to go into great detail with\\nexamples of each of these, the apply function is interesting in',\n",
       " 'size version of each data group), or do a general apply-by-\\ngroup. While we do not have space to go into great detail with\\nexamples of each of these, the apply function is interesting in\\nthat it attempts to combine the results of the aggregation into\\na pandas object. For example, we could group the df object\\nabove by column A, select just the C column, and apply the\\ndescribe function to each subgroup like so:\\n>>> df.groupby(’A’)[’C’].describe().T\\nbar\\nfoo\\ncount\\n3\\n5\\nmean\\n-0.4602\\n-0.4173\\nstd\\n2.526\\n0.9827\\nmin\\n-3.202\\n-1.834\\n10%\\n-2.552\\n-1.368\\n50%\\n0.04931 -0.5215\\n90%\\n1.427\\n0.5341\\nmax\\n1.772\\n0.7927\\nNote that, under the hood, calling describe generates\\nand passes a dynamic function to apply which invokes\\ndescribe on each group and glues the results together. We\\ntransposed the result with .T to make it more readable.\\nEasy spreadsheet-style pivot tables\\nAn obvious application combining groupby and reshaping\\noperations is creating pivot tables, a common way of sum-',\n",
       " 'Easy spreadsheet-style pivot tables\\nAn obvious application combining groupby and reshaping\\noperations is creating pivot tables, a common way of sum-\\nmarizing data in spreadsheet applications such as Microsoft\\nExcel. We’ll take a brief look at a tipping data set collected\\nfrom a restaurant ([Bryant]):\\n>>> tips.head()\\nsex\\nsmoker\\ntime\\nday\\nsize\\ntip_pct\\n1\\nFemale\\nNo\\nDinner\\nSun\\n2\\n0.05945\\n2\\nMale\\nNo\\nDinner\\nSun\\n3\\n0.1605\\n3\\nMale\\nNo\\nDinner\\nSun\\n3\\n0.1666\\n4\\nMale\\nNo\\nDinner\\nSun\\n2\\n0.1398\\n5\\nFemale\\nNo\\nDinner\\nSun\\n4\\n0.1468\\nThe pivot_table function in pandas takes a set of\\ncolumn names to group on the pivot table rows, another set to\\ngroup on the columns, and optionally an aggregation function\\nfor each group (which defaults to mean):\\n>>> import numpy as np\\n>>> from pandas import pivot_table\\n>>> pivot_table(tips, ’tip_pct’, rows=[’time’, ’sex’],\\ncols=’smoker’)\\nsmoker\\nNo\\nYes\\ntime\\nsex\\nDinner Female\\n0.1568\\n0.1851',\n",
       " '8\\nMale\\n0.1594\\n0.1489\\nLunch\\nFemale\\n0.1571\\n0.1753\\nMale\\n0.1657\\n0.1667\\nConveniently, the returned object is a DataFrame, so it can\\nbe further reshaped and manipulated by the user:\\n>>> table = pivot_table(tips, ’tip_pct’,\\nrows=[’sex’, ’day’],\\ncols=’smoker’, aggfunc=len)\\n>>> table\\nsmoker\\nNo\\nYes\\nsex\\nday\\nFemale Fri\\n2\\n7\\nSat\\n13\\n15\\nSun\\n14\\n4\\nThur\\n25\\n7\\nMale\\nFri\\n2\\n8\\nSat\\n32\\n27\\nSun\\n43\\n15\\nThur\\n20\\n10\\n>>> table.unstack(’sex’)\\nsmoker\\nNo\\nYes\\nsex\\nFemale\\nMale\\nFemale\\nMale\\nday\\nFri\\n2\\n2\\n7\\n8\\nSat\\n13\\n32\\n15\\n27\\nSun\\n14\\n43\\n4\\n15\\nThur\\n25\\n20\\n7\\n10\\nFor many users, this will be an attractive alternative to\\ndumping a data set into a spreadsheet for the sole purpose\\nof creating a pivot table.\\n>>> pivot_table(tips, ’size’,\\nrows=[’time’, ’sex’, ’smoker’],\\ncols=’day’, aggfunc=np.sum,\\nfill_value=0)\\nday\\nFri\\nSat\\nSun\\nThur\\ntime\\nsex\\nsmoker\\nDinner Female No\\n2\\n30\\n43\\n2\\nYes\\n8\\n33\\n10\\n0\\nDinner Male\\nNo\\n4\\n85\\n124\\n0\\nYes\\n12\\n71\\n39\\n0\\nLunch\\nFemale No\\n3\\n0\\n0\\n60\\nYes\\n6\\n0\\n0\\n17\\nLunch\\nMale\\nNo\\n0\\n0\\n0\\n50\\nYes\\n5\\n0\\n0\\n23\\nCombining or joining data sets',\n",
       " 'sex\\nsmoker\\nDinner Female No\\n2\\n30\\n43\\n2\\nYes\\n8\\n33\\n10\\n0\\nDinner Male\\nNo\\n4\\n85\\n124\\n0\\nYes\\n12\\n71\\n39\\n0\\nLunch\\nFemale No\\n3\\n0\\n0\\n60\\nYes\\n6\\n0\\n0\\n17\\nLunch\\nMale\\nNo\\n0\\n0\\n0\\n50\\nYes\\n5\\n0\\n0\\n23\\nCombining or joining data sets\\nCombining, joining, or merging related data sets is a quite\\ncommon operation. In doing so we are interested in associating\\nobservations from one data set with another via a merge key\\nof some kind. For similarly-indexed 2D data, the row labels\\nserve as a natural key for the join function:\\n>>> df1\\n>>> df2\\nAAPL\\nGOOG\\nMSFT\\nYHOO\\n2009-12-24\\n209\\n618.5\\n2009-12-24\\n31\\n16.72\\n2009-12-28\\n211.6\\n622.9\\n2009-12-28\\n31.17\\n16.88\\n2009-12-29\\n209.1\\n619.4\\n2009-12-29\\n31.39\\n16.92\\n2009-12-30\\n211.6\\n622.7\\n2009-12-30\\n30.96\\n16.98\\n2009-12-31\\n210.7\\n620\\n>>> df1.join(df2)\\nAAPL\\nGOOG\\nMSFT\\nYHOO\\n2009-12-24\\n209\\n618.5\\n31\\n16.72\\n2009-12-28\\n211.6\\n622.9\\n31.17\\n16.88\\n2009-12-29\\n209.1\\n619.4\\n31.39\\n16.92\\n2009-12-30\\n211.6\\n622.7\\n30.96\\n16.98\\n2009-12-31\\n210.7\\n620\\nNaN\\nNaN\\nOne might be interested in joining on something other than',\n",
       " '16.72\\n2009-12-28\\n211.6\\n622.9\\n31.17\\n16.88\\n2009-12-29\\n209.1\\n619.4\\n31.39\\n16.92\\n2009-12-30\\n211.6\\n622.7\\n30.96\\n16.98\\n2009-12-31\\n210.7\\n620\\nNaN\\nNaN\\nOne might be interested in joining on something other than\\nthe index as well, such as the categorical data we presented\\nin an earlier section:\\n>>> data.join(cats, on=’item’)\\ncountry\\ndate\\nindustry item\\nvalue\\n0\\nUS\\n2009-12-28\\nTECH\\nGOOG\\n622.9\\n1\\nUS\\n2009-12-29\\nTECH\\nGOOG\\n619.4\\n2\\nUS\\n2009-12-30\\nTECH\\nGOOG\\n622.7\\n3\\nUS\\n2009-12-31\\nTECH\\nGOOG\\n620\\n4\\nUS\\n2009-12-28\\nTECH\\nAAPL\\n211.6\\n5\\nUS\\n2009-12-29\\nTECH\\nAAPL\\n209.1\\n6\\nUS\\n2009-12-30\\nTECH\\nAAPL\\n211.6\\n7\\nUS\\n2009-12-31\\nTECH\\nAAPL\\n210.7\\nThis is akin to a SQL join operation between two tables\\nor a VLOOKUP operation in a spreadsheet such as Excel. It\\nis possible to join on multiple keys, in which case the table\\nbeing joined is currently required to have a hierarchical index\\ncorresponding to those keys. We will be working on more\\njoining and merging methods in a future release of pandas.\\nPerformance and use for Large Data Sets',\n",
       " 'corresponding to those keys. We will be working on more\\njoining and merging methods in a future release of pandas.\\nPerformance and use for Large Data Sets\\nUsing DataFrame objects over homogeneous NumPy arrays\\nfor computation incurs overhead from a number of factors:\\n• Computational functions like sum, mean, and std have\\nbeen overridden to omit missing data\\n• Most of the axis Index data structures are reliant on the\\nPython dict for performing lookups and data alignment.\\nThis also results in a slightly larger memory footprint as\\nthe dict containing the label mapping is created once\\nand then stored.\\n• The internal BlockManager data structure consolidates\\nthe data of each type (ﬂoating point, integer, boolean,\\nobject) into 2-dimensional arrays. However, this is an\\nupfront cost that speeds up row-oriented computations\\nand data alignment later.\\n• Performing repeated lookups of values by label passes\\nthrough much more Python code than simple integer-\\nbased lookups on ndarray objects.',\n",
       " 'and data alignment later.\\n• Performing repeated lookups of values by label passes\\nthrough much more Python code than simple integer-\\nbased lookups on ndarray objects.\\nThe savvy user will learn what operations are not very\\nefﬁcient in DataFrame and Series and fall back on working\\ndirectly with the underlying ndarray objects (accessible\\nvia the values attribute) in such cases. What DataFrame\\nsacriﬁces in performance it makes up for in ﬂexibility and\\nexpressiveness.\\nWith 64-bit integers representing timestamps, pandas in\\nfact provides some of the fastest data alignment routines for\\ndifferently-indexed time series to be found in open source soft-\\nware. As working with large, irregularly time series requires\\nhaving a timestamp index, pandas is well-positioned to become\\nthe gold standard for high performance open source time series\\nprocessing.\\nWith regard to memory usage and large data sets, pandas\\nis currently only designed for use with in-memory data sets.',\n",
       " 'the gold standard for high performance open source time series\\nprocessing.\\nWith regard to memory usage and large data sets, pandas\\nis currently only designed for use with in-memory data sets.\\nWe would like to expand its capability to work with data\\nsets that do not ﬁt into memory, perhaps transparently using\\nthe multiprocessing module or a parallel computing\\nbackend to orchestrate large scale computations.',\n",
       " '9\\npandas for R users\\nGiven the “DataFrame” name and feature overlap with the [R]\\nproject and its 3rd party packages, pandas will draw inevitable\\ncomparisons with R. pandas brings a robust, full-featured, and\\nintegrated data analysis toolset to Python while maintaining a\\nsimple and easy-to-use API. As nearly all data manipulations\\ninvolving data.frame objects in R can be easily expressed\\nusing the pandas DataFrame, it is relatively straightforward\\nin most cases to port R functions to Python. It would be\\nuseful to provide a migration guide for R users as we have\\nnot copied R’s naming conventions or syntax in most places,\\nrather naming based on common-sense and making the syntax\\nand API as “Pythonic” as possible.\\nR does not provide indexing functionality in nearly such a\\ndeeply integrated way as pandas does. For example, operations\\nbetween data.frame objects will proceed in R without\\nregard to whether the labels match as long as they are the',\n",
       " 'deeply integrated way as pandas does. For example, operations\\nbetween data.frame objects will proceed in R without\\nregard to whether the labels match as long as they are the\\nsame length and width. Some R packages, such as zoo and\\nxts provides indexed data structures with data alignment,\\nbut they are largely specialized to ordered time series data.\\nHierarchical indexing with constant-time subset selection is\\nanother signiﬁcant feature missing from R’s data structures.\\nOutside of the scope of this paper is a rigorous performance\\ncomparison of R and pandas. In almost all of the benchmarks\\nwe have run comparing R and pandas, pandas signiﬁcantly\\noutperforms R.\\nOther features of note\\nThere are many other features in pandas worth exploring for\\nthe interested users:\\n• Time series functionality: date range generation, shifting\\nand lagging, frequency conversion and forward/backward\\nﬁlling\\n• Integration with [matplotlib] to concisely generate plots\\nwith metadata',\n",
       " '• Time series functionality: date range generation, shifting\\nand lagging, frequency conversion and forward/backward\\nﬁlling\\n• Integration with [matplotlib] to concisely generate plots\\nwith metadata\\n• Moving window statistics (e.g. moving standard devia-\\ntion, exponentially weighted moving average) and moving\\nwindow linear and panel regression\\n• 3-dimensional Panel data structure for manipulating\\ncollections of DataFrame objects\\n• Sparse versions of the data structures\\n• Robust IO tools for reading and writing pandas objects to\\nﬂat ﬁles (delimited text, CSV, Excel) and HDF5 format\\nRelated packages\\nA number of other Python packages have some degree of\\nfeature overlap with pandas. Among these, la ([Larry]) is\\nthe most similar, as it implements a labeled ndarray object\\nintending to closely mimic NumPy arrays. Since ndarray\\nis only applicable many problems in its homogeneous (non-\\nstructured dtype) form, in pandas we have distanced our-\\nselves from ndarray to instead provide a more ﬂexible,',\n",
       " 'is only applicable many problems in its homogeneous (non-\\nstructured dtype) form, in pandas we have distanced our-\\nselves from ndarray to instead provide a more ﬂexible,\\n(potentially) heterogeneous, size-mutable data structure. The\\nreferences include a some other packages of interest.\\npandas will soon become a dependency of statsmodels\\n([StaM]), the main statistics and econometric library in Python,\\nto make statistical modeling and data analysis tools in Python\\nmore cohesive and integrated. We plan to combine pandas\\nwith a formula framework to make specifying statistical mod-\\nels easy and intuitive when working with a DataFrame of\\ndata, for example.\\nConclusions\\nWe believe that in the coming years there will be great oppor-\\ntunity to attract users in need of statistical data analysis tools\\nto Python who might have previously chosen R, MATLAB,\\nor another research environment. By designing robust, easy-\\nto-use data structures that cohere with the rest of the scientiﬁc',\n",
       " 'to Python who might have previously chosen R, MATLAB,\\nor another research environment. By designing robust, easy-\\nto-use data structures that cohere with the rest of the scientiﬁc\\nPython stack, we can make Python a compelling choice for\\ndata analysis applications. In our opinion, pandas provides\\na solid foundation upon which a very powerful data analysis\\necosystem can be established.\\nREFERENCES\\n[pandas]\\nW. McKinney, pandas: a python data analysis library, http:\\n//pandas.sourceforge.net\\n[scipy2010]\\nW. McKinney, Data Structures for Statistical Computing in\\nPython Proceedings of the 9th Python in Science Conference,\\nhttp://http://conference.scipy.org/. 2010\\n[Larry]\\nK. Goodman. la / larry: ndarray with labeled axes, http://larry.\\nsourceforge.net/\\n[SciTS]\\nM. Knox, P. Gerard-Marchant, scikits.timeseries: python time\\nseries analysis, http://pytseries.sourceforge.net/\\n[StaM]\\nS. Seabold, J. Perktold, J. Taylor, statsmodels: statistical\\nmodeling in Python, http://statsmodels.sourceforge.net',\n",
       " 'series analysis, http://pytseries.sourceforge.net/\\n[StaM]\\nS. Seabold, J. Perktold, J. Taylor, statsmodels: statistical\\nmodeling in Python, http://statsmodels.sourceforge.net\\n[SciL]\\nD. Cournapeau, et al., scikit-learn: machine learning in\\nPython, http://scikit-learn.sourceforge.net\\n[PyMC]\\nC. Fonnesbeck, A. Patil, D. Huard, PyMC: Markov Chain\\nMonte Carlo for Python, http://code.google.com/p/pymc/\\n[Tab]\\nD. Yamins, E. Angelino, tabular: tabarray data structure for\\n2D data, http://parsemydata.com/tabular/\\n[NumPy]\\nT. Oliphant, http://numpy.scipy.org\\n[SciPy]\\nE. Jones, T. Oliphant, P. Peterson, http://scipy.org\\n[matplotlib]\\nJ. Hunter, et al., matplotlib: Python plotting, http://matplotlib.\\nsourceforge.net/\\n[EPD]\\nEnthought, Inc., EPD: Enthought Python Distribution, http:\\n//www.enthought.com/products/epd.php\\n[Pythonxy]\\nP. Raybaut, Python(x,y): Scientiﬁc-oriented Python distribu-\\ntion, http://www.pythonxy.com/\\n[CRAN]\\nThe R Project for Statistical Computing, http://cran.r-project.\\norg/\\n[Cython]',\n",
       " '[Pythonxy]\\nP. Raybaut, Python(x,y): Scientiﬁc-oriented Python distribu-\\ntion, http://www.pythonxy.com/\\n[CRAN]\\nThe R Project for Statistical Computing, http://cran.r-project.\\norg/\\n[Cython]\\nG. Ewing, R. W. Bradshaw, S. Behnel, D. S. Seljebotn, et al.,\\nThe Cython compiler, http://cython.org\\n[IPython]\\nFernando Pérez, Brian E. Granger, IPython: A System for\\nInteractive Scientiﬁc Computing, Computing in Science and\\nEngineering, vol. 9, no. 3, pp. 21-29, May/June 2007,\\ndoi:10.1109/MCSE.2007.53. http://ipython.org\\n[Grun]\\nBatalgi,\\nGrunfeld\\ndata\\nset,\\nhttp://www.wiley.com/legacy/\\nwileychi/baltagi/\\n[nipy]\\nJ. Taylor, F. Perez, et al., nipy: Neuroimaging in Python, http:\\n//nipy.sourceforge.net\\n[pydataframe] A. Straw, F. Finkernagel, pydataframe, http://code.google.com/\\np/pydataframe/\\n[R]\\nR Development Core Team. 2010, R: A Language and Envi-\\nronment for Statistical Computing, http://www.R-project.org\\n[MATLAB]\\nThe MathWorks Inc. 2010, MATLAB, http://www.mathworks.\\ncom\\n[Stata]',\n",
       " '[R]\\nR Development Core Team. 2010, R: A Language and Envi-\\nronment for Statistical Computing, http://www.R-project.org\\n[MATLAB]\\nThe MathWorks Inc. 2010, MATLAB, http://www.mathworks.\\ncom\\n[Stata]\\nStatCorp. 2010, Stata Statistical Software: Release 11 http:\\n//www.stata.com\\n[SAS]\\nSAS Institute Inc., SAS System, http://www.sas.com\\n[Bryant]\\nBryant, P. G. and Smith, M (1995) Practical Data Analysis:\\nCase Studies in Business Statistics. Homewood, IL: Richard\\nD. Irwin Publishing:',\n",
       " 'PyTorch: An Imperative Style, High-Performance\\nDeep Learning Library\\nAdam Paszke\\nUniversity of Warsaw\\nadam.paszke@gmail.com\\nSam Gross\\nFacebook AI Research\\nsgross@fb.com\\nFrancisco Massa\\nFacebook AI Research\\nfmassa@fb.com\\nAdam Lerer\\nFacebook AI Research\\nalerer@fb.com\\nJames Bradbury\\nGoogle\\njekbradbury@gmail.com\\nGregory Chanan\\nFacebook AI Research\\ngchanan@fb.com\\nTrevor Killeen\\nSelf Employed\\nkilleent@cs.washington.edu\\nZeming Lin\\nFacebook AI Research\\nzlin@fb.com\\nNatalia Gimelshein\\nNVIDIA\\nngimelshein@nvidia.com\\nLuca Antiga\\nOrobix\\nluca.antiga@orobix.com\\nAlban Desmaison\\nOxford University\\nalban@robots.ox.ac.uk\\nAndreas Köpf\\nXamla\\nandreas.koepf@xamla.com\\nEdward Yang\\nFacebook AI Research\\nezyang@fb.com\\nZach DeVito\\nFacebook AI Research\\nzdevito@cs.stanford.edu\\nMartin Raison\\nNabla\\nmartinraison@gmail.com\\nAlykhan Tejani\\nTwitter\\natejani@twitter.com\\nSasank Chilamkurthy\\nQure.ai\\nsasankchilamkurthy@gmail.com\\nBenoit Steiner\\nFacebook AI Research\\nbenoitsteiner@fb.com\\nLu Fang\\nFacebook\\nlufang@fb.com\\nJunjie Bai',\n",
       " 'Alykhan Tejani\\nTwitter\\natejani@twitter.com\\nSasank Chilamkurthy\\nQure.ai\\nsasankchilamkurthy@gmail.com\\nBenoit Steiner\\nFacebook AI Research\\nbenoitsteiner@fb.com\\nLu Fang\\nFacebook\\nlufang@fb.com\\nJunjie Bai\\nFacebook\\njbai@fb.com\\nSoumith Chintala\\nFacebook AI Research\\nsoumith@gmail.com\\nAbstract\\nDeep learning frameworks have often focused on either usability or speed, but\\nnot both. PyTorch is a machine learning library that shows that these two goals\\nare in fact compatible: it provides an imperative and Pythonic programming style\\nthat supports code as a model, makes debugging easy and is consistent with other\\npopular scientiﬁc computing libraries, while remaining efﬁcient and supporting\\nhardware accelerators such as GPUs.\\nIn this paper, we detail the principles that drove the implementation of PyTorch\\nand how they are reﬂected in its architecture. We emphasize that every aspect of\\nPyTorch is a regular Python program under the full control of its user. We also',\n",
       " 'and how they are reﬂected in its architecture. We emphasize that every aspect of\\nPyTorch is a regular Python program under the full control of its user. We also\\nexplain how the careful and pragmatic implementation of the key components of\\nits runtime enables them to work together to achieve compelling performance.\\nWe demonstrate the efﬁciency of individual subsystems, as well as the overall\\nspeed of PyTorch on several common benchmarks.\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\narXiv:1912.01703v1  [cs.LG]  3 Dec 2019',\n",
       " '1\\nIntroduction\\nWith the increased interest in deep learning in recent years, there has been an explosion of machine\\nlearning tools. Many popular frameworks such as Caffe [1], CNTK [2], TensorFlow [3], and\\nTheano [4], construct a static dataﬂow graph that represents the computation and which can then be\\napplied repeatedly to batches of data. This approach provides visibility into the whole computation\\nahead of time, and can theoretically be leveraged to improve performance and scalability. However, it\\ncomes at the cost of ease of use, ease of debugging, and ﬂexibility of the types of computation that\\ncan be represented.\\nPrior work has recognized the value of dynamic eager execution for deep learning, and some recent\\nframeworks implement this deﬁne-by-run approach, but do so either at the cost of performance\\n(Chainer [5]) or using a less expressive, faster language (Torch [6], DyNet [7]), which limits their\\napplicability.',\n",
       " '(Chainer [5]) or using a less expressive, faster language (Torch [6], DyNet [7]), which limits their\\napplicability.\\nHowever, with careful implementation and design choices, dynamic eager execution can be achieved\\nlargely without sacriﬁcing performance. This paper introduces PyTorch, a Python library that\\nperforms immediate execution of dynamic tensor computations with automatic differentiation and\\nGPU acceleration, and does so while maintaining performance comparable to the fastest current\\nlibraries for deep learning. This combination has turned out to be very popular in the research\\ncommunity with, for instance, 296 ICLR 2019 submissions mentioning PyTorch.\\n2\\nBackground\\nFour major trends in scientiﬁc computing have become increasingly important for deep learning.\\nFirst, starting in the 1960s, the development of domain speciﬁc languages such as APL [8], MATLAB\\n[9], R [10] and Julia [11], turned multidimensional arrays (often referred to as tensors) into ﬁrst-class',\n",
       " '[9], R [10] and Julia [11], turned multidimensional arrays (often referred to as tensors) into ﬁrst-class\\nobjects supported by a comprehensive set of mathematical primitives (or operators) to manipulate\\nthem. Separately, libraries such as NumPy[12], Torch[6], Eigen[13] and Lush[14] made array-based\\nprogramming productive in general purpose languages such as Python, Lisp, C++ and Lua.\\nSecond, the development of automatic differentiation [15] made it possible to fully automate\\nthe daunting labor of computing derivatives. This made it signiﬁcantly easier to experiment with\\ndifferent machine learning approaches while still allowing for efﬁcient gradient based optimization.\\nThe autograd [16] package popularized the use of this technique for NumPy arrays, and similar\\napproaches are used in frameworks such as Chainer [5], DyNet [7], Lush [14], Torch [6], Jax [17]\\nand Flux.jl [18].\\nThird, with the advent of the free software movement, the scientiﬁc community moved away from',\n",
       " 'and Flux.jl [18].\\nThird, with the advent of the free software movement, the scientiﬁc community moved away from\\nclosed proprietary software such as Matlab[9], and towards the open-source Python ecosystem\\nwith packages like NumPy [12], SciPy [19], and Pandas [20]. This fulﬁlled most of the numerical\\nanalysis needs of researchers while allowing them to take advantage of a vast repository of libraries\\nto handle dataset preprocessing, statistical analysis, plotting, and more. Moreover, the openness,\\ninteroperability, and ﬂexibility of free software fostered the development of vibrant communities that\\ncould quickly address new or changing needs by extending the existing functionality of a library or if\\nneeded by developing and releasing brand new ones. While there is a rich offering of open-source\\nsoftware for neural networks in languages other than Python, starting with Lush [14] in Lisp, Torch [6]',\n",
       " 'software for neural networks in languages other than Python, starting with Lush [14] in Lisp, Torch [6]\\nin C++, Objective-C and Lua, EBLearn [21] in C++, Caffe [1] in C++, the network effects of a large\\necosystem such as Python made it an essential skill to jumpstart one’s research. Hence, since 2014,\\nmost deep learning frameworks converged on a Python interface as an essential feature.\\nFinally, the availability and commoditization of general-purpose massively parallel hardware such\\nas GPUs provided the computing power required by deep learning methods. Specialized libraries\\nsuch as cuDNN [22], along with a body of academic work (such as [23] and [24]), produced a\\nset of high-performance reusable deep learning kernels that enabled frameworks such as Caffe [1],\\nTorch7 [25], or TensorFlow [3] to take advantage of these hardware accelerators.\\nPyTorch builds on these trends by providing an array-based programming model accelerated by GPUs',\n",
       " 'Torch7 [25], or TensorFlow [3] to take advantage of these hardware accelerators.\\nPyTorch builds on these trends by providing an array-based programming model accelerated by GPUs\\nand differentiable via automatic differentiation integrated in the Python ecosystem.\\n2',\n",
       " '3\\nDesign principles\\nPyTorch’s success stems from weaving previous ideas into a design that balances speed and ease of\\nuse. There are four main principles behind our choices:\\nBe Pythonic\\nData scientists are familiar with the Python language, its programming model, and its\\ntools. PyTorch should be a ﬁrst-class member of that ecosystem. It follows the commonly established\\ndesign goals of keeping interfaces simple and consistent, ideally with one idiomatic way of doing\\nthings. It also integrates naturally with standard plotting, debugging, and data processing tools.\\nPut researchers ﬁrst\\nPyTorch strives to make writing models, data loaders, and optimizers as\\neasy and productive as possible. The complexity inherent to machine learning should be handled\\ninternally by the PyTorch library and hidden behind intuitive APIs free of side-effects and unexpected\\nperformance cliffs.\\nProvide pragmatic performance\\nTo be useful, PyTorch needs to deliver compelling performance,',\n",
       " 'performance cliffs.\\nProvide pragmatic performance\\nTo be useful, PyTorch needs to deliver compelling performance,\\nalthough not at the expense of simplicity and ease of use. Trading 10% of speed for a signiﬁcantly\\nsimpler to use model is acceptable; 100% is not. Therefore, its implementation accepts added\\ncomplexity in order to deliver that performance. Additionally, providing tools that allow researchers\\nto manually control the execution of their code will empower them to ﬁnd their own performance\\nimprovements independent of those that the library provides automatically.\\nWorse is better [26]\\nGiven a ﬁxed amount of engineering resources, and all else being equal, the\\ntime saved by keeping the internal implementation of PyTorch simple can be used to implement\\nadditional features, adapt to new situations, and keep up with the fast pace of progress in the ﬁeld of\\nAI. Therefore it is better to have a simple but slightly incomplete solution than a comprehensive but',\n",
       " 'AI. Therefore it is better to have a simple but slightly incomplete solution than a comprehensive but\\ncomplex and hard to maintain design.\\n4\\nUsability centric design\\n4.1\\nDeep learning models are just Python programs\\nIn a surprisingly short amount of time, machine learning grew from recognizing individual digits [27]\\ninto autonomously playing StarCraft [28]. Consequently, the neural networks themselves evolved\\nrapidly from simple sequences of feed forward layers into incredibly varied numerical programs\\noften composed of many loops and recursive functions. To support this growing complexity, PyTorch\\nforegoes the potential beneﬁts of a graph-metaprogramming based approach to preserve the imperative\\nprogramming model of Python. This design was pioneered for model authoring by Chainer[5] and\\nDynet[7]. PyTorch extends this to all aspects of deep learning workﬂows. Deﬁning layers, composing\\nmodels, loading data, running optimizers, and parallelizing the training process are all expressed',\n",
       " 'Dynet[7]. PyTorch extends this to all aspects of deep learning workﬂows. Deﬁning layers, composing\\nmodels, loading data, running optimizers, and parallelizing the training process are all expressed\\nusing the familiar concepts developed for general purpose programming.\\nThis solution ensures that any new potential neural network architecture can be easily implemented\\nwith PyTorch. For instance, layers (which in modern machine learning should really be understood\\nas stateful functions with implicit parameters) are typically expressed as Python classes whose\\nconstructors create and initialize their parameters, and whose forward methods process an input\\nactivation. Similarly, models are usually represented as classes that compose individual layers, but let\\nus state again that nothing forces the user to structure their code in that way. Listing 1 demonstrates\\nhow an entire model can be created by composing functionality provided by PyTorch such as 2d',\n",
       " 'us state again that nothing forces the user to structure their code in that way. Listing 1 demonstrates\\nhow an entire model can be created by composing functionality provided by PyTorch such as 2d\\nconvolution, matrix multiplication, dropout, and softmax to classify gray-scale images. Note that\\nlinear layers are of course part of the library, but we show an example implementation to highlight\\nhow simple it is.\\n3',\n",
       " 'class LinearLayer(Module):\\nclass FullBasicModel(nn.Module):\\ndef __init__(self, in_sz, out_sz):\\ndef __init__(self):\\nsuper().__init__()\\nsuper().__init__()\\nt1 = torch.randn(in_sz, out_sz)\\nself.conv = nn.Conv2d(1, 128, 3)\\nself.w = nn.Parameter(t1)\\nself.fc = LinearLayer(128, 10)\\nt2 = torch.randn(out_sz)\\nself.b = nn.Parameter(t2)\\ndef forward(self, x):\\nt1 = self.conv(x)\\ndef forward(self, activations):\\nt2 = nn.functional.relu(t1)\\nt = torch.mm(activations, self.w)\\nt3 = self.fc(t1)\\nreturn t + self.b\\nreturn nn.functional.softmax(t3)\\nListing 1: A custom layer used as a building block for a simple but complete neural network.\\nThis “everything is a just a program” philosophy is not limited to just the models, and applies to\\noptimizers and data loaders as well. This facilitates the experimentation of new training techniques.\\nFor example, to implement the very popular generative adversarial networks, one needs to specify',\n",
       " 'optimizers and data loaders as well. This facilitates the experimentation of new training techniques.\\nFor example, to implement the very popular generative adversarial networks, one needs to specify\\ntwo separate models (the generator and the discriminator), and two loss functions that depend on both\\nmodels at the same time. Rigid APIs would struggle with this setup, but the simple design employed\\nin PyTorch easily adapts to this setting as shown in Listing 2.\\ndiscriminator = create_discriminator()\\ngenerator = create_generator()\\noptimD = optim.Adam(discriminator.parameters())\\noptimG = optim.Adam(generator.parameters())\\ndef step(real_sample):\\n# (1) Update Discriminator\\nerrD_real = loss(discriminator(real_sample), real_label)\\nerrD_real.backward()\\nfake = generator(get_noise())\\nerrD_fake = loss(discriminator(fake.detach(), fake_label)\\nerrD_fake.backward()\\noptimD.step()\\n# (2) Update Generator\\nerrG = loss(discriminator(fake), real_label)\\nerrG.backward()\\noptimG.step()',\n",
       " 'errD_fake = loss(discriminator(fake.detach(), fake_label)\\nerrD_fake.backward()\\noptimD.step()\\n# (2) Update Generator\\nerrG = loss(discriminator(fake), real_label)\\nerrG.backward()\\noptimG.step()\\nListing 2: Simpliﬁed training of a generative adversarial networks.\\nSince PyTorch programs execute eagerly, all the features of Python are available throughout the\\nwhole design process. Print statements, standard debuggers, and common visualization tools like\\nmatplotlib all work as expected. Users do not have to wait for lengthy compilation before they can\\nstart running their programs, and more importantly intermediate computations can be observed to\\nunderstand how a model works and whether its results are correct.\\n4.2\\nInteroperability and extensibility\\nEasy and efﬁcient interoperability is one of the top priorities for PyTorch because it opens the\\npossibility to leverage the rich ecosystem of Python libraries as part of user programs. Hence,',\n",
       " 'Easy and efﬁcient interoperability is one of the top priorities for PyTorch because it opens the\\npossibility to leverage the rich ecosystem of Python libraries as part of user programs. Hence,\\nPyTorch allows for bidirectional exchange of data with external libraries. For example, it provides\\na mechanism to convert between NumPy arrays and PyTorch tensors using the torch.from_numpy()\\nfunction and .numpy() tensor method. Similar functionality is also available to exchange data stored\\nusing the DLPack [29] format. Note that this exchange happens in both cases without any data\\ncopying – objects on both sides only describe how to interpret a memory region which is shared\\namong them. Hence, those operations are actually extremely cheap, and take constant time no matter\\nhow large the converted arrays are.\\n4',\n",
       " 'Moreover, many of the critical systems are designed speciﬁcally to be extensible. For instance, the\\nautomatic differentiation system allows users to add support for custom differentiable functions.\\nTo do that users can deﬁne a new subclass of torch.autograd.Function that implements forward()\\nand backward() methods, which specify the function and its derivative (or more formally the vector-\\nJacobian product). Similarly new datasets can be added by subclassing torch.utils.data.Dataset\\nand implementing two methods: __getitem__ (the indexing operator) and __len__ (the length op-\\nerator), making datasets behave like (possibly lazy) lists. How these work is completely up to the\\nimplementer, and many users leverage other Python packages for data loading. The DataLoader class\\nconsumes objects conforming to this interface and provides an iterator over the data which takes\\ncare of shufﬂing, batching, parallelization, and management of pinned CUDA memory to improve\\nthroughput.',\n",
       " 'care of shufﬂing, batching, parallelization, and management of pinned CUDA memory to improve\\nthroughput.\\nMost importantly, users are free to replace any component of PyTorch that does not meet the needs or\\nperformance requirements of their project. They are all designed to be completely interchangeable,\\nand PyTorch takes great care not to impose any particular solution.\\n4.3\\nAutomatic differentiation\\nSince gradient based optimization is vital to deep learning, PyTorch must be able to automatically\\ncompute gradients of models speciﬁed by our users, and those can be arbitrary Python programs.\\nHowever, Python is a dynamic programming language that allows changing most behaviors at\\nruntime, making ahead of time source-to-source differentiation cumbersome. Instead, PyTorch uses\\nthe operator overloading approach, which builds up a representation of the computed function every\\ntime it is executed. In its current implementation [30], PyTorch performs reverse-mode automatic',\n",
       " 'the operator overloading approach, which builds up a representation of the computed function every\\ntime it is executed. In its current implementation [30], PyTorch performs reverse-mode automatic\\ndifferentiation, which computes the gradient of a scalar output with respect to a multivariate input.\\nDifferentiating functions with more outputs than inputs is more efﬁciently executed using forward-\\nmode automatic differentiation, but this use case is less common for machine learning applications.\\nPyTorch can be easily extended to perform forward-mode differentiation using array-level dual\\nnumbers [31, 32].\\nAnother interesting and uncommon feature of our system is that it can differentiate through code\\nemploying mutation on tensors, which is one of the basic building blocks of imperative programs.\\nTo ensure safety, we have implemented a versioning system for tensors, which lets us track their\\nmodiﬁcations and ensure that we always use the data we expect. One interesting tradeoff is that',\n",
       " 'To ensure safety, we have implemented a versioning system for tensors, which lets us track their\\nmodiﬁcations and ensure that we always use the data we expect. One interesting tradeoff is that\\nwhile we could utilize techniques like copy-on-write to support arbitrary programs, we chose to not\\ngo down this path, as performance-wise it is usually beneﬁcial for the users to rewrite their code\\nto ensure that no copies have to be performed. Hence, while most mutations are benign and can\\nbe handled automatically, the really complicated cases result in a user error, which lets them know\\nthat they likely want to restructure the program. This allows us to avoid introducing subtle and\\nhard-to-ﬁnd performance cliffs.\\n5\\nPerformance focused implementation\\nRunning deep learning algorithms efﬁciently from a Python interpreter is notoriously challenging: for\\ninstance, the global interpreter lock [33] effectively ensures that only one of any number of concurrent',\n",
       " 'instance, the global interpreter lock [33] effectively ensures that only one of any number of concurrent\\nthreads is running at any given time. Deep learning frameworks based on the construction of a static\\ndata-ﬂow graph sidestep this problem by deferring the evaluation of the computation to a custom\\ninterpreter.\\nPyTorch solved the problem differently, by carefully optimizing every aspect of its execution while\\nsimultaneously empowering its users to easily leverage additional optimization strategies.\\n5.1\\nAn efﬁcient C++ core\\nDespite being closely integrated in the Python ecosystem, most of PyTorch is written in C++ to\\nachieve high performance. This core libtorch library implements the tensor data structure, the GPU\\nand CPU operators, and basic parallel primitives. It also provides the automatic differentiation system,\\nincluding the gradient formulas for most built-in functions. This ensures that the computation of the',\n",
       " 'including the gradient formulas for most built-in functions. This ensures that the computation of the\\nderivatives of functions composed of core PyTorch operators is executed entirely in a multithreaded\\nevaluator which does not require holding the Python global interpreter lock [33]. Python bindings\\n5',\n",
       " 'are generated using YAML meta-data ﬁles. An interesting side-effect of this approach is that it\\nallowed our community to quickly create bindings to multiple other languages resulting in projects\\nlike NimTorch [34], hasktorch [35] and others.\\nThis design also allowed us to create ﬁrst-class C++ bindings and modeling libraries that can be\\nused in places where Python is inconvenient, such as the game engine for Starcraft [36] or on mobile\\nplatforms. It is even possible to take the Python code describing a PyTorch model and run it without\\nPython using the TorchScript engine [37].\\n5.2\\nSeparate control and data ﬂow\\nPyTorch maintains a strict separation between its control (i.e. program branches, loops) and data ﬂow\\n(i.e. tensors and the operations performed on them). The resolution of the control ﬂow is handled\\nby Python and optimized C++ code executed on the host CPU, and result in a linear sequence of\\noperator invocations on the device. Operators can be run either on CPU or on GPU.',\n",
       " 'by Python and optimized C++ code executed on the host CPU, and result in a linear sequence of\\noperator invocations on the device. Operators can be run either on CPU or on GPU.\\nPyTorch is designed to execute operators asynchronously on GPU by leveraging the CUDA stream\\nmechanism [38] to queue CUDA kernel invocations to the GPUs hardware FIFO. This allows the\\nsystem to overlap the execution of Python code on CPU with tensor operators on GPU. Because\\nthe tensor operations usually take a signiﬁcant amount of time, this lets us saturate the GPU and\\nreach peak performance even in an interpreted language with fairly high overhead like Python. Note\\nthat this mechanism is nearly invisible to the user. Unless they implement their own multi-stream\\nprimitives all of the CPU-GPU synchronization is handled by the library.\\nPyTorch could leverage a similar mechanism to also execute operators asynchronously on the CPU.',\n",
       " 'primitives all of the CPU-GPU synchronization is handled by the library.\\nPyTorch could leverage a similar mechanism to also execute operators asynchronously on the CPU.\\nHowever the costs of cross-thread communication and synchronization would negate the performance\\nbeneﬁt of such an optimization.\\n5.3\\nCustom caching tensor allocator\\nAlmost every operator must dynamically allocate an output tensor to hold the result of its execution.\\nIt is therefore critical to optimize the speed of the dynamic memory allocators. PyTorch can rely on\\noptimized libraries [39–41] to handle this task on CPU. However, on GPU the cudaFree routine may\\nblock its caller until all previously queued work on all GPUs completes. To avoid this bottleneck,\\nPyTorch implements a custom allocator which incrementally builds up a cache of CUDA memory\\nand reassigns it to later allocations without further use of CUDA APIs. The incremental allocation',\n",
       " 'PyTorch implements a custom allocator which incrementally builds up a cache of CUDA memory\\nand reassigns it to later allocations without further use of CUDA APIs. The incremental allocation\\nis also crucial for better interoperability, because taking up all GPU memory ahead of time would\\nprevent the user from utilizing other GPU-enabled Python packages.\\nTo further improve its effectiveness, this allocator was tuned for the speciﬁc memory usage patterns of\\ndeep learning. For example, it rounds up allocations to multiples of 512 bytes to avoid fragmentation\\nissues. Moreover, it maintains a distinct pool of memory for every CUDA stream (work queue).\\nThe one-pool-per-stream design assumption simpliﬁes the implementation and improves the perfor-\\nmance of the allocator: because the CPU runs ahead of the GPU, memory is freed on the CPU before\\nits last use on the GPU ﬁnishes. Since streams serialize execution, if the free precedes the reallocation',\n",
       " 'its last use on the GPU ﬁnishes. Since streams serialize execution, if the free precedes the reallocation\\non the CPU, the same order will occur on the GPU. So the allocator can reallocate memory freed on\\nthe CPU immediately as long as the new allocation is used on the same stream as the freed region.\\nHowever, if an allocation was last used on one stream and then allocated on another, additional\\nsynchronization is needed.\\nThe one-pool-per-stream design seems limiting since the allocations end up fragmented per stream, but\\nin practice PyTorch almost never uses multiple streams. It is notoriously hard to write CUDA kernels\\nin a way that would let them cooperatively share the GPU because exact scheduling is hardware\\ncontrolled. In practice, kernel writers usually resort to monolithic kernels that combine multiple tasks.\\nData loading and distributed computing utilities are exceptions to the one stream design, and they',\n",
       " 'Data loading and distributed computing utilities are exceptions to the one stream design, and they\\ncarefully insert additional synchronization to avoid bad interactions with the allocator.\\nWhile this design is susceptible to certain corner cases, it almost never exhibits unwanted behaviors\\nin practical code. Most of our users are not aware of its existence.\\n6',\n",
       " '5.4\\nMultiprocessing\\nDue to the global interpreter lock (GIL) Python’s default implementation does not allow concurrent\\nthreads to execute in parallel. To alleviate this problem, the Python community has established a\\nstandard multiprocessing module, containing a number of utilities that allow users to easily spawn\\nchild processes and implement basic inter-process communication primitives.\\nHowever, the implementation of the primitives uses the same form of serialization used for on-disk\\npersistence, which is inefﬁcient when dealing with large arrays. Hence, PyTorch extends the Python\\nmultiprocessing module into torch.multiprocessing, which is a drop-in replacement for the\\nbuilt in package and automatically moves the data of tensors sent to other processes to shared memory\\ninstead of sending it over the communication channel.\\nThis design greatly improves performance and makes the process isolation weaker, resulting in a',\n",
       " 'instead of sending it over the communication channel.\\nThis design greatly improves performance and makes the process isolation weaker, resulting in a\\nprogramming model which more closely resembles regular threaded programs. Users can easily\\nimplement heavily parallel programs that operate on independent GPUs but later synchronize gradients\\nusing all-reduce style primitives.\\nAnother unique feature of this system is that it transparently handles sharing of CUDA tensors,\\nmaking it easy to implement techniques like Hogwild [42].\\n5.5\\nReference counting\\nUsers often design their models to utilize all memory available during training, and increasing batch\\nsizes is a common technique of speeding up the process. Therefore, to deliver great performance,\\nPyTorch has to treat memory as a scarce resource that it needs to manage carefully.\\nLibraries with eager semantics have to manage tensor memory without knowing how it will be used',\n",
       " 'PyTorch has to treat memory as a scarce resource that it needs to manage carefully.\\nLibraries with eager semantics have to manage tensor memory without knowing how it will be used\\nin the future. Garbage collection is the typical way to handle this automatically because it has good\\namortized performance. In this approach, the runtime periodically investigates the state of the system,\\nenumerates used objects and frees everything else. However, by deferring the deallocation, it causes\\nthe program to use more memory overall [43]. Given the scarcity of GPU memory, these overheads\\nare unacceptable. In fact, Torch7 utilized the garbage collector built into Lua, and a common anti-\\npattern among the users was to sprinkle the program with explicit triggers to the garbage collector,\\nhoping that the memory errors go away.\\nPyTorch takes a different approach: it relies on a reference counting scheme to track the number of',\n",
       " 'hoping that the memory errors go away.\\nPyTorch takes a different approach: it relies on a reference counting scheme to track the number of\\nuses of each tensor, and frees the underlying memory immediately once this count reaches zero. Note\\nthat PyTorch tracks both references internal to the libtorch library and external references made by\\nusers in their Python code by integrating with Python’s own reference counting mechanism. This\\nensures that memory is released exactly when tensors become unneeded.\\nOne notable caveat is that we can only guarantee the desired performance characteristics in implemen-\\ntations of languages that either already utilize reference counting (CPython, Swift, but not PyPy or\\nmany scripting languages such as Lua), and those that allow for user-deﬁned behavior for assignment,\\ncopies, and moves (e.g. C++, Rust). Bindings to implementations that do not satisfy those criteria\\nwill have to implement their own specialized memory management on top of PyTorch.\\n6',\n",
       " 'copies, and moves (e.g. C++, Rust). Bindings to implementations that do not satisfy those criteria\\nwill have to implement their own specialized memory management on top of PyTorch.\\n6\\nEvaluation\\nIn this section we compare the performance of PyTorch with several other commonly-used deep\\nlearning libraries, and ﬁnd that it achieves competitive performance across a range of tasks. All\\nexperiments were performed on a workstation with two Intel Xeon E5-2698 v4 CPUs and one\\nNVIDIA Quadro GP100 GPU.\\n6.1\\nAsynchronous dataﬂow\\nWe start by quantifying the ability of PyTorch to asynchronously execute dataﬂow on GPU. We use\\nthe built-in proﬁler [44] to instrument various benchmarks and record a timeline of the execution of a\\nsingle training step.\\n7',\n",
       " 'Figure 1 shows a representative timeline of execution for the ﬁrst few operations of a ResNet-50\\nmodel. The host CPU which queues the work quickly outpaces the execution of the operators on\\nthe GPU. This allows PyTorch to achieve almost perfect device utilization. In this example, GPU\\nexecution takes around three times longer than CPU scheduling. The exact ratio depends on the\\nrelative performance of the host CPU and the GPU, as well as the number of elements in each tensor\\nand the average arithmetic complexity of the ﬂoating point computations to be performed on the\\nGPU.\\nFigure 1: A trace of the ﬁrst few operators of Resnet-50. The top row depicts the execution of the control\\nﬂow running on the host CPU. The gray areas are Python code executed by its interpreter. The colored areas\\ncorrespond to the work done on the host CPU to queue various operators (convolution, batch normalization, and',\n",
       " 'correspond to the work done on the host CPU to queue various operators (convolution, batch normalization, and\\nso on). The bottom row shows the corresponding execution of those operators on the GPU. The arrows pair the\\ntwo events in time.\\n6.2\\nMemory management\\nWe used the NVIDIA proﬁler to trace the execution of the CUDA runtime as well as the execution\\nof the CUDA kernels launched during one training iteration of the ResNet-50 model. As shown in\\nFigure 2, the behavior of the ﬁrst iteration differs signiﬁcantly from that of subsequent ones. At\\nﬁrst, calls to the CUDA memory management functions (cudaMalloc and cudaFree) slow down the\\nexecution quite dramatically by blocking the CPU thread for long periods of time, hence lowering\\nthe utilization of the GPU. This effect disappears in subsequent iterations as the PyTorch caching\\nmemory allocator starts reusing previously allocated regions.\\nFigure 2: Annotated traces of the execution of ResNet-50 on GPU.\\n6.3\\nBenchmarks',\n",
       " 'memory allocator starts reusing previously allocated regions.\\nFigure 2: Annotated traces of the execution of ResNet-50 on GPU.\\n6.3\\nBenchmarks\\nFinally, we can get an overall sense of single-machine eager mode performance of PyTorch by com-\\nparing it to three popular graph-based deep learning frameworks (CNTK, MXNet and TensorFlow), a\\ndeﬁne-by-run framework (Chainer), and production oriented platform (PaddlePaddle). The Appendix\\ndetails all the steps needed to reproduce our setup.\\nOur results are summarized in Table 1. On all the benchmarks, the performance of PyTorch is within\\n17% of that of of the fastest framework. We attribute this result to the fact that these tools ofﬂoad\\nmost of the computation to the same version of the cuDNN and cuBLAS libraries.\\nFramework\\nThroughput (higher is better)\\nAlexNet\\nVGG-19\\nResNet-50\\nMobileNet\\nGNMTv2\\nNCF\\nChainer\\n778 ± 15\\nN/A\\n219 ± 1\\nN/A\\nN/A\\nN/A\\nCNTK\\n845 ± 8\\n84 ± 3\\n210 ± 1\\nN/A\\nN/A\\nN/A\\nMXNet\\n1554 ± 22\\n113 ± 1\\n218 ± 2\\n444 ± 2\\nN/A\\nN/A\\nPaddlePaddle',\n",
       " 'AlexNet\\nVGG-19\\nResNet-50\\nMobileNet\\nGNMTv2\\nNCF\\nChainer\\n778 ± 15\\nN/A\\n219 ± 1\\nN/A\\nN/A\\nN/A\\nCNTK\\n845 ± 8\\n84 ± 3\\n210 ± 1\\nN/A\\nN/A\\nN/A\\nMXNet\\n1554 ± 22\\n113 ± 1\\n218 ± 2\\n444 ± 2\\nN/A\\nN/A\\nPaddlePaddle\\n933 ± 123\\n112 ± 2\\n192 ± 4\\n557 ± 24\\nN/A\\nN/A\\nTensorFlow\\n1422 ± 27\\n66 ± 2\\n200 ± 1\\n216 ± 15\\n9631 ± 1.3%\\n4.8e6 ± 2.9%\\nPyTorch\\n1547 ± 316\\n119 ± 1\\n212 ± 2\\n463 ± 17\\n15512 ± 4.8%\\n5.4e6 ± 3.4%\\nTable 1: Training speed for 6 models using 32bit ﬂoats. Throughput is measured in images per second for the\\nAlexNet, VGG-19, ResNet-50, and MobileNet models, in tokens per second for the GNMTv2 model, and in\\nsamples per second for the NCF model. The fastest speed for each model is shown in bold.\\n8',\n",
       " '6.4\\nAdoption\\nThe validity of design decisions and their impact on ease-of-use is hard to measure. As a proxy,\\nwe tried to quantify how well the machine learning community received PyTorch by counting how\\noften various machine learning tools (including Caffe, Chainer, CNTK, Keras, MXNet, PyTorch,\\nTensorFlow, and Theano) are mentioned on arXiv e-Prints since the initial release of PyTorch in\\nJanuary 2017. In Figure 3 we report the monthly number of mentions of the word \"PyTorch\" as a\\npercentage of all mentions among these deep learning frameworks. We counted tools mentioned\\nmultiple times in a given paper only once, and made the search case insensitive to account for various\\nspellings.\\nFigure 3: Among arXiv papers each month that mention common deep learning frameworks, percentage of\\nthem that mention PyTorch.\\n7\\nConclusion and future work\\nPyTorch has become a popular tool in the deep learning research community by combining a focus',\n",
       " 'them that mention PyTorch.\\n7\\nConclusion and future work\\nPyTorch has become a popular tool in the deep learning research community by combining a focus\\non usability with careful performance considerations. In addition to continuing to support the latest\\ntrends and advances in deep learning, in the future we plan to continue to improve the speed and\\nscalability of PyTorch. Most notably, we are working on the PyTorch JIT: a suite of tools that\\nallow PyTorch programs to be executed outside of the Python interpreter where they can be further\\noptimized. We also intend to improve support for distributed computation by providing efﬁcient\\nprimitives for data parallelism as well as a Pythonic library for model parallelism based around\\nremote procedure calls.\\n8\\nAcknowledgements\\nWe are grateful to the PyTorch community for their feedback and contributions that greatly inﬂuenced\\nthe design and implementation of PyTorch. We thank all the PyTorch core team members, contributors',\n",
       " 'We are grateful to the PyTorch community for their feedback and contributions that greatly inﬂuenced\\nthe design and implementation of PyTorch. We thank all the PyTorch core team members, contributors\\nand package maintainers including Ailing Zhang, Alex Suhan, Alfredo Mendoza, Alican Bozkurt,\\nAndrew Tulloch, Ansha Yu, Anthony Shoumikhin, Bram Wasti, Brian Vaughan, Christian Puhrsch,\\nDavid Reiss, David Riazati, Davide Libenzi, Dmytro Dzhulgakov, Dwaraj Rajagopal, Edward Yang,\\nElias Ellison, Fritz Obermeyer, George Zhang, Hao Lu, Hong Xu, Hung Duong, Igor Fedan, Ilia\\nCherniavskii, Iurii Zdebskyi, Ivan Kobzarev, James Reed, Jeff Smith, Jerry Chen, Jerry Zhang, Jiakai\\nLiu, Johannes M. Dieterich, Karl Ostmo, Lin Qiao, Martin Yuan, Michael Suo, Mike Ruberry, Mikhail\\nZolothukhin, Mingzhe Li, Neeraj Pradhan, Nick Korovaiko, Owen Anderson, Pavel Belevich, Peter\\nJohnson, Pritam Damania, Raghuraman Krishnamoorthi, Richard Zou, Roy Li, Rui Zhu, Sebastian',\n",
       " 'Zolothukhin, Mingzhe Li, Neeraj Pradhan, Nick Korovaiko, Owen Anderson, Pavel Belevich, Peter\\nJohnson, Pritam Damania, Raghuraman Krishnamoorthi, Richard Zou, Roy Li, Rui Zhu, Sebastian\\nMessmer, Shen Li, Simon Wang, Supriya Rao, Tao Xu, Thomas Viehmann, Vincent Quenneville-\\nBelair, Vishwak Srinivasan, Vitaly Fedyunin, Wanchao Liang, Wei Yang, Will Feng, Xiaomeng Yang,\\nXiaoqiang Zheng, Xintao Chen, Yangqing Jia, Yanli Zhao, Yinghai Lu and Zafar Takhirov.\\nReferences\\n[1] Yangqing \"Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,\\nSergio Guadarrama, and Trevor\" Darrell. \"caffe: Convolutional architecture for fast feature\\nembedding\". \"arXiv preprint arXiv:1408.5093\", \"2014\".\\n[2] Frank Seide and Amit Agarwal. Cntk: Microsoft’s open-source deep-learning toolkit. In\\nProceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery\\nand Data Mining, KDD ’16, pages 2135–2135, New York, NY, USA, 2016. ACM.\\n9',\n",
       " '[3] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\\nGreg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\\nAndrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,\\nManjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek\\nMurray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\\nTalwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete\\nWarden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-\\nscale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.\\n[4] Theano Development Team. Theano: A Python framework for fast computation of mathematical\\nexpressions. arXiv e-prints, abs/1605.02688, May 2016.\\n[5] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open',\n",
       " 'expressions. arXiv e-prints, abs/1605.02688, May 2016.\\n[5] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open\\nsource framework for deep learning. In Proceedings of Workshop on Machine Learning Systems\\n(LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing\\nSystems (NIPS), 2015.\\n[6] Ronan Collobert, Samy Bengio, and Johnny Mariéthoz. Torch: a modular machine learning\\nsoftware library. Technical report, Idiap, 2002.\\n[7] G. Neubig, C. Dyer, Y. Goldberg, A. Matthews, W. Ammar, A. Anastasopoulos, M. Balles-\\nteros, D. Chiang, D. Clothiaux, T. Cohn, K. Duh, M. Faruqui, C. Gan, D. Garrette, Y. Ji,\\nL. Kong, A. Kuncoro, G. Kumar, C. Malaviya, P. Michel, Y. Oda, M. Richardson, N. Saphra,\\nS. Swayamdipta, and P. Yin. DyNet: The Dynamic Neural Network Toolkit. ArXiv e-prints,\\nJanuary 2017.\\n[8] Philip S. Abrams. An APL Machine. PhD thesis, Stanford University, 1970.',\n",
       " 'S. Swayamdipta, and P. Yin. DyNet: The Dynamic Neural Network Toolkit. ArXiv e-prints,\\nJanuary 2017.\\n[8] Philip S. Abrams. An APL Machine. PhD thesis, Stanford University, 1970.\\n[9] The MathWorks, Inc., Natick, Massachusetts, United States. MATLAB and Statistics Toolbox.\\n[10] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for\\nStatistical Computing, Vienna, Austria.\\n[11] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to\\nnumerical computing. SIAM review, 59(1):65–98, 2017.\\n[12] Travis Oliphant.\\nNumPy:\\nA guide to NumPy.\\nUSA: Trelgol Publishing, 2006.\\nhttp://www.numpy.org/.\\n[13] Gaël Guennebaud, Benoît Jacob, et al. Eigen v3. http://eigen.tuxfamily.org, 2010.\\n[14] Y LeCun and L Bottou.\\nLush reference manual.\\nTechnical report, code available at\\nhttp://lush.sourceforge.net, 2002.\\n[15] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark',\n",
       " 'Lush reference manual.\\nTechnical report, code available at\\nhttp://lush.sourceforge.net, 2002.\\n[15] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark\\nSiskind. Automatic differentiation in machine learning: A survey. J. Mach. Learn. Res.,\\n18(1):5595–5637, January 2017.\\n[16] Dougal Maclaurin. Modeling, Inference and Optimization with Composable Differentiable\\nProcedures. PhD thesis, Harvard University, April 2016.\\n[17] Matthew Johnson et. al. Jax. https://github.com/google/jax, 2018.\\n[18] Mike Innes et. al. Flux.jl. https://github.com/FluxML/Flux.jl, 2018.\\n[19] Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientiﬁc tools for\\nPython, 2001–. http://www.scipy.org/.\\n[20] Wes McKinney. Data structures for statistical computing in python. In Proceedings of the 9th\\nPython in Science Conference, 51-56, 2010.\\n[21] Pierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Eblearn: Open-source energy-based',\n",
       " 'Python in Science Conference, 51-56, 2010.\\n[21] Pierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Eblearn: Open-source energy-based\\nlearning in c++. In 2009 21st IEEE International Conference on Tools with Artiﬁcial Intelligence,\\npages 693–697. IEEE, 2009.\\n10',\n",
       " '[22] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan D. Cohen, John Tran, Bryan\\nCatanzaro, and Evan Shelhamer.\\ncudnn: Efﬁcient primitives for deep learning.\\nCoRR,\\nabs/1410.0759, 2014.\\n[23] Andrew Lavin. maxdnn: An efﬁcient convolution kernel for deep learning with maxwell gpus,\\nJanuary 2015.\\n[24] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. 2016 IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), pages 4013–4021, 2016.\\n[25] Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet. Torch7: A matlab-like environment\\nfor machine learning. In NIPS 2011, 2011.\\n[26] Richard Gabriel. The rise of worse is better. http://dreamsongs.com/RiseOfWorseIsBetter.html.\\n[27] Yann\\nLeCun\\nand\\nCorinna\\nCortes.\\nMNIST\\nhandwritten\\ndigit\\ndatabase.\\nhttp://yann.lecun.com/exdb/mnist/.\\n[28] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,',\n",
       " '[27] Yann\\nLeCun\\nand\\nCorinna\\nCortes.\\nMNIST\\nhandwritten\\ndigit\\ndatabase.\\nhttp://yann.lecun.com/exdb/mnist/.\\n[28] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\\nMichelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, John\\nQuan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David\\nSilver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence,\\nAnders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement\\nlearning. CoRR, abs/1708.04782, 2017.\\n[29] DMLC. Dlpack: Open in memory tensor structure. https://github.com/dmlc/dlpack.\\n[30] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\\npytorch. In NIPS Workshop, 2017.\\n[31] Dan Piponi. Automatic differentiation, C++ templates, and photogrammetry. J. Graphics, GPU,',\n",
       " 'pytorch. In NIPS Workshop, 2017.\\n[31] Dan Piponi. Automatic differentiation, C++ templates, and photogrammetry. J. Graphics, GPU,\\n& Game Tools, 9(4):41–55, 2004.\\n[32] Holger Leuck and Hans-Hellmut Nagel. Automatic differentiation facilitates of-integration\\ninto steering-angle-based road vehicle tracking. In 1999 Conference on Computer Vision and\\nPattern Recognition (CVPR ’99), 23-25 June 1999, Ft. Collins, CO, USA, pages 2360–2365,\\n1999.\\n[33] The\\nPython\\nteam.\\nThe\\ncpython\\nglobal\\ninterpreter\\nlock.\\nhttps://wiki.python.org/moin/GlobalInterpreterLock.\\n[34] Giovanni Petrantoni and Jörg Wollenschläger.\\nNimtorch.\\nhttps://github.com/fragcolor-\\nxyz/nimtorch.\\n[35] Austin\\nHuang,\\nJunji\\nHashimoto,\\nand\\nSam\\nStites.\\nHasktorch.\\nhttps://github.com/hasktorch/hasktorch.\\n[36] G. Synnaeve, Z. Lin, J. Gehring, D. Gant, V. Mella, V. Khalidov, N. Carion, and N. Usunier.\\nForward modeling for partial observation strategy games - a starcraft defogger. In Advances in',\n",
       " '[36] G. Synnaeve, Z. Lin, J. Gehring, D. Gant, V. Mella, V. Khalidov, N. Carion, and N. Usunier.\\nForward modeling for partial observation strategy games - a starcraft defogger. In Advances in\\nNeural Information Processing Systems, pages 10761–10771, 2018.\\n[37] The PyTorch team. Torch Script. https://pytorch.org/docs/stable/jit.html.\\n[38] Justin Luitjens. Cuda streams. GPU technology conference, 2014.\\n[39] Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson. Hoard:\\nA scalable memory allocator for multithreaded applications. In Proceedings of the Ninth\\nInternational Conference on Architectural Support for Programming Languages and Operating\\nSystems, ASPLOS IX, pages 117–128, New York, NY, USA, 2000. ACM.\\n[40] J. Evans. A scalable concurrent malloc(3) implementation for freebsd. In In BSDCan — The\\nTechnical BSD Conference, May 2006.\\n[41] S. Ghemawat and P. Menage. Tcmalloc: Thread-caching malloc.\\n11',\n",
       " '[42] Benjamin Recht, Christopher Ré, Stephen J. Wright, and Feng Niu. Hogwild: A lock-free\\napproach to parallelizing stochastic gradient descent. In Advances in Neural Information\\nProcessing Systems 24: 25th Annual Conference on Neural Information Processing Systems\\n2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain., pages 693–701,\\n2011.\\n[43] Matthew Hertz and Emery D. Berger. Quantifying the performance of garbage collection vs.\\nexplicit memory management. In Proceedings of the 20th Annual ACM SIGPLAN Conference\\non Object-oriented Programming, Systems, Languages, and Applications, OOPSLA ’05, pages\\n313–326, New York, NY, USA, 2005. ACM.\\n[44] The PyTorch team. Pytorch Autograd Proﬁler. https://pytorch.org/docs/1.0.1/autograd.html#proﬁler.\\n12',\n",
       " \"Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n132 \\nA Research of Challenges and Solutions in Retrieval \\nAugmented Generation (RAG) Systems \\nJiafeng Gu * \\nSchool of CS and Math, University of Puget Sound, WA, United States \\n* Corresponding Author Email: jgu@pugetsound.edu \\nAbstract. Retrieval-Augmented Generation (RAG) systems represent a significant innovation in the \\nfield of Natural Language Processing (NLP), ingeniously integrating Large Language Models (LLMs) \\nwith dynamic external knowledge retrieval. This amalgamation not only enhances the models' \\nresponsiveness to real-world knowledge but also addresses the limitations of conventional \\ngenerative models in terms of knowledge update velocity and factual accuracy. This review \\nexamines the challenges faced by RAG systems and their solutions. It delves into the central \\narchitecture of RAG systems, encompassing retrieval components, generative components, and\",\n",
       " 'examines the challenges faced by RAG systems and their solutions. It delves into the central \\narchitecture of RAG systems, encompassing retrieval components, generative components, and \\nknowledge bases, with a particular focus on recent advancements that have expanded the \\nboundaries of performance and functionality. The study critically analyzes major challenges such as \\nretrieval efficiency and dynamic knowledge management. This paper evaluates various advanced \\nsolutions proposed in recent literature, comparing their efficacy and discussing the trade-offs \\ninvolved. Ultimately, this paper aims to provide researchers, developers, and users of RAG systems \\nwith a comprehensive perspective, fostering ongoing innovation and the expansion of applications \\nin this domain. \\nKeywords: Retrieval augmented generation, natural language processing, information retrieval, \\nknowledge base. \\n1. Introduction',\n",
       " \"in this domain. \\nKeywords: Retrieval augmented generation, natural language processing, information retrieval, \\nknowledge base. \\n1. Introduction \\nRetrieval-Augmented Generation (RAG) systems have emerged as a groundbreaking approach in \\nnatural language processing, tackling the fundamental limitations of traditional Large Language \\nModels (LLMs). By leveraging the power of LLMs with dynamic access to external knowledge, RAG \\nsystems represent a significant advancement in AI and Natural Language Processing (NLP) [1]. This \\ninnovative approach allows for the generation of more accurate, relevant, and up-to-date responses \\nacross a wide range of applications. The significance of RAG research lies in its potential to transform \\nAI applications fundamentally. These systems enhance text accuracy and relevance, improve AI's \\ncapability to handle complex, knowledge-intensive tasks, and enable continuous knowledge updating\",\n",
       " \"AI applications fundamentally. These systems enhance text accuracy and relevance, improve AI's \\ncapability to handle complex, knowledge-intensive tasks, and enable continuous knowledge updating \\nwithout the need for constant model retraining. This dynamic integration of retrieval and generation \\nmechanisms addresses the longstanding challenge of knowledge staleness in pre-trained language \\nmodels, opening new avenues for more adaptive and context-aware AI systems. \\nCurrent challenges in RAG systems span various aspects of their architecture and functionality. \\nWhile recent advancements have made significant strides, they often come with their own limitations. \\nFor instance, the Retrieval-Enhanced Transformer (RETRO) has shown impressive scalability, \\ncapable of retrieving from databases with trillions of tokens and demonstrating competitive \\nperformance with models 25 times its size [2]. However, RETRO faces challenges in computational\",\n",
       " 'capable of retrieving from databases with trillions of tokens and demonstrating competitive \\nperformance with models 25 times its size [2]. However, RETRO faces challenges in computational \\nefficiency and the possibility of mistakes spreading in its iterative retrieval process. Another cutting-\\nedge approach, the atlas model, employs few-shot learning with retrieval augmented language models, \\ngetting great performance on various knowledge-intensive tasks [3]. Despite its impressive \\nperformance, Atlas still faces challenges in efficiently updating its knowledge base and may struggle \\nwith queries that require real-time information retrieval and integration. \\nThis paper aims to provide a comprehensive review of these challenges and the proposed cutting-\\nedge solutions. It analyzes the architecture and components of RAG systems in depth, identifying key \\nchallenges and their impact on system performance. By critically reviewing and comparing existing',\n",
       " 'challenges and their impact on system performance. By critically reviewing and comparing existing \\nsolutions, this paper highlights both their achievements and limitations. Furthermore, this work',\n",
       " 'Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n133 \\nexplores promising future research directions that could address current limitations but may also \\nintroduce new challenges in data processing and model design. \\n2. Architecture of RAG System \\nRAG systems consist of three primary components: the retrieval component, the generation \\ncomponent, and the knowledge base. Each plays a crucial role in producing accurate, relevant, and \\nup-to-date responses (Fig.1). The system retrieves relevant content based on user queries using this \\nembedded knowledge base. The retrieved chunks are then combined with the original query to form \\na prompt, which is processed by a LLM to generate the final response.  \\n \\nFig 1. The workflow of a RAG system (Photo/Picture credit: Original).  \\n2.1. Retrieval Component \\nThe retrieval component serves as the critical bridge between user input and the vast repository of',\n",
       " 'Fig 1. The workflow of a RAG system (Photo/Picture credit: Original).  \\n2.1. Retrieval Component \\nThe retrieval component serves as the critical bridge between user input and the vast repository of \\ninformation stored in the knowledge base. Its primary function is to identify and extract relevant \\ninformation based on the input query. This process involves several complex steps, including query \\nunderstanding, eﬀicient searching, and relevance ranking. \\nRecent advancements in dense retrieval methods, such as those proposed by Karpukhin et al., have \\nsignificantly improved the effectiveness of this component [4]. These methods leverage dense vector \\nrepresentations of both queries and documents, enabling more nuanced semantic matching compared \\nto traditional lexical retrieval approaches. The main advantage of this component lies in its ability to \\naccess and utilize vast amounts of external knowledge, potentially overcoming the limitations of static',\n",
       " 'access and utilize vast amounts of external knowledge, potentially overcoming the limitations of static \\nknowledge inherent in traditional language models. Hybrid retrieval approaches have also emerged \\nas a promising direction. For instance, GAO proposed a method combining sparse and dense retrieval \\ntechniques [5]. This approach aims to leverage the strengths of both lexical and semantic matching, \\npotentially offering more robust performance across diverse query types. \\nHowever, challenges persist in achieving optimal performance, particularly in terms of query \\ninterpretation and balancing semantic relevance with diversity in the retrieved information. The \\nretrieval component must not only consider the semantic similarity between the query and potential \\nmatches but also ensure a diverse set of relevant information to provide comprehensive context for \\nthe generation task.  \\n2.2. Generation Component',\n",
       " 'matches but also ensure a diverse set of relevant information to provide comprehensive context for \\nthe generation task.  \\n2.2. Generation Component \\nThe generation component, typically based on a large language model, is responsible for producing \\nthe final output in RAG systems. This crucial element integrates the retrieved information with the \\noriginal input to generate coherent, contextually appropriate, and informative responses. The',\n",
       " \"Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n134 \\ngeneration process involves several key steps: context integration, text generation, and output \\nrefinement. \\nRecent work has shown promising results in improving the generation component's ability to \\neffectively utilize retrieved information. A significant breakthrough in this area is the Fusion-in-\\nDecoder model proposed by lzacard [6]. This model processes all retrieved passages jointly in the \\ndecoder, allowing for more effective integration of information from multiple sources. This approach \\ndemonstrates the potential for RAG systems to adapt quickly to new tasks and domains with minimal \\nfine-tuning. Another notable advancement is the development of iterative retrieval-generation models, \\nas demonstrated by Shuster [7]. These models involve multiple rounds of retrieval and generation, \\nenabling the system to handle complex queries that may require multi-step reasoning or information\",\n",
       " 'as demonstrated by Shuster [7]. These models involve multiple rounds of retrieval and generation, \\nenabling the system to handle complex queries that may require multi-step reasoning or information \\ngathering. Researchers have also explored integrating external knowledge graphs and structured data \\nwithin the generation process. Xu Yichong proposed an approach that leverages both retrieved textual \\ninformation and structured knowledge, potentially improving the factual accuracy and logical \\ncoherence of generated outputs [8]. \\nThe strength of this component lies in its ability to generate fluent, coherent, and contextually \\nrelevant responses. By leveraging the power of large language models and augmenting them with \\nretrieved information, RAG systems can produce outputs that are both linguistically sophisticated and \\nfactually grounded. \\nHowever, significant challenges remain. Ensuring factual consistency between the generated',\n",
       " 'factually grounded. \\nHowever, significant challenges remain. Ensuring factual consistency between the generated \\ncontent and the retrieved information is a critical issue. The model must accurately incorporate the \\nretrieved facts while maintaining the overall coherence and fluency of the generated text. Additionally, \\nmaintaining consistency and coherence across longer outputs poses another significant challenge, \\nrequiring sophisticated mechanisms for long-range dependency modeling and content planning. \\n2.3. Knowledge Base \\nThe knowledge base serves as the external memory of the RAG system. Recent research has \\nexplored various approaches to knowledge base design, including the integration of diverse data \\nformats. \\nOne significant innovation is the creation of dynamic knowledge bases that can be efficiently \\nupdated. The Generative Pseudo-Labeling (GPL) method proposed by Wang Kexin, allows for',\n",
       " 'formats. \\nOne significant innovation is the creation of dynamic knowledge bases that can be efficiently \\nupdated. The Generative Pseudo-Labeling (GPL) method proposed by Wang Kexin, allows for \\ncontinuous learning and updating of the knowledge base [9]. This approach enables RAG systems to \\nincorporate new information without the need for full retraining, which is particularly crucial in \\ndomains with rapidly evolving knowledge. \\nResearchers have also explored multi-modal knowledge bases. For instance, Gao introduced a \\nmulti-modal retrieval-augmented framework that can process and integrate information from both \\ntextual and visual sources [10]. This advancement allows RAG systems to leverage a broader range \\nof information types, potentially enhancing their ability to handle complex, multi-modal queries. \\n3. Challenges in RAG Systems \\nDynamic knowledge management presents complex challenges in keeping the knowledge base up-',\n",
       " '3. Challenges in RAG Systems \\nDynamic knowledge management presents complex challenges in keeping the knowledge base up-\\nto-date while maintaining system performance. This is particularly critical in domains with rapidly \\nevolving information, such as news, scientific research, or social media trends. \\n3.1. Scalability \\nThe fundamental challenge of scalability lies in the curse of dimensionality. As the volume of data \\nincreases, the search space grows exponentially, making it computationally intractable to perform \\nexact nearest neighbor search in high-dimensional spaces. This is particularly problematic for dense \\nvector representations used in modern retrieval systems. \\nIn high-dimensional spaces, the concept of \"nearest\" neighbor becomes less meaningful due to the \\nphenomenon known as \"distance concentration\". As dimensionality increases, the ratio of the',\n",
       " 'Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n135 \\ndistances of the nearest and farthest neighbors to a given point approaches 1, making it difficult to \\ndistinguish between close and far points [11]. This phenomenon significantly impacts the \\neffectiveness of traditional similarity search algorithms. \\nWhile approximate methods like Locality-Sensitive Hashing (LSH) or Hierarchical Navigable \\nSmall World (HNSW) graphs offer potential solutions, they introduce a complex trade-off between \\naccuracy and speed. For instance, LSH may miss some nearest neighbors, while HNSW requires \\ncareful tuning of its graph structure to balance between search speed and index build time. Optimizing \\nthese trade-offs remains a significant challenge, especially as the scale of data continues to grow. \\nRecent research has explored hybrid approaches to address these scalability issues. For example,',\n",
       " 'these trade-offs remains a significant challenge, especially as the scale of data continues to grow. \\nRecent research has explored hybrid approaches to address these scalability issues. For example, \\nthe ScaNN method combines quantization for fast in-memory search with anisotropic vector \\nquantization for reduced search space [12]. However, such methods still struggle with dynamic \\nupdates to the index, which is crucial for real-time RAG systems. The challenge of developing \\nscalable methods that can adapt to varying conditions while maintaining retrieval quality remains an \\nopen problem in the field. \\n3.2. Query Reformulation \\nQuery reformulation in RAG systems faces significant challenges stemming from the semantic \\ngap between user queries and knowledge base content. This process involves complex natural \\nlanguage understanding and generation tasks, requiring sophisticate models to capture nuanced \\nsemantic relationships.',\n",
       " 'language understanding and generation tasks, requiring sophisticate models to capture nuanced \\nsemantic relationships. \\nA key challenge is handling biased or loaded queries while maintaining objectivity. For instance, \\na query like \"Why are vaccines harmful?\" contains a biased premise that the system must recognize \\nand neutralize to ensure balanced information retrieval. Developing methods to detect and mitigate \\nsuch biases without completely disregarding user intent remains an open problem. Another significant \\nchallenge lies in adapting queries to specific domains or temporal contexts. User queries often contain \\ndomain-specific jargon or references to current events that require specialized knowledge to interpret \\ncorrectly. Balancing the need for domain expertise with general language understanding is a complex \\ntask that current systems struggle to achieve consistently. \\nThe temporal aspect of queries presents its own set of challenges. Queries implicitly referencing',\n",
       " \"task that current systems struggle to achieve consistently. \\nThe temporal aspect of queries presents its own set of challenges. Queries implicitly referencing \\ncurrent events or time-sensitive information require the reformulation process to incorporate temporal \\ncontext. Striking the right balance between current relevance and historical context is particularly \\ndifficult and requires sophisticated temporal reasoning capabilities [13]. \\n3.3. Latency \\nThe core challenge of latency in RAG systems stems from the fundamental trade-off between \\nresponse time and result quality. In interactive applications, the system must carefully balance the \\ndepth of retrieval against the user's patience threshold. This balancing act is particularly critical as the \\nretrieval depth significantly impacts both latency and result quality; deeper retrieval can provide more \\ncomprehensive results but at the cost of increased response time.\",\n",
       " 'retrieval depth significantly impacts both latency and result quality; deeper retrieval can provide more \\ncomprehensive results but at the cost of increased response time. \\nOne of the primary factors contributing to latency is the complexity of multi-step retrieval \\nprocesses, often necessary for handling sophisticated queries or performing multi-hop reasoning. As \\nthe number of retrieval steps increases, managing cumulative latency becomes increasingly \\nchallenging. Each additional step not only adds to the overall response time but also introduces \\npotential points of failure or inconsistency in the retrieval process. \\nThe latency challenge is further exacerbated in scenarios requiring real-time knowledge base \\nupdates. Ensuring that newly added information is immediately available for retrieval, without \\ncompromising query response times, presents a significant technical hurdle. This is particularly',\n",
       " 'updates. Ensuring that newly added information is immediately available for retrieval, without \\ncompromising query response times, presents a significant technical hurdle. This is particularly \\nproblematic in domains with rapidly changing information, where the relevance of retrieval results \\ncan degrade quickly if the knowledge base is not continuously updated [14].',\n",
       " 'Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n136 \\n4. Solutions and Advancements in RAG Systems \\nRecent years have witnessed significant advancements in RAG systems, addressing key challenges \\nin retrieval efficiency, scalability, and knowledge integration. This section explores two innovative \\napproaches that represent the cutting edge of RAG technology. Self-RAG, introduced by Asai et al., \\nand represents a significant shift in RAG system design. It incorporates retrieval, generation, and \\nevaluation into a single framework, allowing the model to iteratively improve its own performance. \\nThis self-improving capability addresses challenges in query reformulation and result quality \\nassessment, potentially leading to more accurate and relevant responses over time [15]. \\nGraphRAG, developed by Microsoft, takes a different approach by leveraging graph structures for',\n",
       " \"assessment, potentially leading to more accurate and relevant responses over time [15]. \\nGraphRAG, developed by Microsoft, takes a different approach by leveraging graph structures for \\nknowledge representation. This framework uses large language models to extract structured data from \\nunstructured text, building labeled knowledge graphs to support various applications. GraphRAG's \\nuse of graph machine learning algorithms for semantic aggregation and hierarchical analysis enables \\nit to answer high-level abstract or summary questions, showcasing the potential of structured \\nknowledge in RAG systems [16]. \\nTable 1. Comparison of advanced RAG Solutions. \\nModel \\nKey feature \\nSelf-Improvement Reasoning Capability \\nSelf-RAG \\nIterative Refinement \\nHigh \\nAdaptive \\nGraphRAG Graph-based Representation \\nModerate \\nHigh \\n \\nAs illustrated in Table 1, these two approaches offer unique strengths and address different aspects\",\n",
       " \"Iterative Refinement \\nHigh \\nAdaptive \\nGraphRAG Graph-based Representation \\nModerate \\nHigh \\n \\nAs illustrated in Table 1, these two approaches offer unique strengths and address different aspects \\nof RAG system design. Self-RAG focuses on iterative self-improvement and adaptive reasoning, \\nwhile GraphRAG introduces structured knowledge representation for enhanced reasoning capabilities. \\nThese advancements collectively represent significant progress in addressing the core challenges \\nof RAG systems. However, each approach also introduces new complexities and trade-offs. For \\ninstance, while GraphRAG's structured knowledge representation offers powerful reasoning \\ncapabilities, it may face challenges in domains where information is inherently unstructured or rapidly \\nchanging. Similarly, the iterative processes in Self-RAG, while powerful, may introduce additional \\ncomputational overhead. \\n5. Future Directions for RAG Systems\",\n",
       " 'changing. Similarly, the iterative processes in Self-RAG, while powerful, may introduce additional \\ncomputational overhead. \\n5. Future Directions for RAG Systems \\nAs the explore promising future research directions for RAG systems, several key areas emerge \\nthat could significantly advance the field while remaining grounded in current technological \\ntrajectories. \\nOne promising direction is the integration of RAG systems with LLMs that have multimodal \\ncapabilities. This combination could enable RAG systems to not only retrieve and process textual \\ninformation but also understand and generate content across various modalities such as images, audio, \\nand video. For instance, a multimodal RAG system could retrieve relevant images or video clips \\nalongside textual information, providing more comprehensive and contextually rich responses. This \\nintegration could be particularly valuable in fields like medical diagnosis, where the ability to retrieve',\n",
       " 'integration could be particularly valuable in fields like medical diagnosis, where the ability to retrieve \\nand analyze both textual reports and medical imaging data could enhance diagnostic accuracy. \\nAnother exciting avenue is the exploration of dynamic knowledge graphs within RAG systems. \\nBy continuously updating and refining a structured knowledge representation based on new \\ninformation retrieved, RAG systems could develop more nuanced and up-to-date understanding of \\ncomplex topics. This approach could involve real-time fact-checking and information validation \\nmechanisms, potentially mitigating the spread of misinformation and ensuring the reliability of \\ngenerated content. The dynamic nature of these knowledge graphs could also allow RAG systems to \\nadapt more quickly to emerging topics and changing information landscapes. \\nA third promising direction is the integration of RAG systems with robotics and embodied AI.',\n",
       " 'adapt more quickly to emerging topics and changing information landscapes. \\nA third promising direction is the integration of RAG systems with robotics and embodied AI. \\nThis combination could lead to robots that not only interact with their environment but also leverage',\n",
       " \"Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n137 \\nvast knowledge bases to make informed decisions and provide rich, contextual information. For \\nexample, a RAG-enhanced robot in a manufacturing setting could access and apply complex technical \\nknowledge in real-time, improving problem-solving capabilities and adaptability. In healthcare, robot \\nassistants equipped with RAG systems could provide personalized care by combining real-time \\npatient data with comprehensive medical knowledge. Furthermore, in educational settings, RAG-\\npowered robotic tutors could offer personalized learning experiences by dynamically retrieving and \\npresenting information tailored to each student's needs and learning style. This fusion of RAG \\ntechnology with robotics could significantly enhance the physical world interaction capabilities of AI \\nsystems, opening up new possibilities in fields ranging from space exploration to disaster response,\",\n",
       " \"systems, opening up new possibilities in fields ranging from space exploration to disaster response, \\nwhere quick access to vast amounts of relevant information could be crucial for mission success and \\nsafety. \\n6. Conclusion \\nThis paper conducts a comprehensive analysis of recent advancements in RAG systems, with a \\nparticular emphasis on key challenges and solutions such as scalability, query reformulation, latency, \\nand RAG system architectures. This work examines innovative approaches such as RETRO's \\ncapability in handling massive datasets, Self-RAG's adaptive query reformulation, and GraphRAG's \\nstructured knowledge representation. The analysis reveals that while significant progress has been \\nmade across each domain, trade-offs remain prevalent. Enhancements in scalability often come at the \\ncost of increased computational complexity. For instance, while RETRO demonstrates its superiority\",\n",
       " 'cost of increased computational complexity. For instance, while RETRO demonstrates its superiority \\non large-scale datasets, the computational overhead and energy consumption cannot be overlooked. \\nMeanwhile, advanced query reformulation techniques like Self-RAG show promise in tackling \\ncomplex queries but may introduce additional latency, posing a challenge for real-time application \\nscenarios. Additionally, the structured knowledge representation methods employed by GraphRAG \\nenhance reasoning capabilities but still face hurdles when confronted with unstructured or rapidly \\nevolving information. \\nTo address these issues, future research can explore multiple directions. Firstly, integrating \\ncutting-edge hardware architectures with optimization algorithms to reduce computational \\ncomplexity can improve the overall performance of systems. Secondly, developing more efficient \\nquery reformulation algorithms to minimize latency ensures that systems maintain real-time',\n",
       " \"complexity can improve the overall performance of systems. Secondly, developing more efficient \\nquery reformulation algorithms to minimize latency ensures that systems maintain real-time \\nresponsiveness even when dealing with intricate problems. Moreover, building dynamic models for \\nboth structured and unstructured information will be a crucial area of research, aiming to increase the \\nflexibility and adaptability of knowledge representation. \\nReference \\n[1] Lewis, Patrick, Scott Reed, Jack Urbanek, and Nando de Freitas. Retrieval-augmented generation for \\nknowledge-intensive NLP tasks. Advances in Neural Information Processing Systems 33 2020: 9459-\\n9474. \\n[2] Borgeaud, Sebastian, Arthur Mensch, Guillaume Lample, and Marc'Aurelio Ranzato. Improving language \\nmodels by retrieving from trillions of tokens. International conference on machine learning. PMLR, 2022. \\n[3] Izacard, Gautier, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models.\",\n",
       " '[3] Izacard, Gautier, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. \\nJournal of Machine Learning Research 24.251 2023: 1-43. \\n[4] Karpukhin, Vladimir, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Alexander Kolesnikov, and \\nSebastian Ruder. Dense passage retrieval for open-domain question answering. arXiv preprint \\narXiv:2004.04906 2020. \\n[5] GAO, Luyu, Wei-Sheng Chin, Yu-Chia Chen, and Cho-Jui Hsieh. Complement lexical retrieval model \\nwith semantic residual embeddings. Advances in Information Retrieval: 43rd European Conference on IR \\nResearch, ECIR 2021, Virtual Event, March 28-April 1, 2021, Part I 43.  \\n[6] Izacard, Gautier, and Edouard Grave. Leveraging passage retrieval with generative models for open \\ndomain question answering. arXiv preprint arXiv:2007.01282 2020.',\n",
       " 'Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n138 \\n[7] Shuster, Kurt, Alexander M. Rush, and Jason Weston. Retrieval augmentation reduces hallucination in \\nconversation. arXiv preprint arXiv:2104.07567 2021. \\n[8] Xu, Yichong, Xiaojun Wan, Xiaoyan Zhu, and Xipeng Qiu. Fusing context into knowledge graph for \\ncommonsense question answering. arXiv preprint arXiv:2012.04808 2020. \\n[9] Wang, Kexin, Zhenzhong Lan, and Jianfeng Gao. GPL: Generative pseudo labeling for unsupervised \\ndomain adaptation of dense retrieval. arXiv preprint arXiv:2112.07577 2021. \\n[10] Shibata, Tetsutaro. Asymptotics of solution curves of Kirchhoff type elliptic equations with logarithmic \\nKirchhoff function. Qualitative Theory of Dynamical Systems 22.2 2023: 64. \\n[11] Peng, Dehua, Zhipeng Gui, and Huayi Wu. Interpreting the curse of dimensionality from distance \\nconcentration and manifold effect. arXiv preprint arXiv:2401.00422 2023.',\n",
       " '[11] Peng, Dehua, Zhipeng Gui, and Huayi Wu. Interpreting the curse of dimensionality from distance \\nconcentration and manifold effect. arXiv preprint arXiv:2401.00422 2023. \\n[12] Hassantabar, Shayan, Zeyu Wang, and Niraj K. Jha. SCANN: Synthesis of compact and accurate neural \\nnetworks. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 41.9 2021: \\n3012-3025. \\n[13] Dhingra, Bhuwan, and Graham Neubig. Time-aware language models as temporal knowledge bases. \\nTransactions of the Association for Computational Linguistics 10 2022: 257-273. \\n[14] GAO, Yunfan, Zhenzhong LAN, and Jianfeng GAO. Retrieval-augmented generation for large language \\nmodels: A survey. ArXiv preprint arXiv: 2312.10997 2023. \\n[15] Asai, Akari, and Masaaki Komachi. Self-rag: Learning to retrieve, generate, and critique through self-\\nreflection. arXiv preprint arXiv:2310.11511 2023. \\n[16] Edge, Darren, and Peter J. Stuckey. From local to global: A graph rag approach to query-focused',\n",
       " 'reflection. arXiv preprint arXiv:2310.11511 2023. \\n[16] Edge, Darren, and Peter J. Stuckey. From local to global: A graph rag approach to query-focused \\nsummarization. arXiv preprint arXiv:2404.16130 2024.',\n",
       " 'Research on Tensor Flow with a system for large-scale machine \\nlearning \\nHafiz Nur \\nGenovasi University College, Malaysia \\nGoogle Brain \\nAbstract \\nTensorFlow is a machine learning system that operates at \\nlarge \\nscale \\nand \\nin \\nheterogeneous \\nenvironments. \\nTensorFlow \\nuses \\ndataflow \\ngraphs \\nto \\nrepresent \\ncomputation, shared state, and the operations that mutate \\nthat state. It maps the nodes of a dataflow graph across \\nmany machines in a cluster, and within a machine across \\nmultiple computational devices, including multicore CPUs, \\ngeneralpurpose GPUs, and custom designed ASICs known as \\nTensor Processing Units (TPUs). This architecture gives \\nflexibility to the application developer: whereas in previous \\n“parameter server” designs the management of shared \\nstate is built into the system, TensorFlow enables \\ndevelopers to experiment with novel optimizations and \\ntraining algorithms. TensorFlow supports a variety of \\napplications, with particularly strong support for training',\n",
       " 'developers to experiment with novel optimizations and \\ntraining algorithms. TensorFlow supports a variety of \\napplications, with particularly strong support for training \\nand inference on deep neural networks. Several Google \\nservices use TensorFlow in production, we have released it \\nas an open-source project, and it has become widely used \\nfor machine learning research. In this paper, we describe \\nthe TensorFlow dataflow model in contrast to existing \\nsystems, and demonstrate the compelling performance \\nthat \\nTensorFlow \\nachieves \\nfor \\nseveral \\nreal-world \\napplications. \\n1 Introduction \\nIn recent years, machine learning has driven advances in \\nmany different fields [3, 5, 23, 24, 30, 27, 40, 45, 48, 50, 55, \\n68, 69, 73, 76]. We attribute this success to the invention of \\nmore sophisticated machine learning models [42, 51], the \\n                                                                \\n \\navailability of large datasets for tackling problems in these',\n",
       " 'more sophisticated machine learning models [42, 51], the \\n                                                                \\n \\navailability of large datasets for tackling problems in these \\nfields [10, 65], and the development of software platforms \\nthat enable the easy use \\nof large amounts of computational resources for training \\nsuch models on these large datasets [14, 21]. \\nWe introduce the TensorFlow system\\n1\\n for \\nexperimenting with new models, training them on \\nlarge datasets, and moving them into production. We \\nhave based TensorFlow on years of experience with \\nour first-generation system, DistBelief [21], both \\nsimplifying and generalizing it to enable researchers to \\nexplore a wider variety of ideas with relative ease. \\nTensorFlow supports both large-scale training and \\ninference: it efficiently uses hundreds of powerful \\n(GPU-enabled) servers for fast training, and it runs \\ntrained models for inference in production on various',\n",
       " 'inference: it efficiently uses hundreds of powerful \\n(GPU-enabled) servers for fast training, and it runs \\ntrained models for inference in production on various \\nplatforms, ranging from large distributed clusters in a \\ndatacenter, down to performing inference locally on \\nmobile devices. At the same time, it is flexible and \\ngeneral enough to support experimentation and \\nresearch into new machine learning models and \\nsystem-level optimizations. \\nTensorFlow uses a unified dataflow graph to \\nrepresent both the computation in an algorithm and \\nthe state on which the algorithm operates. We draw \\ninspiration from the high-level programming models \\nof dataflow systems [2, 22, 75], and the low-level \\nefficiency of parameter servers [14, 21, 46]. Unlike \\ntraditional dataflow systems, in which graph vertices \\nrepresent functional computation on immutable data, \\nTensorFlow allows vertices to represent computations \\nthat own or update mutable state. Edges carry tensors',\n",
       " 'represent functional computation on immutable data, \\nTensorFlow allows vertices to represent computations \\nthat own or update mutable state. Edges carry tensors \\n(multi-dimensional arrays) between nodes, and',\n",
       " '2 \\nTensorFlow transparently inserts the appropriate \\ncommunication \\nbetween \\ndistributed \\nsubcomputations. By unifying the computation and \\nstate management in a single programming model, \\nTensorFlow allows programmers to experiment with \\ndifferent parallelization schemes that, for example, \\noffload computation onto the servers that hold the \\nshared state to reduce the amount of network traffic. \\nWe have also built various coordination protocols, and \\nachieved encouraging results with synchronous \\nreplication, echoing recent results [11, 19] that \\ncontradict \\nthe \\ncommonly \\nheld \\nbelief \\nthat \\nasynchronous replication is required for scalable \\nlearning [14, 21, 46]. \\nOver the past year, more than 60 teams at Google have \\nused TensorFlow, and we have released the system as an \\nopen-source project. Thanks to our large community of \\nusers we have gained experience with many different \\nmachine learning applications. In this paper, we focus on',\n",
       " 'open-source project. Thanks to our large community of \\nusers we have gained experience with many different \\nmachine learning applications. In this paper, we focus on \\nneural network training as a challenging systems problem, \\nand select two representative applications from this space: \\nimage classification and language modeling. These \\napplications \\nstress \\ncomputational \\nthroughput \\nand \\naggregate model size respectively, and we use them both to \\ndemonstrate the extensibility of TensorFlow, and to \\nevaluate the efficiency and scalability of our present \\nimplementation. \\n2 Background & Motivation \\nTo make the case for developing TensorFlow, we start by \\noutlining the requirements for a large-scale machine \\nlearning system (§2.1), then consider how related work \\nmeets or does not meet those requirements (§2.2). \\n2.1 Requirements \\nDistributed execution A cluster of powerful computers can \\nsolve many machine learning problems more efficiently, \\nusing more data and larger models.',\n",
       " '2.1 Requirements \\nDistributed execution A cluster of powerful computers can \\nsolve many machine learning problems more efficiently, \\nusing more data and larger models. \\nMachine learning algorithms generally perform better \\nwith more training data. For example, recent breakthroughs \\nin image classification models have benefited from the \\npublic ImageNet dataset, which contains 136 gigabytes of \\ndigital images [65]; and language modeling has benefited \\nfrom efforts like the One Billion Word Benchmark [10]. The \\nscale of these datasets motivates a dataparallel approach \\nto training: a distributed file system holds the data, and a \\nset of workers processes different subsets of data in \\nparallel. Data-parallelism eliminates the I/O bottleneck for \\ninput data, and any preprocessing operations can be \\napplied to input records independently. \\nEffective learned models for image recognition, language \\nmodeling, document clustering, and many other problems',\n",
       " 'applied to input records independently. \\nEffective learned models for image recognition, language \\nmodeling, document clustering, and many other problems \\nhave a large number of parameters. For example, the \\ncurrent state-of-the-art image classification model, ResNet, \\nuses 2.3 million floating-point parameters to classify images \\ninto one of 1000 categories [26]. The One Billion Word \\nBenchmark has a vocabulary of 800,000 words, and it has \\nbeen used to train language models with 1.04 billion \\nparameters [39]. A distributed system can shard the model \\nacross many processes, to increase the available network \\nbandwidth when many workers are simultaneously reading \\nand updating the model. \\nA distributed system for model training must use \\nthe network efficiently. Many scalable algorithms \\ntrain a model using mini-batch gradient descent [21, \\n47], where a worker reads the current version of the \\nmodel and a small batch of input examples, calculates',\n",
       " 'train a model using mini-batch gradient descent [21, \\n47], where a worker reads the current version of the \\nmodel and a small batch of input examples, calculates \\nan update to the model that reduces a loss function \\non those examples, and applies the update to the \\nmodel. Mini-batch methods are most effective when \\neach worker uses the most current model as a starting \\npoint, which requires a large amount of data to be \\ntransferred to the worker with low latency. \\nAccelerator support Machine learning algorithms \\noften perform expensive computations, such as matrix \\nmultiplication and multi-dimensional convolution, \\nwhich are highly parallelizable, but have many data \\ndependencies \\nthat \\nrequire \\na \\ntightly \\ncoupled \\nimplementation. The recent availability of general-\\npurpose GPUs has provided a large number of cores \\nthat can operate on fast local memory. For example, a \\nsingle NVIDIA Titan X GPU card has 6 TFLOPS peak \\nperformance [60]. In 2012, state-ofthe-art results for',\n",
       " 'that can operate on fast local memory. For example, a \\nsingle NVIDIA Titan X GPU card has 6 TFLOPS peak \\nperformance [60]. In 2012, state-ofthe-art results for \\ndifferent image classification tasks were achieved \\nusing 16,000 CPU cores for three days [45], and using \\ntwo GPUs for six days [42]. Since then, GPU vendors \\nhave innovated in their support for machine learning: \\nNVIDIA’s cuDNN library [13] for GPU-based neural \\nnetwork training accelerates several popular image \\nmodels by 2–4× when using version R4 in place of R2 \\n[15]. \\nIn addition to general-purpose devices, many \\nspecialpurpose accelerators for deep learning have',\n",
       " '3 \\nachieved significant performance improvements and \\npower savings. At Google, our colleagues have built \\nthe Tensor Processing Unit (TPU) specifically for \\nmachine learning, and it achieves an order of \\nmagnitude improvement in performance-per-watt \\ncompared to alternative state-of-the-art technology \\n[38]. The Movidius Deep Learning Accelerator uses a \\nlow-power Myriad 2 processor with custom vector \\nprocessing units that accelerate many machine \\nlearning and computer vision algorithms [53]. \\nOvtcharov \\net \\nal. \\nhave \\nachieved \\nsignificant \\nperformance improvements and power savings for \\nsome convolutional models using field programmable \\ngate arrays (FPGAs) [58]. Since it is difficult to predict \\nthe next popular architecture for executing machine \\nlearning algorithms, we require that TensorFlow uses \\na portable programming model that can target a \\ngeneric device abstraction, and allows its operations \\nto be specialized for new architectures as they \\nemerge.',\n",
       " 'a portable programming model that can target a \\ngeneric device abstraction, and allows its operations \\nto be specialized for new architectures as they \\nemerge. \\nTraining & inference support In addition to training, \\nscalable and high-performance inference is a \\nrequirement for using models in production [18]. \\nDepending on the nature of the application, the \\ninference may be required to produce results with \\nvery low latency in an interactive service, or execute \\non a disconnected mobile device. If the model is large, \\nit might require multiple servers to participate in each \\ninference computation, and thus require distributed \\ncomputation support. Developers benefit when they \\ncan use the same code to define a model for both \\ntraining and inference. Training and inference demand \\nsimilar performance, so we prefer a common \\nwelloptimized system for both computations. Since \\ninference can be computationally intensive (e.g., an \\nimage classification model might perform 5 billion',\n",
       " 'welloptimized system for both computations. Since \\ninference can be computationally intensive (e.g., an \\nimage classification model might perform 5 billion \\nFLOPS per image [70]), it must be possible to \\naccelerate it with GPUs. \\nExtensibility Single-machine machine learning frameworks \\n[36, 2, 17] have extensible programming models that enable \\ntheir users to advance the state of the art with new \\napproaches, such as adversarial learning [25] and deep \\nreinforcement learning [51]. We seek a system that \\nprovides the same ability to experiment, and also allows \\nusers to scale up the same code to run in production. The \\nsystem must support expressive control-flow and stateful \\nconstructs, while also satisfying our other requirements. \\n2.2 Related work \\nSingle-machine frameworks Many machine learning \\nresearchers carry out their work on a single—often \\nGPUequipped—computer [41, 42], and many flexible \\nsinglemachine frameworks have emerged to support this',\n",
       " 'researchers carry out their work on a single—often \\nGPUequipped—computer [41, 42], and many flexible \\nsinglemachine frameworks have emerged to support this \\nscenario. Caffe [36] is a high-performance framework for \\ntraining declaratively specified convolutional neural \\nnetworks that runs on multicore CPUs and GPUs. Theano [2] \\nallows programmers to express a model as a dataflow \\ngraph, and generates efficient compiled code for training \\nthat model. Torch [17] has an imperative programming \\nmodel for scientific computation (including machine \\nlearning) that supports fine-grained control over the order \\nof execution and memory utilization. \\nWhile these frameworks do not satisfy our requirement \\nfor distributed execution, TensorFlow’s programming \\nmodel is close to Theano’s dataflow representation \\n(§3). \\nBatch dataflow systems Starting with MapReduce [22], \\nbatch dataflow systems have been applied to a large \\nnumber of machine learning algorithms [71], and more',\n",
       " '(§3). \\nBatch dataflow systems Starting with MapReduce [22], \\nbatch dataflow systems have been applied to a large \\nnumber of machine learning algorithms [71], and more \\nrecent systems have focused on increasing expressivity and \\nperformance. DryadLINQ [74] adds a high-level query \\nlanguage that supports more sophisticated algorithms than \\nMapReduce. Spark [75] extends DryadLINQ with the ability \\nto cache previously computed datasets in memory, and is \\ntherefore better suited to iterative machine learning \\nalgorithms (such as k-means clustering and logistic \\nregression) when the input data fit in memory. Dandelion \\nextends DryadLINQ to support generating code for GPUs \\n[63] and FPGAs [16]. \\nThe principal limitation of a batch dataflow system \\nis that it requires the input data to be immutable, and \\nall of the subcomputations to be deterministic, so that \\nthe system can re-execute subcomputations when \\nmachines in the cluster fail. This feature—which is',\n",
       " 'all of the subcomputations to be deterministic, so that \\nthe system can re-execute subcomputations when \\nmachines in the cluster fail. This feature—which is \\nbeneficial for many conventional workloads—makes \\nupdating a machine learning model a heavy operation. \\nFor example, the SparkNet system for training deep \\nneural networks on Spark takes 20 seconds to \\nbroadcast weights and collect updates from five \\nworkers [52]. As a result, these systems must process \\nlarger batches in each model update step, which slows',\n",
       " '4 \\nconvergence [9]. We show in Subsection 6.3 that \\nTensorFlow can train larger models on larger clusters \\nwith step times as short as 2 seconds. \\nWhile not a batch dataflow system, Naiad [54] \\naugments a dataflow model with streaming execution, \\nstateful vertices, and structured timestamps (“timely \\ndataflow”) that enable it to handle incremental \\nupdates and iterative algorithms in the same \\ncomputation. Naiad represents iteration using cyclic \\ndataflow graphs, which together with mutable state \\nmake it possible to implement algorithms that require \\nmillisecond-scale latencies for coordination. Naiad is \\ndesigned for computing on sparse, discrete data, and \\ndoes not support GPU (or any other form of) \\nacceleration, but we borrow aspects of timely \\ndataflow iteration in Subsection 3.4. \\nParameter servers Inspired by work on distributed \\nkey-value stores, a parameter server architecture uses \\na set of servers to manage shared state that is updated',\n",
       " 'Parameter servers Inspired by work on distributed \\nkey-value stores, a parameter server architecture uses \\na set of servers to manage shared state that is updated \\nby a set of data-parallel workers. Unlike a standard \\nkey-value store, the write operation in a parameter \\nserver is specialized for parameter updates: it is \\ntypically an associative and commutative combiner, \\nlike addition-assignment (+=), that is applied to the \\ncurrent parameter value and the incoming update to \\nproduce a new parameter value. \\nParameter servers emerged as an architecture for \\nscalable topic modeling [66], and our previous system \\nDistBelief [21] showed how a similar architecture \\ncould be applied to deep neural network training. \\nProject Adam [14] demonstrated an efficient \\nparameter \\nserver \\narchitecture \\nfor \\ntraining \\nconvolutional neural networks, and Li et al.’s \\n“Parameter Server” [46] added innovations in \\nconsistency models, fault tolerance, and elastic',\n",
       " 'parameter \\nserver \\narchitecture \\nfor \\ntraining \\nconvolutional neural networks, and Li et al.’s \\n“Parameter Server” [46] added innovations in \\nconsistency models, fault tolerance, and elastic \\nrescaling. Despite earlier skepticism that parameter \\nservers would be compatible with GPU acceleration \\n[14], Cui et al. have recently shown that GeePS [19], a \\nparameter server specialized for use with GPUs, can \\nachieve speedups on modest-sized clusters. \\nMXNet [12] is a recent system that uses a parameter \\nserver to scale training, supports GPU acceleration, and \\nincludes a flexible programming model with interfaces for \\nmany languages. While MXNet partially fulfills our \\nextensibility requirements, the parameter server is \\n“privileged” code, which makes it difficult for researchers to \\ncustomize the handling of large models (§4.2). \\nThe parameter server architecture meets most of our \\nrequirements, and our DistBelief [21] uses parameter',\n",
       " 'customize the handling of large models (§4.2). \\nThe parameter server architecture meets most of our \\nrequirements, and our DistBelief [21] uses parameter \\nservers with a Caffe-like model definition format [36] to \\ngreat effect. We found this architecture to be insufficiently \\nextensible, because adding a new optimization algorithm, \\nor \\nexperimenting \\nwith \\nan \\nunconventional \\nmodel \\narchitecture would require our users to modify the \\nparameter server implementation, which uses C++ for \\nperformance. While some of the practitioners who use that \\nsystem are comfortable with making these changes, the \\nmajority are accustomed to writing models in high-level \\nlanguages, such as Python and Lua, and the complexity of \\nthe highperformance parameter server implementation is a \\nbarrier to entry. With TensorFlow we therefore sought a \\nhighlevel programming model that allows users to \\ncustomize the code that runs in all parts of the system (§3). \\n3 TensorFlow execution model',\n",
       " 'barrier to entry. With TensorFlow we therefore sought a \\nhighlevel programming model that allows users to \\ncustomize the code that runs in all parts of the system (§3). \\n3 TensorFlow execution model \\nTensorFlow uses a single dataflow graph to represent all \\ncomputation and state in a machine learning algorithm, \\nincluding the individual mathematical operations, the \\nparameters and their update rules, and the input \\npreprocessing \\n(Figure \\n1). \\nDataflow \\nmakes \\nthe \\ncommunication between subcomputations explicit, and \\ntherefore makes it easy to execute independent \\ncomputations in parallel, and partition the computation \\nacross multiple distributed devices. Dataflow TensorFlow \\ndiffers from batch dataflow systems (§2.2) in two respects: \\n• The model supports multiple concurrent executions on \\noverlapping subgraphs of the overall graph. \\n• Individual vertices may have mutable state that can be \\nshared between different executions of the graph.',\n",
       " 'overlapping subgraphs of the overall graph. \\n• Individual vertices may have mutable state that can be \\nshared between different executions of the graph. \\nThe key observation in the parameter server architecture \\n[21, 14, 46] is that mutable state is crucial when training \\nvery large models, because it becomes possible to make in-\\nplace updates to very large parameters, and propagate \\nthose updates to parallel training steps as quickly as \\npossible. Dataflow with mutable state enables TensorFlow \\nto mimic the functionality of a parameter server, but with \\nadditional flexibility, because it becomes possible to \\nexecute arbitrary dataflow subgraphs on the machines that \\nhost the shared model parameters. As a result, our users \\nhave been able to experiment with different optimization \\nalgorithms, consistency schemes, and parallelization \\nstrategies.',\n",
       " '5 \\n3.1 Dataflow graph elements \\nIn a TensorFlow graph, each vertex represents an \\natomic unit of computation, and each edge represents \\nthe output from or input to a vertex. We refer to the \\ncomputation at vertices as operations, and the values \\nthat flow along edges as tensors, because TensorFlow \\nis designed for mathematical computation, and uses \\ntensors (or multidimensional arrays) to represent all \\ndata in those computations. \\nTensors In TensorFlow, we model all data as tensors \\n(dense n-dimensional arrays) with each element \\nhaving one of a small number of primitive types, such \\nas int32, float32, or string. Tensors naturally represent \\nthe inputs to and results of the common mathematical \\noperations in many machine learning algorithms: for \\nexample, a matrix multiplication takes two 2-D tensors \\nand produces a 2-D tensor; and a mini-batch 2-D \\nconvolution takes two 4-D tensors and produces \\nanother 4-D tensor. \\nAll tensors in TensorFlow are dense. This decision',\n",
       " 'and produces a 2-D tensor; and a mini-batch 2-D \\nconvolution takes two 4-D tensors and produces \\nanother 4-D tensor. \\nAll tensors in TensorFlow are dense. This decision \\nensures that the lowest levels of the system can have \\nsimple implementations for memory allocation and \\nserialization, which reduces the overhead imposed by \\nthe framework. To represent sparse tensors, \\nTensorFlow offers two alternatives: either encode the \\ndata into variable-length string elements of a dense \\ntensor, or use a tuple of dense tensors (e.g., an n-D \\nsparse tensor with m non-zero elements could be \\nrepresented an m×n index matrix and a length-m \\nvalue vector). The size of a tensor can vary in one or \\nmore dimensions, making it possible to represent \\nsparse tensors with differing numbers of elements, at \\nthe cost of more sophisticated shape inference. \\nOperations An operation takes m ≥ 0 tensors as input, \\nand produces n ≥ 0 tensors as output. An operation',\n",
       " 'the cost of more sophisticated shape inference. \\nOperations An operation takes m ≥ 0 tensors as input, \\nand produces n ≥ 0 tensors as output. An operation \\nhas a named “type” (such as Const, MatMul, or Assign) \\nand may have zero or more compile-time attributes \\nthat determine its behavior. An operation can be \\ngeneric and variadic at compile-time: its attributes \\ndetermine both the expected types and arity of its \\ninputs and outputs. preprocessing, training, and \\ncheckpointing state. \\nFor example, the simplest operation Const has no inputs \\nand a single output. Const has an attribute T that \\ndetermines the type of its output, and an attribute Value \\nthat determines the value that it produces. AddN is variadic: \\nit has a type attribute T, and an integer attribute N that \\ndefines how many inputs (of type T) it accepts. \\nStateful operations: variables An operation can contain \\nmutable state that is read and/or written each time it',\n",
       " 'defines how many inputs (of type T) it accepts. \\nStateful operations: variables An operation can contain \\nmutable state that is read and/or written each time it \\nexecutes. A Variable operation owns a mutable buffer that \\nis used to store the shared parameters of a model as it is \\ntrained. A Variable has no inputs, and produces a reference \\nhandle, which acts as a typed capability for reading and \\nwriting the buffer. A Read operation takes a reference \\nhandle as input, and outputs the value of the variable as a \\ndense tensor. Several operations can modify the underlying \\nbuffer: for example, AssignAdd takes a reference handle r \\nand a tensor value x, and when executed performs the \\nupdate State0[r] ← State[r]+x. Subsequent Read(r) \\noperations produce the value State0[r]. \\nStateful operations: queues TensorFlow includes several \\nqueue implementations, which support more advanced \\nforms of coordination. The simplest queue is FIFOQueue,',\n",
       " 'Stateful operations: queues TensorFlow includes several \\nqueue implementations, which support more advanced \\nforms of coordination. The simplest queue is FIFOQueue, \\nwhich owns an internal queue of tensors, and supports \\nconcurrent access. Like a Variable, the FIFOQueue \\noperation produces a reference handle that can be \\nconsumed by one of the standard queue operations, such \\nas Enqueue and Dequeue. These operations respectively \\npush their input onto the tail of the queue, or pop the head \\nelement and output it. Enqueue will block if its given queue \\nis full, and Dequeue will block if its given queue is empty. \\nWhen queues are used in an input preprocessing pipeline, \\nthis blocking provides backpressure; it also supports \\nsynchronization (§4.4). \\n \\nFigure 1: A schematic TensorFlow dataflow graph for a training pipeline contains subgraphs for reading input data,',\n",
       " '6 \\n3.2 Partial and concurrent execution \\nTensorFlow uses the dataflow graph to represent all \\npossible computations in a particular application, and the \\nAPI for executing a graph allows the client to specify the \\nsubgraph that should be executed. A subgraph is specified \\ndeclaratively: the client selects zero or more edges to feed \\ninput tensors into the dataflow, and one or more edges to \\nfetch output tensors from the dataflow; the runtime then \\nprunes the graph to contain the necessary set of operations. \\nEach invocation of the API is called a step, and TensorFlow \\nsupports multiple concurrent steps on the same graph, \\nwhere stateful operations enable coordination between the \\nsteps. \\nFigure 1 shows a typical training application, with \\nmultiple subgraphs that execute concurrently, and \\ninteract through shared variables and queues. The \\ncore training subgraph depends on a set of model \\nparameters, and input batches from a queue. Many',\n",
       " 'interact through shared variables and queues. The \\ncore training subgraph depends on a set of model \\nparameters, and input batches from a queue. Many \\nconcurrent steps of the training subgraph update the \\nmodel based on different input batches, to implement \\ndata-parallel training. To fill the input queue, \\nconcurrent preprocessing steps transform individual \\ninput records (e.g., decoding images and applying \\nrandom distortions), and a separate I/O subgraph \\nreads records from a distributed file system. A \\ncheckpointing subgraph runs periodically for fault \\ntolerance (§4.3). \\nPartial and concurrent execution is responsible for \\nmuch of TensorFlow’s flexibility. Adding mutable state \\nand coordination via queues makes it possible to \\nspecify a wide variety of model architectures in \\n“unprivileged” code, which enables advanced users to \\nexperiment without modifying the internals of the \\nTensorFlow runtime. \\n3.3 Distributed execution \\nDataflow simplifies distributed execution, because it',\n",
       " 'experiment without modifying the internals of the \\nTensorFlow runtime. \\n3.3 Distributed execution \\nDataflow simplifies distributed execution, because it \\nmakes communication between subcomputations \\nexplicit. In principle, the same TensorFlow program \\ncan be deployed to a distributed cluster of GPUs for \\ntraining, a cluster of TPUs for serving, and a cellphone \\nfor mobile inference. \\nEach operation resides on a particular device, such \\nas a CPU or GPU in a particular task. A device is \\nresponsible for executing a kernel for each operation \\nassigned to it. \\nTensorFlow allows multiple kernels to be registered for a \\nsingle operation, with specialized implementations for a \\nparticular device or data type (see §5 for details). For many \\noperations, such as element-wise operators (Add, Sub, \\netc.), we use a single kernel implementation that can be \\ncompiled for CPU and GPU using different compilers. \\nThe TensorFlow runtime places operations on devices,',\n",
       " 'etc.), we use a single kernel implementation that can be \\ncompiled for CPU and GPU using different compilers. \\nThe TensorFlow runtime places operations on devices, \\nsubject to implicit or explicit device constraints in the graph. \\nThe placement algorithm computes a feasible set of devices \\nfor each operation, calculates the sets of operations that \\nmust be colocated, and selects a satisfying device for each \\ncolocation group. Stateful operations and operations their \\nstate must be placed on the same device, which leads to \\nimplicit colocation constraints. In addition, the user may \\nspecify partial device preferences such as “any device in a \\nparticular task”, or “a GPU in any task”, and the runtime will \\nrespect these constraints. A typical training application will \\nuse client-side programming constructs to add constraints \\nsuch that, for example, parameters are distributed among a \\nset of “PS” tasks. \\nOnce the operations in a graph have been placed, and the',\n",
       " 'use client-side programming constructs to add constraints \\nsuch that, for example, parameters are distributed among a \\nset of “PS” tasks. \\nOnce the operations in a graph have been placed, and the \\npartial subgraph has been computed for a step (§3.2), \\nTensorFlow partitions the operations into per-device \\nsubgraphs. A per-device subgraph for device d contains all \\nof the operations that were assigned to d, with additional \\nSend and Recv operations that replace edges across device \\nboundaries. Send transmits its single input to a specified \\ndevice as soon as the tensor is available, using a rendezvous \\nkey to name the value. Recv has a single output, and blocks \\nuntil the value for a specified rendezvous key is available \\nlocally, before producing that value. Send and Recv have \\nspecialized implementations for several device-type pairs; \\nwe describe some of these in Section 5. \\nWe optimized TensorFlow for executing large subgraphs',\n",
       " 'specialized implementations for several device-type pairs; \\nwe describe some of these in Section 5. \\nWe optimized TensorFlow for executing large subgraphs \\nrepeatedly with low latency. Once the graph for a step has \\nbeen pruned, placed, and partitioned, its subgraphs are \\ncached in their respective devices. A client session \\nmaintains the mapping from step definitions to cached \\nsubgraphs, so that a distributed step on a large graph can \\nbe initiated with one small message to each participating \\ntask. This model favors static, reusable graphs, but it can \\nsupport dynamic computations using dynamic control flow, \\nas the next subsection describes. \\n3.4 Dynamic control flow \\nMost evaluation in TensorFlow is strict: all inputs to an \\noperation must be computed before the operation',\n",
       " '7 \\nexecutes. Advanced algorithms—such as efficiently training \\na recurrent neural network [37]—require dynamic control \\nflow, which for efficiency requires non-strict evaluation. \\n \\nFigure 2: A conditional graph using Switch and Merge \\nTensorFlow supports conditional control flow using \\nthe primitive Switch and Merge operations, which are \\nbased on Arvind and Culler’s original dynamic \\ndataflow architectures [4]. Switch acts like a \\ndemultiplexer: it takes a data input and a control \\ninput, and uses the control input to select which of its \\ntwo outputs should produce a value. The Switch \\noutput not taken receives a special dead value, which \\npropagates recursively through the rest of the graph \\nuntil it reaches a Merge operation. Merge acts like a \\nmultiplexer: it forwards at most one non-dead input \\nto its output, or produces a dead output if both of its \\ninputs are dead. We use these primitives to build a \\nnonstrict conditional subgraph (Figure 2) that',\n",
       " 'to its output, or produces a dead output if both of its \\ninputs are dead. We use these primitives to build a \\nnonstrict conditional subgraph (Figure 2) that \\nexecutes one of two branches, based on the runtime \\nvalue of a tensor. \\nSwitch and Merge also support iteration. The \\nimplementation of loops in TensorFlow is based on \\nSwitch and Merge [4], with additional structural \\nconstraints based on timely dataflow [54] to simplify \\nthe distributed execution state. Like timely dataflow, \\nTensorFlow supports multiple concurrent iterations \\nand nested loops, but simplifies memory management \\nby restricting each operation to producing a single \\nvalue per output per iteration. \\n4 Extensibility case studies \\nBy choosing a unified dataflow graph to represent all \\ncomputation in TensorFlow, we have enabled users to \\nexperiment with features that were built into the \\nruntime of our previous system [21]. In this section, \\nwe discuss four extensions to TensorFlow that we',\n",
       " 'experiment with features that were built into the \\nruntime of our previous system [21]. In this section, \\nwe discuss four extensions to TensorFlow that we \\nhave built using simple dataflow primitives and “user-\\nlevel” code. \\n4.1 Differentiation and optimization \\nMany learning algorithms train a set of parameters \\nusing some variant of stochastic gradient descent \\n(SGD), which entails computing the gradients of a cost \\nfunction with respect to those parameters, then \\nupdating the parameters based on those gradients. \\nWe implement a user-level library for TensorFlow that \\nautomatically differentiates expressions. A user can, \\nfor example, define a neural network as a composition \\nof layers and a loss function, and the library will derive \\nthe backpropagation [64]. \\nThe differentiation algorithm performs breadth-first \\nsearch to identify all of the backwards paths from the target \\noperation (e.g., a loss function) to a set of parameters, and',\n",
       " 'The differentiation algorithm performs breadth-first \\nsearch to identify all of the backwards paths from the target \\noperation (e.g., a loss function) to a set of parameters, and \\nsums the partial gradients that each path contributes. Our \\nusers frequently specialize the gradients for some \\noperations, and they have implemented optimizations like \\nbatch normalization [32] and gradient clipping [59] to \\naccelerate training and make it more robust. We have \\nextended the algorithm to differentiate conditional and \\niterative \\nsubcomputations \\n(§3.4), \\nand \\ndeveloped \\ntechniques for managing GPU memory when iterating (and \\naccumulating intermediate values) over long sequences in \\nthe input data (similar to GeePS [19]). \\nTensorFlow users can also experiment with a wide range \\nof optimization algorithms, which compute new values for \\nthe parameters in each training step. SGD is easy to \\nimplement in a parameter server: for each parameter W,',\n",
       " 'of optimization algorithms, which compute new values for \\nthe parameters in each training step. SGD is easy to \\nimplement in a parameter server: for each parameter W, \\ngradient ∂L/∂W, and learning rate α, the update rule is W0 \\n← W − α × ∂L/∂W. A parameter server can implement SGD \\nby using -= as the write operation, and writing α × ∂L/∂W \\nto each W after a training step. \\nHowever, there are many more advanced optimization \\nschemes that are difficult to express as a single write \\noperation. For example, the Momentum algorithm \\naccumulates a “velocity” for each parameter based on its \\ngradient over multiple iterations, then computes the \\nparameter update from that accumulation; and many \\nrefinements to this algorithm have been proposed [67]. To \\nimplement Momentum in DistBelief [21], we had to modify \\nthe C++ code of the parameter server to change the \\nrepresentation of parameter data, and execute arbitrary \\ncode in the write operation; such modifications are beyond',\n",
       " 'the C++ code of the parameter server to change the \\nrepresentation of parameter data, and execute arbitrary \\ncode in the write operation; such modifications are beyond \\nthe majority of our users. Optimization algorithms are the \\nT r u e   b r a n c h \\nF a l s e   b r a n c h \\nS w i t c h \\nM e r g e \\nT r u e',\n",
       " '8 \\ntopic of active research, and our users have implemented \\nseveral on top of TensorFlow, including Momentum, \\nAdagrad, Adadelta, RMSProp, Adam, and L-BFGS. These can \\nbe built in TensorFlow using Variable operations and \\nprimitive mathematical operations without needing to \\nmodify the underlying system, which makes it easy to \\nexperiment with new algorithms as they emerge. \\n4.2 Handling very large models \\nTo train a model on high-dimensional data, such as words in \\na corpus of text [7], it is common to use a distributed \\nrepresentation, which embeds a training example as a \\npattern of activity across several neurons, which can be \\n \\nFigure 3: Schematic dataflow graph for a sparse \\nembedding layer containing a two-way sharded \\nembedding matrix. \\nlearned by backpropagation [29]. For example, in a \\nlanguage model, a training example might be a sparse \\nvector with non-zero entries corresponding to the IDs \\nof words in a vocabulary, and the distributed',\n",
       " 'language model, a training example might be a sparse \\nvector with non-zero entries corresponding to the IDs \\nof words in a vocabulary, and the distributed \\nrepresentation for each word will be a lower-\\ndimensional vector [6]. \\nInference proceeds by multiplying a batch of b \\nsparse vectors against an n×d embedding matrix, \\nwhere n is the number of words in the vocabulary, and \\nd is the desired dimensionality, to produce a much \\nsmaller b × d dense matrix representation; for \\ntraining, most optimization algorithms modify only \\nthe rows of the embedding matrix that were read by \\nthe sparse multiplication. In many TensorFlow models \\nthat process sparse data, n×d can amount to gigabytes \\nof parameters: e.g., a large language model may use \\nover 109 parameters with a vocabulary of 800,000 \\nwords [39], and we have experience with document \\nmodels [20] where the parameters occupy several \\nterabytes. Such models are too large to copy to a \\nworker on every use, or even to store in RAM on a',\n",
       " 'models [20] where the parameters occupy several \\nterabytes. Such models are too large to copy to a \\nworker on every use, or even to store in RAM on a \\nsingle host. \\nWe implement sparse embedding layers in the \\nTensorFlow graph as a composition of primitive \\noperations. Figure 3 shows a simplified graph for an \\nembedding layer that is split across two parameter \\nserver tasks. The core operation of this subgraph is \\nGather, which extracts a sparse set of rows from a \\ntensor, and TensorFlow colocates this operation with \\nthe variable on which it operates. The dynamic \\npartition (Part) operation divides the incoming indices \\ninto variable-sized tensors that contain the indices \\ndestined for each shard, and the dynamic static \\n(Stitch) operation reassembles the partial results from \\neach shard into a single result tensor. Each of these \\noperations has a corresponding gradient, so it \\nsupports automatic differentiation (§4.1), and the',\n",
       " 'each shard into a single result tensor. Each of these \\noperations has a corresponding gradient, so it \\nsupports automatic differentiation (§4.1), and the \\nresult is a set of sparse update operations that act on \\njust the values that were originally gathered from each \\nof the shards. \\nWhile sparse reads and updates are possible in a \\nparameter server [46], TensorFlow adds the flexibility \\nto offload arbitrary computation onto the devices that \\nhost \\nthe \\nshared \\nparameters. \\nFor \\nexample, \\nclassification models typically use a softmax classifier \\nthat multiplies the final output by a weight matrix with \\nc columns, where c is the number of possible classes; \\nfor a language model, c is the size of the vocabulary, \\nwhich can be large. Our users have experimented with \\nseveral schemes to accelerate the softmax calculation. \\nThe first is similar to an optimization in Project Adam \\n[14], whereby the weights are sharded across several \\ntasks, and the multiplication and gradient calculation',\n",
       " 'The first is similar to an optimization in Project Adam \\n[14], whereby the weights are sharded across several \\ntasks, and the multiplication and gradient calculation \\nare colocated with the shards. More efficient training \\nis possible using a sampled softmax [35], which \\nperforms a sparse multiplication based on the true \\nclass for an example and a set of randomly sampled \\nfalse classes. We compare the performance of these \\ntwo schemes in §6.4. \\n4.3 Fault tolerance \\nTraining a model can take several hours or days, even using \\na large number of machines [21, 14]. It is desirable to be \\nable to train a model using non-dedicated resources, for \\nexample using a cluster manager, like Mesos [28] or Borg \\n[72], that does not guarantee availability of the same \\nresources for the duration of the training process.',\n",
       " '9 \\nTherefore, a TensorFlow job is likely to experience failure \\nduring the training process, and we require some form of \\nfault tolerance. However, failures are unlikely to be so \\ncommon that individual operations need fault tolerance, so \\na mechanism like Spark’s RDDs [75] would impose \\nsignificant overhead for little benefit. There is no need to \\nmake every write to the parameter state durable, because \\nwe can recompute any update from the input data, and \\nmany learning algorithms do not require strong consistency \\n[62]. Although we do not use strong consistency for the \\ntraining state, we rely on a system like Chubby [8] or \\nZooKeeper [31] to map task IDs to IP addresses. \\nWe implement user-level checkpointing for fault \\ntolerance in TensorFlow, using primitive operations in the \\ngraph (Figure 1): Save writes one or more tensors to a \\ncheckpoint file, and Restore reads one or more tensors from \\na checkpoint file. Our typical configuration connects each',\n",
       " 'graph (Figure 1): Save writes one or more tensors to a \\ncheckpoint file, and Restore reads one or more tensors from \\na checkpoint file. Our typical configuration connects each \\nVariable in a task to the same Save operation, with one Save \\nper task, to maximize the I/O bandwidth to a distributed file \\nsystem. The Restore operations read named tensors from a \\nfile, and a standard Assign stores the restored value in its \\nrespective variable. During training, a typical client runs all \\nof the Save operations periodically to produce a new \\ncheckpoint; when the client starts up, it attempts to Restore \\nthe latest checkpoint. \\nTensorFlow includes a client library for constructing the \\nappropriate graph structure, and invoking Save and Restore \\nas necessary. This behavior is customizable: the user can \\napply different policies to subsets of the variables in a \\nmodel, or customize the checkpoint retention scheme. For \\nexample, many users retain checkpoints with the highest',\n",
       " 'apply different policies to subsets of the variables in a \\nmodel, or customize the checkpoint retention scheme. For \\nexample, many users retain checkpoints with the highest \\nscore in a custom evaluation metric. The implementation is \\nalso reusable: it may be used for model fine-tuning and \\nunsupervised pre-training [43, 45], which are forms of \\ntransfer learning, in which the parameters of a model \\ntrained on one task (e.g. recognizing general images) are \\nused as the starting point for another task (e.g. recognizing \\nparticular breeds of dog). Having checkpoint and parameter \\nmanagement as programmable operations in the graph \\ngives users the flexibility to implement schemes like these \\nand others that we have not anticipated. \\nThe checkpointing library does not attempt to \\nproduce consistent checkpoints: if training and \\ncheckpointing execute concurrently, the checkpoint \\nmay include none, all, or some of the updates from the \\ntraining step. This is no problem for models that we',\n",
       " 'checkpointing execute concurrently, the checkpoint \\nmay include none, all, or some of the updates from the \\ntraining step. This is no problem for models that we \\ntrain by asynchronous gradient descent [21]. \\nConsistent \\ncheckpoints \\nrequire \\nadditional \\nsynchronization to ensure that checkpointing does not \\nrun concurrently with update operations. For \\nexample, one can use the scheme in next subsection \\nto take a checkpoint after the synchronous update \\nstep. \\n4.4 Synchronous replica coordination \\nSGD is robust to asynchrony [62], and previous \\nsystems \\ntrain \\ndeep \\nneural \\nnetworks \\nusing \\nasynchronous parameter updates [21, 14], which are \\nbelieved scalable because they maintain high \\nthroughput in the presence of stragglers. The \\nincreased throughput comes at the cost of training \\nsteps using stale data. Some have recently revisited \\nthe assumption that synchronous training does not \\nscale [11, 19]. Since GPUs enable training with \\nhundreds—rather than thousands [45]—of machines,',\n",
       " 'the assumption that synchronous training does not \\nscale [11, 19]. Since GPUs enable training with \\nhundreds—rather than thousands [45]—of machines, \\nit may be possible to train a model synchronously in \\nless time than asynchronous training on the same \\nmachines. \\nThough we designed TensorFlow for asynchronous \\ntraining, we have begun experimenting with \\nsynchronous methods. The TensorFlow graph enables \\nusers to change how parameters are read and written \\nwhen training a model, and we implement three \\nalternatives. In the asynchronous case (Figure 4(a)), \\neach worker reads the current value when the step \\nbegins, and applies its gradient to the different current \\nvalue at the end: this ensures high utilization, but the \\nindividual steps use stale information, making each \\nstep less effective. The synchronous cases use queues \\n(§3.1) to coordinate execution: a blocking queue acts \\nas a barrier to ensure that all workers read the same \\nparameter version, and a second queue accumulates',\n",
       " '(§3.1) to coordinate execution: a blocking queue acts \\nas a barrier to ensure that all workers read the same \\nparameter version, and a second queue accumulates \\nmul-',\n",
       " '10 \\nFigure 5: The layered TensorFlow architecture. \\ntiple gradient updates in order to apply them atomically. \\nThe simple synchronous version (Figure 4(b)) accumulates \\nupdates from all workers before applying them, but slow \\nworkers limit overall throughput. \\nTo mitigate stragglers, we implement backup workers \\n(Figure 4(c), [11]), which are similar to MapReduce backup \\ntasks [22]. Whereas MapReduce starts backup tasks \\nreactively—after \\ndetecting \\na \\nstraggler—our \\nbackup \\nworkers run proactively, and the aggregation takes the first \\nm of n updates produced. We exploit the fact that SGD \\nsamples training data randomly, so each worker processes \\na different random batch. In Subsection 6.3 we show how \\nbackup workers improve throughput by up to 15%. \\n5 Implementation \\nWe implement TensorFlow as an extensible, crossplatform \\nlibrary. Figure 5 illustrates the system architecture: a thin C \\nAPI separates user-level in various languages from the core',\n",
       " 'We implement TensorFlow as an extensible, crossplatform \\nlibrary. Figure 5 illustrates the system architecture: a thin C \\nAPI separates user-level in various languages from the core \\nlibrary. In this section, we discuss the implementation of the \\nvarious components. \\nThe core TensorFlow library is implemented in C++ for \\nportability and performance: it runs on several operating \\nsystems including Linux, Mac OS X, Android, and iOS; the \\nx86 and various ARM-based CPU architectures; and \\nNVIDIA’s Kepler, Maxwell, and Pascal GPU \\nmicroarchitectures. The implementation is open-source, \\nand we have accepted several external contributions that \\nenable TensorFlow to run on other architectures. \\nThe distributed master translates user requests into \\nexecution across a set of tasks. Given a graph and a \\nstep definition, it prunes (§3.2) and partitions (§3.3) \\nthe graph to obtain subgraphs for each participating \\ndevice, and caches these subgraphs so that they may',\n",
       " 'step definition, it prunes (§3.2) and partitions (§3.3) \\nthe graph to obtain subgraphs for each participating \\ndevice, and caches these subgraphs so that they may \\nbe re-used in subsequent steps. Since the master sees \\nthe overall computation for a step, it applies standard \\noptimizations \\nsuch \\nas \\ncommon \\nsubexpression \\nelimination and constant folding; pruning is a form of \\ndead code elimination. It then coordinates execution \\nof the optimized subgraphs across a set of tasks. \\nThe dataflow executor in each task handles \\nrequests from the master, and schedules the \\nexecution of the kernels that comprise a local \\nsubgraph. We optimize the dataflow executor for \\nrunning large, fine-grained graphs with low overhead; \\nour current implementation dispatches approximately \\n2,000,000 null operations per second. The dataflow \\nexecutor dispatches kernels to local devices and runs \\nkernels in parallel when possible: e.g., by using \\nmultiple cores in a CPU device, or multiple streams on',\n",
       " 'executor dispatches kernels to local devices and runs \\nkernels in parallel when possible: e.g., by using \\nmultiple cores in a CPU device, or multiple streams on \\na GPU. \\nThe runtime contains over 200 standard operations, \\nincluding mathematical, array manipulation, control \\nflow, and state management operations. Many of the \\noperation \\nkernels \\nare \\nimplemented \\nusing \\nEigen::Tensor [34], which uses C++ templates to \\ngenerate efficient parallel code for multicore CPUs \\nand GPUs; however, we liberally use libraries like \\ncuDNN [13] to implement kernels where a more \\nefficient specialization is possible. We have also \\nimplemented support for quantization, which enables \\nfaster inference in environments such as mobile \\ndevices and high-throughput datacenter applications, \\nand use the gemmlowp low-precision matrix \\nmultiplication library [33] to accelerate quantized \\ncomputation. \\nWe specialize Send and Recv operations for each pair of',\n",
       " 'and use the gemmlowp low-precision matrix \\nmultiplication library [33] to accelerate quantized \\ncomputation. \\nWe specialize Send and Recv operations for each pair of \\nsource and destination device types. Transfers between \\nlocal CPU and GPU devices use the cudaMemcpyAsync() API \\nto overlap computation and data transfer; transfers \\nbetween two local GPUs use DMA to relieve pressure on the \\nhost. For transfers between tasks, TensorFlow supports \\n \\nFigure 4: Three parameter synchronization schemes for a single parameter in data-parallel training (§4.4): (a) \\nasynchronous, (b) synchronous without backup workers, and (c) synchronous with backup workers.',\n",
       " '11 \\nmultiple protocols, including gRPC over TCP, and RDMA \\nover Converged Ethernet. We are also investigating \\noptimizations for GPU-to-GPU communication that use \\ncollective operations [57]. \\nSection 4 describes features that we implement totally \\nabove the C API, in user-level code. Typically, users compose \\nstandard operations to build higher-level abstractions, such \\nas neural network layers, optimization algorithms (§4.1), \\nand sharded embedding computations (§4.2). TensorFlow \\nsupports multiple client languages, and we have prioritized \\nsupport for Python and C++, because our internal users are \\nmost familiar with these languages. As features become \\nmore established, we typically port them to C++, so that \\nusers can access an optimized implementation from all \\nclient languages. \\nIf it is difficult or inefficient to represent a \\nsubcomputation as a composition of operations, users can \\nregister additional kernels that provide an efficient',\n",
       " 'client languages. \\nIf it is difficult or inefficient to represent a \\nsubcomputation as a composition of operations, users can \\nregister additional kernels that provide an efficient \\nimplementation written in C++. We have found it profitable \\nto hand-implement fused kernels for some performance \\ncritical operations, such as a the ReLU and Sigmoid \\nactivation functions and their corresponding gradients. We \\nare currently investigating automatic kernel fusion using \\nHalide [61] and other compiler-based techniques. \\nIn addition to the core runtime, our colleagues have built \\nseveral tools that aid users of TensorFlow. These include \\nserving infrastructure for running inference in production, a \\nvisualization dashboard that enables users to follow the \\nprogress of a training run, a graph visualizer that helps users \\nto understand the connections in a model, and a distributed \\nprofiler that traces the execution of a computation across',\n",
       " 'progress of a training run, a graph visualizer that helps users \\nto understand the connections in a model, and a distributed \\nprofiler that traces the execution of a computation across \\nmultiple devices and tasks. We describe these tools in an \\nextended whitepaper [1], and they can be downloaded \\nfrom the project repository. \\n6 Evaluation \\nIn this section, we evaluate the performance of TensorFlow \\non several synthetic and realistic workloads. Unless \\notherwise stated, we run all experiments on a shared \\nproduction cluster, and all figures plot median values with \\nerror bars showing the 10th and 90th percentiles. \\nHere we focus on system performance metrics, rather \\nthan learning objectives like time to accuracy. TensorFlow is \\na system that allows machine learning practitioners and \\nresearchers to experiment with new techniques, and this \\nevaluation demonstrates that the system (i) has little \\noverhead, and (ii) can employ large amounts of',\n",
       " 'researchers to experiment with new techniques, and this \\nevaluation demonstrates that the system (i) has little \\noverhead, and (ii) can employ large amounts of \\ncomputation to accelerate real-world applications. While \\ntechniques like synchronous replication can enable some \\nmodels to converge in fewer steps overall, we defer the \\nanalysis of such improvements to other papers. \\n6.1 Single-machine benchmarks \\nAlthough TensorFlow is a system for “large-scale” \\nmachine learning, it is imperative that scalability does \\nnot mask poor performance at small scales [49]. Table \\n1 contains results from Chintala’s independent \\nbenchmark of convolutional models on TensorFlow \\nand three singlemachine frameworks [15]. All \\nframeworks use a six-core Intel Core i7-5930K CPU at \\n3.5GHz and an NVIDIA Titan X GPU. \\nLibrary \\nTraining step time (ms) \\nAlexNet \\nOverfeat \\nOxfordNet GoogleNet \\nCaffe [36] \\n324 \\n823 \\n1068 \\n1935 \\nNeon [56] \\n87 \\n211 \\n320 \\n270 \\nTorch [17] \\n81 \\n268 \\n529 \\n470 \\nTensorFlow \\n81 \\n279',\n",
       " 'Library \\nTraining step time (ms) \\nAlexNet \\nOverfeat \\nOxfordNet GoogleNet \\nCaffe [36] \\n324 \\n823 \\n1068 \\n1935 \\nNeon [56] \\n87 \\n211 \\n320 \\n270 \\nTorch [17] \\n81 \\n268 \\n529 \\n470 \\nTensorFlow \\n81 \\n279 \\n540 \\n445 \\nTable 1: Step times for training four convolutional \\nmodels with different libraries, using one GPU. All \\nresults are for training with 32-bit floats. The fastest \\nlibrary for each model is shown in bold. \\nTable 1 shows that TensorFlow achieves shorter \\nstep times than Caffe [36], and performance within 6% \\nof the latest version of Torch [17]. We attribute the \\nsimilar performance of TensorFlow and Torch to the \\nfact that both use the same version of the cuDNN \\nlibrary [13], which implements the convolution and \\npooling operations on the critical path for training; \\nCaffe uses open-source implementations for these \\noperations that are simpler but less efficient than \\ncuDNN. \\nThe \\nNeon \\nlibrary \\n[56] \\noutperforms \\nTensorFlow on three of the models, by using \\nhandoptimized \\nconvolutional',\n",
       " 'operations that are simpler but less efficient than \\ncuDNN. \\nThe \\nNeon \\nlibrary \\n[56] \\noutperforms \\nTensorFlow on three of the models, by using \\nhandoptimized \\nconvolutional \\nkernels \\n[44] \\nimplemented in assembly language; in principle, we \\ncould implement these kernels in TensorFlow, but we \\nhave not yet done so. \\n6.2 Synchronous replica microbenchmark \\nThe performance of our coordination implementation \\n(§4.4) is the main limiting factor for scaling with \\nadditional machines. Figure 6 shows that number of',\n",
       " '12 \\nnull training steps that TensorFlow performs per \\nsecond for varying model sizes, and increasing \\nnumbers of synchronous workers. In a null training \\nstep, a worker fetches the \\n \\nFigure 6: Baseline throughput for synchronous replication \\nwith a null model. Sparse accesses enable TensorFlow to \\nhandle larger models, such as embedding matrices (§4.2). \\nshared model parameters from 16 PS tasks, performs a \\ntrivial computation, and sends updates to the parameters. \\nThe Scalar curve in Figure 6 shows the best performance \\nthat we could expect for a synchronous training step, \\nbecause only a single 4-byte value is fetched from each PS \\ntask. The median step time is 1.8ms using a single worker, \\ngrowing to 8.8ms with 100 workers. These times measure \\nthe overhead of the synchronization mechanism, and \\ncapture some of the noise that we expect when running on \\na shared cluster. \\nThe Dense curves show the performance of a null step \\nwhen the worker fetches the entire model. We repeat the',\n",
       " 'capture some of the noise that we expect when running on \\na shared cluster. \\nThe Dense curves show the performance of a null step \\nwhen the worker fetches the entire model. We repeat the \\nexperiment with models of size 100MB and 1GB, with the \\nparameters sharded equally over 16 PS tasks. The median \\nstep time for 100MB increases from 147ms with one worker \\nto 613ms with 100 workers. For 1GB, it increases from 1.01s \\nwith one worker to 7.16s with 100 workers. \\nFor large models, it is typical that a training step accesses \\nonly a subset of the parameters, and the Sparse curves \\nshow the throughput of the embedding lookup operation \\nfrom Subsection 4.2. Each worker reads 32 randomly \\nselected entries from a large embedding matrix containing \\n1GB or 16GB of data. As expected, the step times do not \\nvary with the size of the embedding, and TensorFlow \\nachieves step times ranging from 5 to 20ms. \\n6.3 Image classification \\nDeep neural networks have achieved breakthrough',\n",
       " 'vary with the size of the embedding, and TensorFlow \\nachieves step times ranging from 5 to 20ms. \\n6.3 Image classification \\nDeep neural networks have achieved breakthrough \\nperformance on computer vision tasks such as recognizing \\nobjects in photographs [42], and these tasks are a key \\napplication for TensorFlow at Google. Training a network to \\nhigh accuracy requires a large amount of computation, and \\nwe use TensorFlow to scale out the computation across a \\ncluster of GPU-enabled servers. In these experiments, we \\nfocus on Google’s Inception-v3 model, which achieves \\n78.8% accuracy the ILSVRC 2012 image classification \\nchallenge [70]; the same techniques apply to other deep \\nconvolutional models—such as Microsoft’s ResNet [26]—\\nthat TensorFlow users have implemented. We investigate \\nthe scalability of training the Inception-v3 model using \\nmultiple replicas. We configure a TensorFlow job with 17 PS \\ntasks, and vary the number of worker tasks. Each worker',\n",
       " 'the scalability of training the Inception-v3 model using \\nmultiple replicas. We configure a TensorFlow job with 17 PS \\ntasks, and vary the number of worker tasks. Each worker \\ntask has one NVIDIA K40 GPU and 5 IvyBridge cores, and a \\nPS task has 8 IvyBridge cores. We investigate the effect of \\ncoordination (§4.4) on training performance, using up to \\n200 workers to validate recent promising results for \\nsynchronous training [11, 19]. In particular, if synchronous \\ntraining can be made efficient, a model such as Inception-\\nV3 will train in fewer steps, and converge to a higher \\naccuracy than with asynchronous training [11]. \\nTraining throughput improves to 2,300 images per \\nsecond as we increase the number of workers to 200, \\nbut with diminishing returns (Figure 7(a)). Figures 7(b) \\nand (c) explain the limits to scaling: as we add more \\nworkers, the step time increases, because there is \\nmore contention on the PS tasks, both at the network',\n",
       " 'and (c) explain the limits to scaling: as we add more \\nworkers, the step time increases, because there is \\nmore contention on the PS tasks, both at the network \\ninterface and in the aggregation of updates. As \\nexpected, for all configurations, synchronous steps \\nare longer than asynchronous steps, because all \\nworkers must wait for the slowest worker to catch up \\nbefore starting the next step. While the median \\nsynchronous step is approximately 10% longer than an \\nasynchronous step with the same workers, above the \\n90th percentile the synchronous performance \\ndegrades \\nsharply, \\nbecause \\nstragglers \\ndisproportionately impact the tail. \\nTo mitigate tail latency, we can add backup workers, \\nso that a step completes when the first m of n tasks \\nproduce gradients. Figure 8 shows the effect on step \\ntime of adding backup workers to a 50-worker \\nInception training job. Each additional backup worker \\nup to and including the fourth reduces the median',\n",
       " 'time of adding backup workers to a 50-worker \\nInception training job. Each additional backup worker \\nup to and including the fourth reduces the median \\nstep time, because the probability of a straggler',\n",
       " '13 \\naffecting the step decreases. Adding a fifth backup \\nworker slightly degrades performance, because the \\n51st worker (i.e., the first whose result is discarded) is \\nmore likely to be a non-straggler that generates more \\nincoming traffic for the PS tasks. Figure 8 also plots the \\nnormalized speedup for each configuration, which we \\ndefine as t(b)/t(0)×50/(50+b) (where t(b) is the \\nmedian step time with b backup workers), and which \\ndiscounts the speedup by the fraction of additional \\nresources consumed. Although adding 4 backup \\nworkers achieves the shortest overall step time \\n(1.93s), adding 3 achieves the highest normalized \\nspeedup (9.5%), and hence trains the model to the \\nsame quality using less aggregate GPU-time. \\nFigure 8: Backup workers reduce the step time for 50worker \\nInception-v3 training. 4 backup workers give the shortest \\noverall step time, but 3 backup workers are most efficient \\nwhen we normalize for the total resources used. \\n6.4 Language modeling',\n",
       " 'Inception-v3 training. 4 backup workers give the shortest \\noverall step time, but 3 backup workers are most efficient \\nwhen we normalize for the total resources used. \\n6.4 Language modeling \\nGiven a sequence of words, a language model predicts the \\nmost probable next word [6]. Therefore, language models \\nare integral to predictive text, speech recognition, and \\ntranslation applications. In this experiment, we investigate \\nhow TensorFlow can train a recurrent neural network (viz. \\nLSTM-512-512 [39]) to model the text in the One Billion \\nWord Benchmark [10]. The vocabulary size |V | limits the \\nperformance of training, because the final layer must \\ndecode the output state into probabilities for each of |V | \\nclasses [35]. The resulting parameters can be large (|V |×d \\nfor output state dimension d) so we use the techniques for \\nhandling large models from Subsection 4.2. We use a \\nrestricted vocabulary of the most common 40,000 words—\\ninstead of the full 800,000 words [10]—in order to',\n",
       " 'handling large models from Subsection 4.2. We use a \\nrestricted vocabulary of the most common 40,000 words—\\ninstead of the full 800,000 words [10]—in order to \\nexperiment with smaller configurations. \\nFigure 9 shows the training throughput, measured in \\nFigure 9: Increasing the number of PS tasks leads to \\nincreased throughput for language model training, by \\nparallelizing the softmax computation. Sampled softmax \\nincreases throughput by performing less computation. \\nwords per second, for varying numbers of PS and \\nworker tasks, and two softmax implementations. The \\nfull softmax (dashed lines) multiplies each output by a \\n \\nFigure 7: (a) Inception-v3 training throughput increases with up to 200 workers. However, adding more workers \\ngets diminishing returns because the step time increases for both (b) asynchronous and (c) synchronous \\nreplication. \\n \\n0 \\n1 \\n2 \\n3 \\n4 \\n5 \\nNumber of backup workers \\n1.9 \\n2.0 \\n2.1 \\n2.2 \\n2.3 \\n2.4 \\n2.5 \\nStep time \\n1.00 \\n1.02 \\n1.04 \\n1.06 \\n1.08 \\nSpeedup',\n",
       " '14 \\n512 × 40,000 weight matrix sharded across the PS \\ntasks. Adding more PS tasks increases the throughput, \\nbecause TensorFlow can exploit distributed model \\nparallelism [21, 41] and perform the multiplication \\nand gradient calculation on the PS tasks, as in Project \\nAdam [14]. Adding a second PS task is more effective \\nthan increasing from 4 to 32, or 32 to 256 workers. \\nEventually the throughput saturates, as the LSTM \\ncalculations dominate the training step. \\nThe sampled softmax (solid lines) reduces the data \\ntransferred and the computation performed at the PS \\ntasks [35]. Instead of a dense weight matrix, it \\nmultiplies the output by a random sparse matrix \\ncontaining weights for the true class and a random \\nsample of false classes. We sample 512 classes for \\neach batch, which reduces the softmax data transfer \\nand computation by a factor of 78. \\n7 Conclusions \\nWe have described the TensorFlow system and its \\nextensible dataflow-based programming model. The core',\n",
       " 'and computation by a factor of 78. \\n7 Conclusions \\nWe have described the TensorFlow system and its \\nextensible dataflow-based programming model. The core \\nidea of this paper is that TensorFlow’s dataflow \\nrepresentation subsumes existing work on parameter \\nserver systems, and offers a uniform programming model \\nthat allows users to harness large-scale heterogeneous \\nsystems, both for production tasks and for experimenting \\nwith new approaches. We have shown several examples of \\nhow the TensorFlow programming model supports \\nexperimentation (§4) and demonstrated that the resulting \\nimplementations are performant and scalable (§6). \\nOur initial experience with TensorFlow is encouraging. A \\nlarge number of groups at Google have deployed \\nTensorFlow in production, and TensorFlow is helping our \\nresearch colleagues to make new advances in machine \\nlearning. Since we released TensorFlow as open-source \\nsoftware, over 8,000 people have forked the source code',\n",
       " 'research colleagues to make new advances in machine \\nlearning. Since we released TensorFlow as open-source \\nsoftware, over 8,000 people have forked the source code \\nrepository, the binary distribution has been downloaded \\n500,000 times, and our users have published dozens of \\nmachine learning models that use TensorFlow. \\nTensorFlow is a work in progress. Its flexible dataflow \\nrepresentation enables power users to achieve excellent \\nperformance, but we have not yet determined default \\npolicies that work well for most users. Further research on \\nautomatic optimization should bridge this gap. On the \\nsystem level, we are actively developing algorithms for \\nautomatic placement, kernel fusion, memory management, \\nand scheduling. While the current implementations of \\nmutable state and fault tolerance suffice for applications \\nwith weak consistency requirements, we expect that some \\nTensorFlow applications will require stronger consistency,',\n",
       " 'mutable state and fault tolerance suffice for applications \\nwith weak consistency requirements, we expect that some \\nTensorFlow applications will require stronger consistency, \\nand we are investigating how to build such policies at user-\\nlevel. Finally, our users are demanding, and some have \\nbegun to chafe at the limitations of a static dataflow graph, \\nespecially for algorithms like deep reinforcement learning \\n[51]. Therefore, we face the intriguing problem of providing \\na system that transparently and efficiently uses distributed \\nresources, even when the structure of the computation \\nunfolds dynamically. \\nBy sharing the implementation of TensorFlow and \\nengaging with the research community, we hope that this \\nwork will spur further research in distributed systems and \\nmachine learning. \\nAcknowledgments \\nWe gratefully acknowledge contributions from our \\ncolleagues within Google, and from members of the wider \\nmachine learning community. In particular, we appreciate',\n",
       " 'Acknowledgments \\nWe gratefully acknowledge contributions from our \\ncolleagues within Google, and from members of the wider \\nmachine learning community. In particular, we appreciate \\nthe feedback we have received both from the rest of the \\nGoogle Brain team and the hundreds of DistBelief and \\nTensorFlow users that has helped us improve the usability \\nof functionality of the system. \\nMany individuals have contributed to TensorFlow, \\nincluding: John Giannandrea (for creating a supportive \\nresearch environment); Irina Kofman, Amy McDonald \\nSandjideh, and Phing Turner (project management); \\nAshish Agarwal, Dave Andersen, Anelia Angelova, \\nEugene Brevdo, Yaroslav Bulatov, Jerjou Cheng, \\nMaciek Chociej, Craig Citro, Greg Corrado, George \\nDahl, Andrew Dai, Lucy Gao, mig Gerard, Ian \\nGoodfellow, Stephan Gouws, Gunhan Gulsoy, Steinar \\nGunderson, Andrew Harp, Peter Hawkins, Yangqing \\nJia, Rafal Jozefowicz, Łukasz Kaiser, Naveen Kumar, \\nGeoffrey Hinton, Mrinal Kalakrishnan, Anjuli Kannan,',\n",
       " 'Gunderson, Andrew Harp, Peter Hawkins, Yangqing \\nJia, Rafal Jozefowicz, Łukasz Kaiser, Naveen Kumar, \\nGeoffrey Hinton, Mrinal Kalakrishnan, Anjuli Kannan, \\nRasmus Larsen, \\nYutaka Leon-Suematsu, Frank Li, Peter Liu, Xiaobing \\nLiu, Olivia Nordquist, Chris Olah, Nishant Patil, \\nSaurabh Saxena, Mike Schuster, Andrew Selle, Pierre \\nSermanet, Noam Shazeer, Jonathon Shlens, Jascha \\nSohl-Dickstein, Ilya Sutskever, Kunal Talwar, Philip \\nTucker, Vincent Vanhoucke, Oriol Vinyals, Chad \\nWhipkey, Yonghui Wu, Ke Yang, Zongheng Yang, and \\nYao Zhang (general contributions to the project); Shan \\nCarter, Doug Fritz, Patrick Hurst, Dilip Krishnan, Dan \\nMane, Daniel Smilkov, Fer-´ nanda Viegas, Martin \\nWattenberg, James Wexler, Jimbo´ Wilson, Kanit \\nWongsuphasawat, Cassandra Xia, and the Big Picture',\n",
       " '15 \\nteam (visualization); Chris Leary, Robert Hundt, \\nRobert Springer, Cliff Young, and the Stream Executor \\nteam (accelerator support); Norm Jouppi and the \\nteam that created the Tensor Processing Unit; Kayur \\nPatel, Michael Piatek, and the coLab team; and the \\ngrowing community of open-source contributors and \\nusers who have helped make TensorFlow better. \\nReferences \\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, \\nZ. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, \\nM. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, \\nG. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,´ \\nM. Kudlur, J. Levenberg, D. Mane, R. Monga, S. \\nMoore, D. G. Murray, C. Olah, M. Schuster, J. \\nShlens, B. Steiner, I. Sutskever, K. Talwar, P. A. \\nTucker, V. Vanhoucke, V. Vasudevan, F. B. \\nViegas, O. Vinyals, P. Warden, M. Watten-´ berg, \\nM. Wicke, Y. Yu, and X. Zheng. Tensorflow: Large-\\nscale machine learning on heterogeneous \\ndistributed systems. CoRR, abs/1603.04467, \\n2016. \\narxiv.org/abs/1603.04467.',\n",
       " 'M. Wicke, Y. Yu, and X. Zheng. Tensorflow: Large-\\nscale machine learning on heterogeneous \\ndistributed systems. CoRR, abs/1603.04467, \\n2016. \\narxiv.org/abs/1603.04467. \\nSoftware \\navailable from tensorflow.org. \\n[2] R. Al-Rfou, G. Alain, A. Almahairi, C. Angermueller, D. \\nBahdanau, N. Ballas, F. Bastien, J. Bayer, A. Belikov, A. \\nBelopolsky, Y. Bengio, A. Bergeron, J. Bergstra, V. \\nBisson, J. Bleecher Snyder, N. Bouchard, N. Boulanger-\\nLewandowski, X. Bouthillier, A. de Brebisson, O. \\nBreuleux, P.-´ L. Carrier, K. Cho, J. Chorowski, P. \\nChristiano, T. Cooijmans, M.-A. Cotˆ e, M. C´ otˆ e, A. \\nCourville,´ Y. N. Dauphin, O. Delalleau, J. Demouth, G. \\nDesjardins, S. Dieleman, L. Dinh, M. Ducoffe, V. \\nDumoulin, S. Ebrahimi Kahou, D. Erhan, Z. Fan, O. \\nFirat, M. Germain, X. Glorot, I. Goodfellow, M. \\nGraham, C. Gulcehre, P. Hamel, I. Harlouchet, J.-P. \\nHeng, B. Hidasi, S. Honari, A. Jain, S. Jean, K. Jia, M. \\nKorobov, V. Kulkarni, A. Lamb, P. Lamblin, E. Larsen, C.',\n",
       " 'Graham, C. Gulcehre, P. Hamel, I. Harlouchet, J.-P. \\nHeng, B. Hidasi, S. Honari, A. Jain, S. Jean, K. Jia, M. \\nKorobov, V. Kulkarni, A. Lamb, P. Lamblin, E. Larsen, C. \\nLaurent, S. Lee, S. Lefrancois, S. Lemieux, N. Leonard, \\nZ. Lin, J. A. Livezey,´ C. Lorenz, J. Lowin, Q. Ma, P.-A. \\nManzagol, O. Mastropietro, R. T. McGibbon, R. \\nMemisevic, B. van Merrienboer, V. Michalski, M. \\nMirza,¨ A. Orlandi, C. Pal, R. Pascanu, M. Pezeshki, C. \\nRaffel, D. Renshaw, M. Rocklin, A. Romero, M. Roth, P. \\nSadowski, J. Salvatier, F. Savard, J. Schluter, J. \\nSchulman, G. Schwartz, I. V. Serban,¨ D. Serdyuk, S. \\nShabanian, E. Simon, S. Spieckermann, S. R. \\nSubramanyam, J. Sygnowski, J. Tanguay, G. van Tulder, \\nJ. Turian, S. Urban, P. Vincent, F. Visin, H. de Vries, D. \\nWarde-Farley, D. J. Webb, M. Willson, K. Xu, L. Xue, L. \\nYao, S. Zhang, and Y. Zhang. Theano: A Python \\nframework for fast computation of mathematical \\nexpressions. arXiv e-prints, abs/1605.02688, May \\n2016. arxiv.org/abs/1605.02688.',\n",
       " 'Yao, S. Zhang, and Y. Zhang. Theano: A Python \\nframework for fast computation of mathematical \\nexpressions. arXiv e-prints, abs/1605.02688, May \\n2016. arxiv.org/abs/1605.02688. \\n[3] A. Angelova, A. Krizhevsky, and V. Vanhoucke. \\nPedestrian detection with a large-field-of-view deep \\nnetwork. In Robotics and Automation (ICRA), 2015 \\nIEEE International Conference on, pages 704–711. \\nIEEE, 2015. CalTech PDF. \\n[4] Arvind and D. E. Culler. Annual review of computer \\nscience vol. 1, 1986. chapter Dataflow Architectures, \\npages \\n225–253. \\n1986. \\nwww.dtic.mil/cgi-\\nbin/GetTRDoc?Location=U2& \\ndoc=GetTRDoc.pdf&AD=ADA166235. \\n[5] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object \\nrecognition with visual attention. arXiv preprint \\narXiv:1412.7755, 2014. arxiv.org/abs/1412.7755. \\n[6] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A \\nneural probabilistic language model. \\nJournal of Machine Learning Research, 3:1137– \\n1155, \\n2003. \\nwww.iro.umontreal.ca/˜lisa/pointeurs/',\n",
       " '[6] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A \\nneural probabilistic language model. \\nJournal of Machine Learning Research, 3:1137– \\n1155, \\n2003. \\nwww.iro.umontreal.ca/˜lisa/pointeurs/ \\nBengioDucharmeVincentJauvin jmlr.pdf. \\n[7] T. Brants and A. Franz. Web 1T 5-gram version 1, 2006. \\ncatalog.ldc.upenn.edu/LDC2006T13. \\n[8] M. \\nBurrows. \\nThe \\nChubby \\nlock \\nservice \\nfor \\nlooselycoupled distributed systems. In Proceedings of \\nthe 7th Symposium on Operating Systems Design and \\nImplementation, OSDI ’06, pages 335–350, Berkeley, \\nCA, \\nUSA, \\n2006. \\nUSENIX \\nAssociation. \\nwww.usenix.org/legacy/event/osdi06/tech/full \\npapers/burrows/burrows.pdf. \\n[9] R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu. Sample \\nsize selection in optimization methods for machine \\nlearning. Mathematical Programming, 134(1):127–\\n155, 2012. dx.doi.org/10.1007/s10107012-0572-5. \\n[10] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, \\nand P. Koehn. One billion word benchmark for',\n",
       " '155, 2012. dx.doi.org/10.1007/s10107012-0572-5. \\n[10] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, \\nand P. Koehn. One billion word benchmark for \\nmeasuring progress in statistical language modeling. \\nCoRR, abs/1312.3005, 2013. arxiv.org/abs/1312.3005.',\n",
       " '16 \\n[11] J. Chen, R. Monga, S. Bengio, and R. Jozefowicz. \\nRevisiting \\ndistributed \\nsynchronous \\nSGD. \\nIn \\nInternational Conference on Learning Representations \\nWorkshop Track, 2016. arxiv.org/abs/1604.00981. \\n[12] T. Chen, \\nM. Li, \\nY. Li, \\nM. Lin, N. Wang, \\nM. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. \\nMXNet: A flexible and efficient machine learning \\nlibrary for heterogeneous distributed systems. In \\nProceedings of the Workshop on Machine \\nLearning \\nSystems \\nat \\nNeural \\nInformation \\nProcessing Systems (LearningSys), Dec. 2015. \\nwww.cs.cmu.edu/ muli/file/mxnet-learning-sys.pdf. \\n[13] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. \\nTran, B. Catanzaro, and E. Shelhamer. cuDNN: Efficient \\nprimitives \\nfor \\ndeep \\nlearning. \\narXiv \\npreprint \\narXiv:1410.0759, 2014. arxiv.org/abs/1410.0759. \\n[14] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. \\nProject Adam: Building an efficient and scalable deep \\nlearning training system. In 11th USENIX Symposium',\n",
       " '[14] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. \\nProject Adam: Building an efficient and scalable deep \\nlearning training system. In 11th USENIX Symposium \\non Operating Systems Design and Implementation \\n(OSDI 14), pages 571–582, 2014. \\nwww.usenix.org/system/files/conference/osdi14/ \\nosdi14-paper-chilimbi.pdf. \\n[15] S. Chintala. convnet-benchmarks, \\n2016. \\ngithub.com/soumith/convnet-benchmarks. \\n[16] E. S. Chung, J. D. Davis, and J. Lee. LINQits: Big data on \\nlittle clients. In Proceedings of the 40th Annual \\nInternational Symposium on Computer Architecture, \\nISCA ’13, pages 261–272, New York, NY, USA, 2013. \\nACM. doi.acm.org/10.1145/2485922.2485945. \\n[17] R. Collobert, S. Bengio, and J. Mariethoz.´ Torch: A \\nmodular machine learning software library. Technical \\nreport, \\nIDIAP, \\n2002. \\ninfoscience.epfl.ch/record/82802/files/rr02-46.pdf. \\n[18] D. Crankshaw, P. Bailis, J. E. Gonzalez, H. Li, Z. Zhang, \\nM. J. Franklin, A. Ghodsi, and M. I. Jordan. The missing',\n",
       " 'report, \\nIDIAP, \\n2002. \\ninfoscience.epfl.ch/record/82802/files/rr02-46.pdf. \\n[18] D. Crankshaw, P. Bailis, J. E. Gonzalez, H. Li, Z. Zhang, \\nM. J. Franklin, A. Ghodsi, and M. I. Jordan. The missing \\npiece in complex analytics: Low latency, scalable \\nmodel management and serving with Velox. In CIDR \\n2015, Seventh Biennial Conference on Innovative Data \\nSystems Research, Asilomar, CA, USA, January 4-7, \\n2015, \\nOnline \\nProceedings, \\n2015. \\narxiv.org/abs/1409.3809. \\n[19] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and E. P. \\nXing. GeePS: Scalable deep learning on distributed \\nGPUs with a GPUspecialized parameter server. In \\nProceedings of the Eleventh European Conference on \\nComputer \\nSystems, \\nEuroSys \\n’16, \\n2016. \\nwww.pdl.cmu.edu/PDLFTP/CloudComputing/GeePS-\\ncui-eurosys16.pdf. \\n[20] A. Dai, C. Olah, and Q. V. Le. Document embedding \\nwith \\nparagraph \\nvectors. \\narXiv \\npreprint \\narXiv:1507.07998, 2015. arxiv.org/abs/1507.07998.',\n",
       " 'cui-eurosys16.pdf. \\n[20] A. Dai, C. Olah, and Q. V. Le. Document embedding \\nwith \\nparagraph \\nvectors. \\narXiv \\npreprint \\narXiv:1507.07998, 2015. arxiv.org/abs/1507.07998. \\n[21] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. \\nV. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. \\nYang, and A. Y. Ng. Large scale distributed deep \\nnetworks. In NIPS, 2012. Google Research PDF. \\n[22] J. Dean and S. Ghemawat. Mapreduce: Simplified data \\nprocessing on large clusters. In Proceedings of the 6th \\nConference on Symposium on Opearting Systems \\nDesign & Implementation - Volume 6, OSDI’04, \\nBerkeley, CA, USA, 2004. USENIX Association. \\nresearch.google.com/archive/mapreduceosdi04.pdf. \\n[23] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, \\nT. Mikolov, et al. DeVISE: A deep visualsemantic \\nembedding model. In Advances in Neural Information \\nProcessing \\nSystems, \\npages \\n2121–2129, \\n2013. \\nresearch.google.com/pubs/archive/41473.pdf. \\n[24] J. Gonzalez-Dominguez, I. Lopez-Moreno, P. J.',\n",
       " 'embedding model. In Advances in Neural Information \\nProcessing \\nSystems, \\npages \\n2121–2129, \\n2013. \\nresearch.google.com/pubs/archive/41473.pdf. \\n[24] J. Gonzalez-Dominguez, I. Lopez-Moreno, P. J. \\nMoreno, and J. Gonzalez-Rodriguez. Frame-by-frame \\nlanguage identification in short utterances using deep \\nneural networks. Neural Networks, 64:49–58, 2015. \\nresearch.google.com/en//pubs/archive/42929.pdf. \\n[25] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. \\nWarde-Farley, S. Ozair, A. C. Courville, and Y. Bengio. \\nGenerative adversarial nets. In Advances in Neural \\nInformation \\nProcessing \\nSystems \\n27: \\nAnnual \\nConference on Neural Information Processing Systems \\n2014, December 8-13 2014, Montreal, Quebec, \\nCanada, \\npages \\n2672– \\n2680, \\n2014. \\npapers.nips.cc/paper/5423-generativeadversarial-\\nnets. \\n[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual \\nlearning \\nfor \\nimage \\nrecognition. \\nCoRR, \\nabs/1512.03385, 2015. arxiv.org/abs/1512.03385.',\n",
       " 'nets. \\n[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual \\nlearning \\nfor \\nimage \\nrecognition. \\nCoRR, \\nabs/1512.03385, 2015. arxiv.org/abs/1512.03385. \\n[27] G. Heigold, V. Vanhoucke, A. Senior, P. Nguyen, M. \\nRanzato, M. Devin, and J. Dean. Multilingual acoustic',\n",
       " '17 \\nmodels using distributed deep neural networks. In \\nAcoustics, Speech and Signal Processing (ICASSP), \\n2013 IEEE International Conference on, pages 8619–\\n8623. \\nIEEE, \\n2013. \\nresearch.google.com/pubs/archive/40807.pdf. \\n[28] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. \\nJoseph, R. Katz, S. Shenker, and I. Stoica. Mesos: A \\nplatform for fine-grained resource sharing in the data \\ncenter. In Proceedings of the 8th USENIX Conference \\non Networked Systems Design and Implementation, \\nNSDI’11, pages 295–308, Berkeley, CA, USA, 2011. \\nUSENIX \\nAssociation. \\nwww.cs.berkeley.edu/˜alig/papers/mesos.pdf. \\n[29] G. E. Hinton. Learning distributed representations of \\nconcepts. In Proceedings of the Eighth Annual \\nConference of the Cognitive Science Society, pages 1–\\n12. \\nHillsdale, \\nNJ: \\nErlbaum, \\n1986. \\nwww.cogsci.ucsd.edu/˜ajyu/Teaching/Cogs202 \\nsp13/Readings/hinton86.pdf. \\n[30] G. E. Hinton, \\nL. Deng, D. Yu, \\nG. E. Dahl, \\nA. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke,',\n",
       " 'NJ: \\nErlbaum, \\n1986. \\nwww.cogsci.ucsd.edu/˜ajyu/Teaching/Cogs202 \\nsp13/Readings/hinton86.pdf. \\n[30] G. E. Hinton, \\nL. Deng, D. Yu, \\nG. E. Dahl, \\nA. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, \\nP. Nguyen, T. N. Sainath, and B. Kingsbury. Deep \\nneural networks for acoustic modeling in speech \\nrecognition: The shared views of four research \\ngroups. IEEE Signal Process. Mag., 29(6):82– 97, \\n2012. \\nwww.cs.toronto.edu/˜gdahl/papers/ \\ndeepSpeechReviewSPM2012.pdf. \\n[31] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed. \\nZooKeeper: Wait-free coordination for internetscale \\nsystems. In Proceedings of the 2010 USENIX \\nConference on USENIX Annual Technical Conference, \\nUSENIXATC’10, pages 11–11, Berkeley, CA, USA, 2010. \\nUSENIX \\nAssociation. \\nwww.usenix.org/legacy/event/atc10/tech/full \\npapers/Hunt.pdf. \\n[32] S. Ioffe and C. Szegedy. Batch normalization: \\nAccelerating deep network training by reducing \\ninternal covariate shift. CoRR, abs/1502.03167, 2015. \\narxiv.org/abs/1502.03167.',\n",
       " 'papers/Hunt.pdf. \\n[32] S. Ioffe and C. Szegedy. Batch normalization: \\nAccelerating deep network training by reducing \\ninternal covariate shift. CoRR, abs/1502.03167, 2015. \\narxiv.org/abs/1502.03167. \\n[33] B. Jacob et al. gemmlowp: a small selfcontained low-\\nprecision \\nGEMM \\nlibrary, \\n2015. \\ngithub.com/google/gemmlowp. \\n[34] B. Jacob, G. Guennebaud, et al. Eigen library for linear \\nalgebra. eigen.tuxfamily.org. \\n[35] S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using \\nvery large target vocabulary for neural machine \\ntranslation. In Proceedings of the 53rd Annual Meeting \\nof the Association for Computational Linguistics and \\nthe 7th International Joint Conference on Natural \\nLanguage Processing (Volume 1: Long Papers), pages \\n1–10, Beijing, China, July 2015. Association for \\nComputational \\nLinguistics. \\nwww.aclweb.org/anthology/P15-1001. \\n[36] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. \\nGirshick, S. Guadarrama, and T. Darrell. Caffe: \\nConvolutional \\narchitecture \\nfor',\n",
       " 'Linguistics. \\nwww.aclweb.org/anthology/P15-1001. \\n[36] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. \\nGirshick, S. Guadarrama, and T. Darrell. Caffe: \\nConvolutional \\narchitecture \\nfor \\nfast \\nfeature \\nembedding. In Proceedings of the ACM International \\nConference on Multimedia, pages 675–678. ACM, \\n2014. arxiv.org/pdf/1408.5093. \\n[37] M. I. Jordan. Serial order: A parallel distributed \\nprocessing approach. ICS report 8608, Institute for \\nCognitive \\nScience, \\nUCSD, \\nLa \\nJolla, \\n1986. \\ncseweb.ucsd.edu/˜gary/PAPERSUGGESTIONS/Jordan-\\nTR-8604.pdf. \\n[38] N. Jouppi. Google supercharges machine learning \\ntasks \\nwith \\nTPU \\ncustom \\nchip, \\n2016. \\ncloudplatform.googleblog.com/2016/05/Googlesupe\\nrcharges-machine-learning-tasks-with-\\ncustomchip.html. \\n[39] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer,´ and \\nY. Wu. Exploring the limits of language modeling. \\nCoRR, \\nabs/1602.02410, \\n2016. \\narxiv.org/abs/1602.02410. \\n[40] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R.',\n",
       " 'Y. Wu. Exploring the limits of language modeling. \\nCoRR, \\nabs/1602.02410, \\n2016. \\narxiv.org/abs/1602.02410. \\n[40] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. \\nSukthankar, and L. Fei-Fei. Large-scale video \\nclassification with convolutional neural networks. In \\nComputer Vision and Pattern Recognition (CVPR), \\n2014 IEEE Conference on, pages 1725–1732. IEEE, \\n2014. research.google.com/pubs/archive/42455.pdf. \\n[41] A. Krizhevsky. One weird trick for parallelizing \\nconvolutional \\nneural \\nnetworks. \\narXiv \\npreprint \\narXiv:1404.5997, 2014. arxiv.org/abs/1404.5997. \\n[42] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet \\nclassification \\nwith \\ndeep \\nconvolutional \\nneural \\nnetworks. In Advances in Neural Information \\nProcessing \\nSystems, \\n2012. \\npapers.nips.cc/paper/4824imagenet-classification-\\nwith-deep-convolutionalneural-networks.pdf.',\n",
       " '18 \\n[43] H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin. \\nExploring strategies for training deep neural networks. \\nJournal of Machine Learning Research, 10:1–40, Jan. \\n2009. \\ndeeplearning.cs.cmu.edu/pdfs/1111/jmlr10 \\nlarochelle.pdf. \\n[44] A. Lavin and S. Gray. Fast algorithms for convolutional \\nneural networks. CoRR, abs/1509.09308, 2015. \\narxiv.org/abs/1509.09308. \\n[45] Q. Le, M. Ranzato, R. Monga, M. Devin, G. Corrado, K. \\nChen, J. Dean, and A. Ng. Building highlevel features \\nusing large scale unsupervised learning. In ICML’2012, \\n2012. Google Research PDF. \\n[46] M. Li, D. G. Andersen, J. Park, A. J. Smola, A. Ahmed, \\nV. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su. Scaling \\ndistributed machine learning with the Parameter \\nServer. In 11th USENIX Symposium on Operating \\nSystems Design and Implementation (OSDI 14), pages \\n583–598, \\n2014. \\nwww.usenix.org/system/files/conference/osdi14/osd\\ni14paper-chilimbi.pdf. \\n[47] M. Li, T. Zhang, Y. Chen, and A. J. Smola.',\n",
       " 'Systems Design and Implementation (OSDI 14), pages \\n583–598, \\n2014. \\nwww.usenix.org/system/files/conference/osdi14/osd\\ni14paper-chilimbi.pdf. \\n[47] M. Li, T. Zhang, Y. Chen, and A. J. Smola. \\nEfficient mini-batch training for stochastic \\noptimization. In Proceedings of the 20th ACM \\nSIGKDD International Conference on Knowledge \\nDiscovery and Data Mining, KDD ’14, pages 661–\\n670, New York, NY, USA, 2014. ACM. \\nwww.cs.cmu.edu/˜muli/file/minibatch sgd.pdf. \\n[48] C. J. Maddison, A. Huang, I. Sutskever, and D. Silver. \\nMove evaluation in Go using deep convolutional \\nneural networks. arXiv preprint arXiv:1412.6564, \\n2014. arxiv.org/abs/1412.6564. \\n[49] F. McSherry, M. Isard, and D. G. Murray. Scalability! \\nBut at what COST? In Proceedings of the 15th USENIX \\nConference on Hot Topics in Operating Systems, \\nHOTOS’15, Berkeley, CA, USA, 2015. USENIX \\nAssociation. \\nwww.usenix.org/system/files/conference/hotos15/ \\nhotos15-paper-mcsherry.pdf.',\n",
       " 'Conference on Hot Topics in Operating Systems, \\nHOTOS’15, Berkeley, CA, USA, 2015. USENIX \\nAssociation. \\nwww.usenix.org/system/files/conference/hotos15/ \\nhotos15-paper-mcsherry.pdf. \\n[50] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient \\nestimation of word representations in vector space. In \\nInternational \\nConference \\non \\nLearning \\nRepresentations: \\nWorkshops \\nTrack, \\n2013. \\narxiv.org/abs/1301.3781. \\n[51] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. \\nVeness, M. G. Bellemare, A. Graves, M. Riedmiller, A. \\nK. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. \\nSadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, \\nS. Legg, and D. Hassabis. Human-level control through \\ndeep reinforcement learning. Nature, 518(7540):529–\\n533, 02 2015. dx.doi.org/10.1038/nature14236. \\n[52] P. Moritz, R. Nishihara, I. Stoica, and M. I. Jordan. \\nSparkNet: Training deep networks in Spark. In \\nInternational \\nConference \\non \\nLearning \\nRepresentations, 2016. arxiv.org/abs/1511.06051.',\n",
       " '[52] P. Moritz, R. Nishihara, I. Stoica, and M. I. Jordan. \\nSparkNet: Training deep networks in Spark. In \\nInternational \\nConference \\non \\nLearning \\nRepresentations, 2016. arxiv.org/abs/1511.06051. \\n[53] Movidius Ltd. Movidius announces Deep Learning \\nAccelerator and Fathom software framework, 2016. \\nwww.movidius.com/news/movidius-announcesdeep-\\nlearning-accelerator-and-fathom-\\nsoftwareframework. \\n[54] D. G. Murray, F. McSherry, R. Isaacs, M. Isard, P. \\nBarham, and M. Abadi. Naiad: a timely dataflow \\nsystem. In Proceedings of the Twenty-Fourth ACM \\nSymposium on Operating Systems Principles, pages \\n439–455. ACM, 2013. Microsoft Research PDF. \\n[55] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. \\nFearon, A. De Maria, V. Panneershelvam, M. \\nSuleyman, C. Beattie, S. Petersen, et al. Massively \\nparallel methods for deep reinforcement learning. \\narXiv preprint arXiv:1507.04296, 2015. \\narxiv.org/abs/1507.04296. \\n[56] Nervana \\nSystems. neon, \\n2016. \\ngithub.com/NervanaSystems/neon.',\n",
       " 'parallel methods for deep reinforcement learning. \\narXiv preprint arXiv:1507.04296, 2015. \\narxiv.org/abs/1507.04296. \\n[56] Nervana \\nSystems. neon, \\n2016. \\ngithub.com/NervanaSystems/neon. \\n[57] NVIDIA Corporation. NCCL: Optimized primitives for \\ncollective \\nmulti-gpu \\ncommunication, \\n2016. \\ngithub.com/NVIDIA/nccl. \\n[58] K. Ovtcharov, O. Ruwase, J.-Y. Kim, J. Fowers, K. \\nStrauss, and E. Chung. Toward accelerating deep \\nlearning at scale using specialized logic. In Hot Chips: \\nA Symposium on High Performance Chips. HOTCHIPS, \\nAugust 2015. re- \\nsearch.microsoft.com/apps/pubs/default.aspx?id=246506. \\n[59] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty \\nof training recurrent neural networks. In ICML (3), \\nvolume 28 of JMLR',\n",
       " '19 \\nProceedings, pages 1310–1318. JMLR.org, 2013. \\nwww.jmlr.org/proceedings/papers/v28/pascanu13.p\\ndf. \\n[60] K. Powell. Nvidia \\ndevtech blog \\npost. \\nblogs.nvidia.com/blog/2015/03/17/digits-devbox/. \\n[61] J. Ragan-Kelley, C. Barnes, A. Adams, S. Paris, F. \\nDurand, and S. Amarasinghe. Halide: A language and \\ncompiler for optimizing parallelism, locality, and \\nrecomputation in image processing pipelines. ACM \\nSIGPLAN \\nNotices, \\n48(6):519– \\n530, \\n2013. \\npeople.csail.mit.edu/fredo/tmp/Halide5min.pdf. \\n[62] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-\\nfree approach to parallelizing stochastic gradient \\ndescent. In Advances in Neural Information Processing \\nSystems, \\npages \\n693–701, \\n2011. \\npapers.nips.cc/paper/4390-hogwild-a-lockfree-\\napproach-to-parallelizing-stochastic-gradientdescent. \\n[63] C. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. \\nFetterly. Dandelion: a compiler and runtime for \\nheterogeneous systems. In Proceedings of the Twenty-',\n",
       " '[63] C. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. \\nFetterly. Dandelion: a compiler and runtime for \\nheterogeneous systems. In Proceedings of the Twenty-\\nFourth ACM Symposium on Operating Systems \\nPrinciples, pages 49–68. ACM, 2013. \\nresearch-\\nsrv.microsoft.com/pubs/201110/sosp13dandelion-\\nfinal.pdf. \\n[64] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. \\nLearning representations by backpropagating errors. \\nCognitive modeling, 5:3, 1988. \\nwww.cs.toronto.edu/ hinton/absps/naturebp.pdf. \\n[65] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, \\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, \\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual \\nRecognition Challenge. International Journal of \\nComputer Vision (IJCV), 115(3):211–252, 2015. \\narxiv.org/abs/1409.0575. \\n[66] A. Smola and S. Narayanamurthy. An architecture for \\nparallel topic models. Proc. \\nVLDB \\nEndow., 3(1-2):703–710, Sept. \\n2010. \\nvldb.org/pvldb/vldb2010/papers/R63.pdf.',\n",
       " 'arxiv.org/abs/1409.0575. \\n[66] A. Smola and S. Narayanamurthy. An architecture for \\nparallel topic models. Proc. \\nVLDB \\nEndow., 3(1-2):703–710, Sept. \\n2010. \\nvldb.org/pvldb/vldb2010/papers/R63.pdf. \\n[67] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. \\nOn the importance of initialization and momentum in \\ndeep learning. In Proceedings of the 30th International \\nConference on Machine Learning (ICML-13), pages \\n1139–1147. \\nJMLR \\nWorkshop \\nand \\nConference \\nProceedings, \\n2013. \\njmlr.org/proceedings/papers/v28/sutskever13.pdf. \\n[68] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to \\nsequence learning with neural networks. In NIPS, \\n2014. \\npapers.nips.cc/paper/5346-sequenceto-\\nsequence-learning-with-neural. \\n[69] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. \\nAnguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. \\nGoing deeper with convolutions. In CVPR’2015, 2015. \\narxiv.org/abs/1409.4842. \\n[70] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z.',\n",
       " 'Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. \\nGoing deeper with convolutions. In CVPR’2015, 2015. \\narxiv.org/abs/1409.4842. \\n[70] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. \\nWojna. Rethinking the inception architecture for \\ncomputer vision. CoRR, abs/1512.00567, 2015. \\narxiv.org/abs/1512.00567. \\n[71] C. tao Chu, S. K. Kim, Y. an Lin, Y. Yu, G. Bradski, K. \\nOlukotun, and A. Y. Ng. Map-reduce for machine \\nlearning on multicore. In B. Scholkopf, J. C.¨ Platt, and \\nT. Hoffman, editors, Advances in Neural Information \\nProcessing Systems 19, pages 281–288. MIT Press, \\n2007. \\npapers.nips.cc/paper/3150-mapreduce-for-\\nmachine-learning-on-multicore.pdf. \\n[72] A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, \\nE. \\nTune, \\nand \\nJ. \\nWilkes. \\nLarge-scale \\ncluster \\nmanagement at Google with Borg. In Proceedings of \\nthe Tenth European Conference on Computer Systems, \\npage \\n18. \\nACM, \\n2015. \\nresearch.google.com/pubs/archive/43438.pdf.',\n",
       " 'Large-scale \\ncluster \\nmanagement at Google with Borg. In Proceedings of \\nthe Tenth European Conference on Computer Systems, \\npage \\n18. \\nACM, \\n2015. \\nresearch.google.com/pubs/archive/43438.pdf. \\n[73] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and \\nG. Hinton. Grammar as a foreign language. Technical \\nreport, \\narXiv:1412.7449, \\n2014. \\narxiv.org/abs/1412.7449. \\n[74] Y. Yu, M. Isard, D. Fetterly, M. Budiu, U. Erlingsson, P. \\nK. Gunda, and J. Currey. DryadLINQ: A system for \\ngeneral-purpose distributed dataparallel computing \\nusing a high-level language. In Proceedings of the 8th \\nUSENIX Conference on Operating Systems Design and \\nImplementation, OSDI’08, pages 1–14, Berkeley, CA, \\nUSA, \\n2008. \\nUSENIX \\nAssociation. \\nwww.usenix.org/legacy/event/osdi08/tech/full \\npapers/yu y/yu y.pdf. \\n[75] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. \\nMcCauley, M. J. Franklin, S. Shenker, and I. Stoica.',\n",
       " '20 \\nResilient \\ndistributed \\ndatasets: \\nA \\nfault-tolerant \\nabstraction for in-memory cluster computing. In \\nProceedings of the 9th USENIX conference on \\nNetworked Systems Design and Implementation. \\nUSENIX Association, 2012. \\nwww.usenix.org/system/files/conference/nsdi12/nsd\\ni12final138.pdf. \\n[76] M. D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, \\nQ. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean, and \\nG. E. Hinton. On rectified linear units for speech \\nprocessing. \\nIn \\nICASSP, \\n2013. \\nresearch.google.com/pubs/archive/40811.pdf. \\n[77] Dhaval Sahija, “Critical review of machine learning \\nintegration with augmented reality fordiscrete \\nmanufacturing” \\n2021, \\n10.2015/IJIRMF.2455.0620/202112017',\n",
       " 'Scikit-Learn Made Easy: API Fast Guide\\nMohamad Yamen AL Mohamad\\nyamenmohamad@tabrizu.ac.ir\\nMay 29, 2025\\nAbstract\\nThis document provides a comprehensive reference guide to the core modules of scikit-learn,\\nthe premier machine learning library for Python. Each section introduces a module, explains\\nits purpose, and details key APIs with itemizes, use cases, and features. Visual diagrams\\nillustrate data flow and relationships between components, while practical code examples\\ndemonstrate real-world applications. The guide covers major functionalities including super-\\nvised learning (classification, regression), unsupervised learning (clustering, dimensionality\\nreduction), model selection, preprocessing, and pipeline construction. It is intended for data\\nscientists, machine learning practitioners, and developers who need a structured overview\\nof scikit-learn’s capabilities. By combining clear explanations with practical examples, this',\n",
       " 'scientists, machine learning practitioners, and developers who need a structured overview\\nof scikit-learn’s capabilities. By combining clear explanations with practical examples, this\\ndocument helps users understand and implement effective machine learning workflows using\\nscikit-learn’s consistent API design. Keywords: Scikit-learn, Machine Learning, Python,\\nClassification, Regression, Clustering, Dimensionality Reduction, Model Selection, Prepro-\\ncessing, Pipelines, Feature Extraction, Cross-Validation, Hyperparameter Tuning, Super-\\nvised Learning, Unsupervised Learning\\n1',\n",
       " '1\\nOverview\\nScikit-learn [1, 2] is a comprehensive Python library for machine learning that provides simple\\nand eﬀicient tools for data mining and data analysis. Built on NumPy, SciPy, and matplotlib, it\\nfeatures various classification, regression, clustering algorithms, and includes utilities for model\\nevaluation, data preprocessing, and pipeline construction.\\nThis document serves as a structured reference to scikit-learn’s core modules:\\n• Clear definitions of each module’s purpose\\n• Detailed explanations of key APIs and functions\\n• Visual diagrams illustrating concepts and workflows\\n• Practical code examples demonstrating real-world usage\\nScikit-learn’s consistent API design enables practitioners to:\\n– Quickly implement and compare different algorithms\\n– Build end-to-end machine learning pipelines\\n– Evaluate models using robust validation techniques\\n– Preprocess data eﬀiciently\\n2\\nSupervised Learning',\n",
       " '– Build end-to-end machine learning pipelines\\n– Evaluate models using robust validation techniques\\n– Preprocess data eﬀiciently\\n2\\nSupervised Learning\\nDefinition 2.1. The sklearn package provides numerous algorithms for supervised learning,\\nwhere the goal is to predict target variables based on input features. These include both classi-\\nfication (predicting discrete labels) and regression (predicting continuous values) tasks.\\nLabeled Data\\nTrain/Test Split\\nClassifier/Regressor\\nEvaluation Metrics\\nFigure 1: Supervised Learning Workflow\\n2',\n",
       " '2.1\\nKey APIs\\n2.1.1\\nClassification\\n- LogisticRegression:\\n• Purpose: Linear model for binary and multiclass classification\\n• Use Case: When probabilities of class membership are needed\\n• Features: L1/L2 regularization, multiclass support\\n- SVC (Support Vector Classification):\\n• Purpose: Kernel-based classification\\n• Use Case: Complex decision boundaries in high-dimensional spaces\\n• Features: Multiple kernel options (linear, poly, rbf, sigmoid)\\n- RandomForestClassifier:\\n• Purpose: Ensemble of decision trees\\n• Use Case: Robust classification with feature importance\\n• Features: Handles missing values, parallel training\\nExample 2.1.\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import\\ntrain_test_split\\nfrom sklearn.metrics import accuracy_score\\n# Load dataset\\niris = load_iris()\\nX, y = iris.data, iris.target\\n# Split into train/test sets\\nX_train , X_test, y_train , y_test = train_test_split\\n(X, y, test_size=0.2)',\n",
       " '# Load dataset\\niris = load_iris()\\nX, y = iris.data, iris.target\\n# Split into train/test sets\\nX_train , X_test, y_train , y_test = train_test_split\\n(X, y, test_size=0.2)\\n# Train model\\nclf = RandomForestClassifier(n_estimators=100)\\nclf.fit(X_train , y_train)\\n# Evaluate\\ny_pred = clf.predict(X_test)\\nprint(f\"Accuracy: {accuracy_score(y_test , y_pred)\\n:.2f}\")\\n2.1.2\\nRegression\\n- LinearRegression:\\n• Purpose: Ordinary least squares regression\\n• Use Case: Linear relationships between features and target\\n• Features: Fast training, interpretable coeﬀicients\\n3',\n",
       " '- Ridge:\\n• Purpose: L2-regularized linear regression\\n• Use Case: When features are correlated\\n• Features: Reduces overfitting through regularization\\n- SVR (Support Vector Regression):\\n• Purpose: Kernel-based regression\\n• Use Case: Non-linear relationships\\n• Features: Robust to outliers with proper kernel choice\\nExample 2.2.\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.datasets import make_regression\\n# Generate synthetic regression data\\nX, y = make_regression(n_features=10, noise=0.1)\\n# Train model\\nridge = Ridge(alpha=1.0)\\nridge.fit(X, y)\\n# Predict and evaluate\\nscore = ridge.score(X, y)\\nprint(f\"R^2 Score: {score:.2f}\")\\n3\\nUnsupervised Learning\\nDefinition 3.1. The sklearn package provides algorithms for unsupervised learning tasks where\\nthe data has no labels. These include clustering (grouping similar data points) and dimension-\\nality reduction (reducing the number of features while preserving structure).\\nUnlabeled Data\\nPreprocessing\\nClustering/Dimensionality\\nReduction',\n",
       " 'ality reduction (reducing the number of features while preserving structure).\\nUnlabeled Data\\nPreprocessing\\nClustering/Dimensionality\\nReduction\\nLabels/Reduced Features\\nFigure 2: Unsupervised Learning Workflow\\n4',\n",
       " '3.1\\nKey APIs\\n3.1.1\\nClustering\\n- KMeans:\\n• Purpose: Partition data into k clusters\\n• Use Case: When number of clusters is known\\n• Features: Scalable, simple interpretation\\n- DBSCAN:\\n• Purpose: Density-based clustering\\n• Use Case: Clusters of arbitrary shape\\n• Features: Handles noise, no need to specify cluster count\\n- AgglomerativeClustering:\\n• Purpose: Hierarchical clustering\\n• Use Case: When hierarchy is important\\n• Features: Dendrogram visualization possible\\nExample 3.1.\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.datasets import make_blobs\\nimport matplotlib.pyplot as plt\\n# Generate synthetic data\\nX, _ = make_blobs(n_samples=300, centers=4,\\nrandom_state=42)\\n# Cluster data\\nkmeans = KMeans(n_clusters=4)\\nkmeans.fit(X)\\nlabels = kmeans.labels_\\n# Visualize\\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\\'\\nviridis\\')\\nplt.title(\"KMeans Clustering\")\\nplt.show()\\n5',\n",
       " '3.1.2\\nDimensionality Reduction\\n- PCA (Principal Component Analysis):\\n• Purpose: Linear dimensionality reduction\\n• Use Case: Feature extraction, visualization\\n• Features: Preserves variance, orthogonal components\\n- t-SNE:\\n• Purpose: Non-linear dimensionality reduction\\n• Use Case: Visualization of high-dimensional data\\n• Features: Preserves local structure\\n- TruncatedSVD:\\n• Purpose: Dimensionality reduction for sparse matrices\\n• Use Case: Text data, recommendation systems\\n• Features: Works with sparse input\\nExample 3.2.\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.datasets import load_digits\\n# Load digit images\\ndigits = load_digits()\\nX = digits.data\\n# Apply PCA\\npca = PCA(n_components=2)\\nX_pca = pca.fit_transform(X)\\n# Plot\\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.\\ntarget, cmap=\\'tab10\\', alpha=0.6)\\nplt.title(\"Digits Projected onto First 2 Principal\\nComponents\")\\nplt.xlabel(\"PC1\")\\nplt.ylabel(\"PC2\")\\nplt.colorbar(label=\"Digit Class\")\\nplt.show()\\n6',\n",
       " '4\\nModel Selection\\nDefinition 4.1. The sklearn.model_selection module provides tools for evaluating models\\nand selecting hyperparameters, including cross-validation strategies and hyperparameter search\\nmethods.\\nData\\nCV Splitter\\nParameter Search\\nBest Model\\nFigure 3: Model Selection Workflow\\n4.1\\nKey APIs\\n- train_test_split:\\n• Purpose: Split data into train and test sets\\n• Use Case: Simple model evaluation\\n• Features: Stratification option for classification\\n- KFold:\\n• Purpose: K-fold cross-validation\\n• Use Case: Robust model evaluation\\n• Features: Shuffle option, stratification\\n- GridSearchCV:\\n• Purpose: Exhaustive hyperparameter search\\n• Use Case: When parameter space is small\\n• Features: Parallel computation, refitting\\n- RandomizedSearchCV:\\n• Purpose: Randomized hyperparameter search\\n• Use Case: Large parameter spaces\\n• Features: More eﬀicient than grid search\\n7',\n",
       " 'Example 4.1.\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import SVC\\nfrom sklearn.datasets import load_iris\\n# Load data\\niris = load_iris()\\nX, y = iris.data, iris.target\\n# Define parameter grid\\nparam_grid = {\\'C\\': [0.1, 1, 10], \\'kernel\\': [\\'linear\\n\\', \\'rbf\\']}\\n# Perform grid search\\ngrid_search = GridSearchCV(SVC(), param_grid , cv=5)\\ngrid_search.fit(X, y)\\n# Output results\\nprint(f\"Best parameters: {grid_search.best_params_}\\n\")\\nprint(f\"Best score: {grid_search.best_score_:.2f}\")\\n8',\n",
       " '5\\nPreprocessing\\nDefinition 5.1. The sklearn.preprocessing module provides tools for transforming raw data\\ninto formats suitable for machine learning, including scaling, normalization, encoding categorical\\nvariables, and feature extraction.\\nRaw Data\\nScaler\\nEncoder\\nProcessed Data\\nFigure 4: Preprocessing Workflow\\n5.1\\nKey APIs\\n- StandardScaler:\\n• Purpose: Standardize features by removing mean and scaling to unit variance\\n• Use Case: When features have different scales\\n• Features: Preserves outliers\\n- MinMaxScaler:\\n• Purpose: Scale features to a given range (default [0, 1])\\n• Use Case: When bounded features are required\\n• Features: Sensitive to outliers\\n- OneHotEncoder:\\n• Purpose: Convert categorical features to binary indicators\\n• Use Case: When categories have no ordinal relationship\\n• Features: Handles unknown categories\\n- LabelEncoder:\\n• Purpose: Encode target labels with value between 0 and n_classes-1\\n• Use Case: Preparing classification targets\\n• Features: Simple transformation\\n9',\n",
       " \"Example 5.1.\\nfrom sklearn.preprocessing import StandardScaler ,\\nOneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\nimport pandas as pd\\n# Create sample data\\ndata = pd.DataFrame({\\n'age': [25, 30, 35],\\n'income': [50000, 60000, 70000],\\n'gender': ['M', 'F', 'M']\\n})\\n# Define preprocessing\\npreprocessor = ColumnTransformer(\\ntransformers=[\\n('num', StandardScaler(), ['age', 'income']),\\n('cat', OneHotEncoder(), ['gender'])\\n])\\n# Apply transformations\\nprocessed = preprocessor.fit_transform(data)\\nprint(processed)\\n10\",\n",
       " \"6\\nPipelines\\nDefinition 6.1. The sklearn.pipeline module provides utilities to chain multiple processing\\nsteps together, ensuring proper data flow and preventing data leakage during cross-validation.\\nRaw Data\\nPreprocessing\\nModel\\nPredictions\\nFigure 5: Pipeline Workflow\\n6.1\\nKey APIs\\n- Pipeline:\\n• Purpose: Chain multiple estimators into one\\n• Use Case: Ensuring proper data flow\\n• Features: Single interface for all steps\\nExample 6.1.\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import\\ntrain_test_split\\nfrom sklearn.datasets import load_breast_cancer\\n# Load data\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train , X_test, y_train , y_test = train_test_split\\n(X, y, test_size=0.2)\\n# Create pipeline\\npipe = Pipeline([\\n('scaler', StandardScaler()),\\n('clf', LogisticRegression())\\n])\\n# Fit and evaluate\\npipe.fit(X_train , y_train)\",\n",
       " '(X, y, test_size=0.2)\\n# Create pipeline\\npipe = Pipeline([\\n(\\'scaler\\', StandardScaler()),\\n(\\'clf\\', LogisticRegression())\\n])\\n# Fit and evaluate\\npipe.fit(X_train , y_train)\\nprint(f\"Test Accuracy: {pipe.score(X_test , y_test)\\n:.2f}\")\\n11',\n",
       " 'References\\n[1] Scikit-learn Development Team. Scikit-learn Documentation. https://scikit-learn.org/\\nstable/, 2025.\\n[2] Scikit-learn\\nContributors.\\nScikit-learn\\nGitHub\\nRepository.\\nhttps://github.com/\\nscikit-learn/scikit-learn, 2025.\\n12',\n",
       " 'seaborn: statistical data visualization\\nMichael L. Waskom1\\n1 Center for Neural Science, New York University\\nDOI: 10.21105/joss.03021\\nSoftware\\n• Review\\n• Repository\\n• Archive\\nEditor: Lorena Pantano\\nReviewers:\\n• @dangeles\\n• @Sara-ShiHo\\nSubmitted: 29 January 2021\\nPublished: 06 April 2021\\nLicense\\nAuthors of papers retain\\ncopyright and release the work\\nunder a Creative Commons\\nAttribution 4.0 International\\nLicense (CC BY 4.0).\\nSummary\\nseaborn is a library for making statistical graphics in Python. It provides a high-level interface\\nto matplotlib and integrates closely with pandas data structures. Functions in the seaborn\\nlibrary expose a declarative, dataset-oriented API that makes it easy to translate questions\\nabout data into graphics that can answer them. When given a dataset and a specification\\nof the plot to make, seaborn automatically maps the data values to visual attributes such\\nas color, size, or style, internally computes statistical transformations, and decorates the plot',\n",
       " 'of the plot to make, seaborn automatically maps the data values to visual attributes such\\nas color, size, or style, internally computes statistical transformations, and decorates the plot\\nwith informative axis labels and a legend. Many seaborn functions can generate figures with\\nmultiple panels that elicit comparisons between conditional subsets of data or across different\\npairings of variables in a dataset. seaborn is designed to be useful throughout the lifecycle of\\na scientific project. By producing complete graphics from a single function call with minimal\\narguments, seaborn facilitates rapid prototyping and exploratory data analysis.\\nAnd by\\noffering extensive options for customization, along with exposing the underlying matplotlib\\nobjects, it can be used to create polished, publication-quality figures.\\nStatement of need\\nData visualization is an indispensable part of the scientific process. Effective visualizations',\n",
       " 'objects, it can be used to create polished, publication-quality figures.\\nStatement of need\\nData visualization is an indispensable part of the scientific process. Effective visualizations\\nwill allow a scientist both to understand their own data and to communicate their insights to\\nothers (Tukey, 1977). These goals can be furthered by tools for specifying a graph that provide\\na good balance between efficiency and flexibility. Within the scientific Python ecosystem, the\\nmatplotlib (Hunter, 2007) project is very well established, having been under continuous\\ndevelopment for nearly two decades. It is highly flexible, offering fine-grained control over the\\nplacement and visual appearance of objects in a plot. It can be used interactively through GUI\\napplications, and it can output graphics to a wide range of static formats. Yet its relatively\\nlow-level API can make some common tasks cumbersome to perform. For example, creating',\n",
       " 'applications, and it can output graphics to a wide range of static formats. Yet its relatively\\nlow-level API can make some common tasks cumbersome to perform. For example, creating\\na scatter plot where the marker size represents a numeric variable and the marker shape\\nrepresents a categorical variable requires one to transform the size values to graphical units\\nand to loop over the categorical levels, separately invoking a plotting function for each marker\\ntype.\\nThe seaborn library offers an interface to matplotlib that permits rapid data exploration\\nand prototyping of visualizations while retaining much of the flexibility and stability that are\\nnecessary to produce publication-quality graphics. It is domain-general and can be used to\\nvisualize a wide range of datasets that are well-represented within a tabular format.\\nExample\\nThe following example demonstrates the creation of a figure with seaborn. The example',\n",
       " 'visualize a wide range of datasets that are well-represented within a tabular format.\\nExample\\nThe following example demonstrates the creation of a figure with seaborn. The example\\nmakes use of one of the built-in datasets that are provided for documentation and generation of\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n1',\n",
       " 'reproducible bug reports. It illustrates several of the features described in the Overview section,\\nincluding the declarative API, semantic mappings, faceting across subplots, aggregation with\\nerror bars, and visual theme control.\\nimport seaborn as sns\\nsns.set_theme(context=\"paper\")\\nfmri = sns.load_dataset(\"fmri\")\\ng = sns.relplot(\\ndata=fmri, kind=\"line\",\\nx=\"timepoint\", y=\"signal\",\\nhue=\"event\", style=\"event\", col=\"region\",\\nheight=3.5, aspect=.8,\\n)\\ng.savefig(\"paper_demo.pdf\")\\n0\\n5\\n10\\n15\\ntimepoint\\n0.1\\n0.0\\n0.1\\n0.2\\n0.3\\nsignal\\nregion = parietal\\n0\\n5\\n10\\n15\\ntimepoint\\nregion = frontal\\nevent\\nstim\\ncue\\nFigure 1: An example seaborn figure demonstrating some of its key features.\\nThe image was\\ngenerated using seaborn v0.11.1.\\nOverview\\nUsers interface with seaborn through a collection of plotting functions that share a common\\nAPI for plot specification and offer many more specific options for customization.\\nThese',\n",
       " 'Overview\\nUsers interface with seaborn through a collection of plotting functions that share a common\\nAPI for plot specification and offer many more specific options for customization.\\nThese\\nfunctions range from basic plot types such as scatter and line plots to functions that apply\\nvarious transformations and abstractions, such as histogram binning, kernel density estimation,\\nand regression model fitting. Functions in seaborn are classified as either “axes-level” or\\n“figure-level.” Axes-level functions behave like most plotting functions in the matplotlib.\\npyplot namespace. By default, they hook into the state machine that tracks a “current”\\nfigure and add a layer to it, but they can also accept a matplotlib axes object to control\\nwhere the plot is drawn, similar to using the matplotlib “object-oriented” interface. Figure-\\nlevel functions create their own figure when invoked, allowing them to “facet” the dataset',\n",
       " 'where the plot is drawn, similar to using the matplotlib “object-oriented” interface. Figure-\\nlevel functions create their own figure when invoked, allowing them to “facet” the dataset\\nby creating multiple conditional subplots, along with adding conveniences such as putting\\nthe legend outside the space of the plot by default. Each figure-level function corresponds\\nto several axes-level functions that serve similar purposes, with a single parameter selecting\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n2',\n",
       " 'the kind of plot to make. For example, the displot function can produce several different\\nrepresentations of a distribution, including a histogram, kernel density estimate, or empirical\\ncumulative distribution function. The figure-level functions make use of a seaborn class that\\ncontrols the layout of the figure, mediating between the axes-level functions and matplotlib.\\nThese classes are part of the public API and can be used directly for advanced applications.\\nOne of the key features in seaborn is that variables in a dataset can be automatically\\n“mapped” to visual attributes of the graph. These transformations are referred to as “se-\\nmantic” mappings because they endow the attributes with meaning vis a vis the dataset. By\\nfreeing the user from manually specifying the transformations – which often requires looping\\nand multiple function invocations when using matplotlib directly – seaborn allows rapid',\n",
       " 'freeing the user from manually specifying the transformations – which often requires looping\\nand multiple function invocations when using matplotlib directly – seaborn allows rapid\\nexploration of multidimensional relationships. To further aid efficiency, the default parameters\\nof the mappings are opinionated. For example, when mapping the color of the elements in a\\nplot, seaborn infers whether to use a qualitative or quantitative mapping based on whether\\nthe input data are categorical or numeric. This behavior can be further configured or even\\noverridden by setting additional parameters of each plotting function.\\nSeveral seaborn functions also apply statistical transformations to the input data before\\nplotting, ranging from estimating the mean or median to fitting a general linear model. When\\ndata are transformed in this way, seaborn automatically computes and shows error bars to\\nprovide a visual cue about the uncertainty of the estimate. Unlike many graphical libraries,',\n",
       " 'data are transformed in this way, seaborn automatically computes and shows error bars to\\nprovide a visual cue about the uncertainty of the estimate. Unlike many graphical libraries,\\nseaborn shows 95% confidence interval error bars by default, rather than standard errors. The\\nconfidence intervals are computed with a bootstrap algorithm, allowing them to generalize over\\nmany different statistics, and the default level allows the user to perform “inference by eye”\\n(Cumming & Finch, 2005). Historically, error bar specification has been relatively limited, but\\na forthcoming release (v0.12) will introduce a new configuration system that makes it possible\\nto show nonparametric percentile intervals and scaled analytic estimates of standard error or\\nstandard deviation statistics.\\nseaborn aims to be flexible about the format of its input data. The most convenient usage\\npattern provides a pandas (McKinney, 2010) dataframe with variables encoded in a long-',\n",
       " 'seaborn aims to be flexible about the format of its input data. The most convenient usage\\npattern provides a pandas (McKinney, 2010) dataframe with variables encoded in a long-\\nform or “tidy” (Wickham, 2014) format. With this format, columns in the dataframe can\\nbe explicitly assigned to roles in the plot, such as specifying the x and y positions of a\\nscatterplot along with size and shape semantics. Long-form data supports efficient exploration\\nand prototyping because variables can be assigned different roles in the plot without modifying\\nanything about the original dataset.\\nBut most seaborn functions can also consume and\\nvisualize “wide-form” data, typically producing similar output to how the analogous matplot\\nlib function would interpret a 2D array (e.g., producing a boxplot where each box represents a\\ncolumn in the dataframe) while making use of the index and column names to label the graph.\\nUsing the label information in a pandas object can help make plots that are interpretable',\n",
       " 'column in the dataframe) while making use of the index and column names to label the graph.\\nUsing the label information in a pandas object can help make plots that are interpretable\\nwithout further tweaking – reducing the chance of interpretive errors – but seaborn also\\naccepts data from a variety of more basic formats, including numpy (Harris et al., 2020) arrays\\nand simple Python collection types.\\nseaborn also offers multiple built-in themes that users can select to modify the visual appear-\\nance of their graphs. The themes make use of the matplotlib rcParams system, meaning\\nthat they will take effect for any figure created using matplotlib, not just those made by\\nseaborn. The themes are defined by two disjoint sets of parameters that separately control\\nthe style of the figure and the scaling of its elements (such as line widths and font sizes). This\\nseparation makes it easy to generate multiple versions of a figure that are scaled for different',\n",
       " 'the style of the figure and the scaling of its elements (such as line widths and font sizes). This\\nseparation makes it easy to generate multiple versions of a figure that are scaled for different\\ncontexts, such as written reports and slide presentations. The theming system can also be\\nused to set a default color palette. As color is particularly important in data visualization and\\nno single set of defaults is universally appropriate, every plotting function makes it easy to\\nchoose an alternate categorical palette or continuous gradient mapping that is well-suited for\\nthe particular dataset and plot type. The seaborn documentation contains a tutorial on the\\nuse of color in data visualization to help users make this important decision.\\nseaborn does not aim to completely encapsulate or replace matplotlib.\\nMany useful\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n3',\n",
       " 'graphs can be created through the seaborn interface, but more advanced applications –\\nsuch as defining composite figures with multiple arbitrary plot types – will require importing\\nand using matplotlib as well.\\nEven when calling only seaborn functions, deeper cus-\\ntomization of the plot appearance is achieved by specifying parameters that are passed-\\nthrough to the underlying matplotlib functions, and tweaks to the default axis limits,\\nticks, and labels are made by calling methods on the matplotlib object that axes-level\\nseaborn functions return.\\nThis approach is distinct from other statistical graphing sys-\\ntems, such as ggplot2 (Wickham, 2016).\\nWhile seaborn offers some similar features\\nand, in some cases, uses similar terminology to ggplot2, it does not implement the for-\\nmal Grammar of Graphics and cannot be used to produce arbitrary visualizations. Rather, its\\naim is to facilitate rapid exploration and prototyping through named functions and opinion-',\n",
       " 'mal Grammar of Graphics and cannot be used to produce arbitrary visualizations. Rather, its\\naim is to facilitate rapid exploration and prototyping through named functions and opinion-\\nated defaults while allowing the user to leverage the considerable flexibility of matplotlib\\nto create more domain-specific graphics and to polish figures for publication.\\nAn exam-\\nple of a successful use of this approach to produce reproducible figures can be found at\\nhttps://github.com/WagnerLabPapers/Waskom_PNAS_2017 (Waskom & Wagner, 2017).\\nAcknowledgements\\nM.L.W. has been supported by the National Science Foundation IGERT program (0801700)\\nand by the Simons Foundation as a Junior Fellow in the Simons Society of Fellows (527794).\\nMany others have helped improve seaborn by asking questions, reporting bugs, and con-\\ntributing code; thank you to this community.\\nReferences\\nCumming, G., & Finch, S. (2005). Inference by eye: confidence intervals and how to read',\n",
       " 'tributing code; thank you to this community.\\nReferences\\nCumming, G., & Finch, S. (2005). Inference by eye: confidence intervals and how to read\\npictures of data. The American Psychologist, 60(2), 170–180. https://doi.org/10.1037/\\n0003-066X.60.2.170\\nHarris, C. R., Millman, K. J., Walt, S. J. van der, Gommers, R., Virtanen, P., Cournapeau,\\nD., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., Kerkwijk,\\nM. H. van, Brett, M., Haldane, A., R’ıo, J. F. del, Wiebe, M., Peterson, P., … Oliphant,\\nT. E. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https:\\n//doi.org/10.1038/s41586-020-2649-2\\nHunter, J. D. (2007). Matplotlib: A 2D graphics environment. Computing in Science &\\nEngineering, 9(3), 90–95. https://doi.org/10.1109/MCSE.2007.55\\nMcKinney, W. (2010). Data structures for statistical computing in python. In S. van der Walt\\n& J. Millman (Eds.), Proceedings of the 9th Python in Science Conference (pp. 51–56).',\n",
       " 'McKinney, W. (2010). Data structures for statistical computing in python. In S. van der Walt\\n& J. Millman (Eds.), Proceedings of the 9th Python in Science Conference (pp. 51–56).\\nhttps://doi.org/10.25080/Majora-92bf1922-00a\\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley. ISBN: 978-0201076165\\nWaskom, M. L., & Wagner, A. D. (2017). Distributed representation of context by intrinsic\\nsubnetworks in prefrontal cortex. Proceedings of the National Academy of Sciences, 2030–\\n2035. https://doi.org/10.1073/pnas.1615269114\\nWickham, H. (2014). Tidy data.\\nJournal of Statistical Software, Articles, 59(10), 1–23.\\nhttps://doi.org/10.18637/jss.v059.i10\\nWickham, H. (2016).\\nggplot2:\\nElegant graphics for data analysis.\\nSpringer-Verlag.\\nISBN: 978-3-319-24277-4\\nWaskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021. https://doi.org/10.21105/joss.\\n03021\\n4',\n",
       " 'Jian Tao\\njtao@tamu.edu\\nSpring 2020 HPRC Short Course  \\n03/27/2020\\nIntroduction to Deep Learning \\nwith TensorFlow',\n",
       " 'Schedule\\n●Part I. Deep Learning (70 mins)\\n●Break (10 mins)\\n●Part II. Intro to TensorFlow (70 mins)',\n",
       " 'GitHub Repository for the Webinars\\nhttps://github.com/jtao/dswebinar',\n",
       " 'Jupyter Notebook and JupyterLab\\nJupyter Notebook\\nJupyterLab',\n",
       " 'Google Colaboratory',\n",
       " 'Google Colaboratory\\nSearch GitHub user: jtao/dswebinar',\n",
       " 'Part I. Deep Learning\\nDeep Learning\\nby Ian Goodfellow, Yoshua Bengio, and Aaron Courville\\nhttp://www.deeplearningbook.org/\\nAnimation of Neutron Networks\\nby Grant Sanderson\\nhttps://www.3blue1brown.com/',\n",
       " 'Relationship of AI, ML, and DL\\nArtificial Intelligence\\n \\nMachine Learning\\n \\nDeep Learning\\n●Artificial Intelligence (AI)  \\nis anything about \\nman-made intelligence \\nexhibited by machines.\\n●Machine Learning (ML) is \\nan approach to achieve AI.\\n●Deep Learning (DL) is one \\ntechnique to implement \\nML.',\n",
       " 'Machine Learning\\nTraditional Modeling\\nMachine Learning (Supervised Learning)\\nSample \\nData\\nExpected \\nOutput\\nComputer\\nModel\\nData\\nScientific \\nModel\\nComputer\\nPrediction\\nModel\\nData\\nComputer\\nPrediction',\n",
       " 'Types of ML Algorithms\\n●\\nSupervised Learning\\n○\\ntrained with labeled data; \\nincluding regression and \\nclassification problems\\n●\\nUnsupervised Learning\\n○\\ntrained with unlabeled data; \\nclustering and association rule \\nlearning problems.\\n●\\nReinforcement Learning\\n○\\nno training data; stochastic \\nMarkov decision process; robotics \\nand self-driving cars.\\nSupervised Learning\\nReinforcement Learning\\nUnsupervised Learning\\nMachine Learning',\n",
       " 'Supervised Learning\\nWhen both input variables - X and output variables - Y are known, one can \\napproximate the mapping function from  X to Y.\\nTraining Data\\nML Algorithm\\nModel\\nTest Data\\nStep 1: Training\\nStep 2: Testing',\n",
       " 'Unsupervised Learning\\nWhen only input variables - X are known and the training data is neither \\nclassified nor labeled. It is usually used for clustering problems.\\nData\\nClass 1\\nClass 2\\nClass 3',\n",
       " 'Reinforcement Learning\\nWhen the input variables are only available via interacting with the \\nenvironment, reinforcement learning can be used to train an \"agent\".\\n(Image Credit: Wikipedia.org)\\n(Image Credit: deeplearning4j.org)',\n",
       " 'Why Deep Learning?\\n●Limitations of traditional machine learning algorithms\\n○not good at handling high dimensional data.\\n○difficult to do feature extraction and object recognition.\\n●Advantages of deep learning\\n○DL is computationally expensive, but it is capable of \\nhandling high dimensional data.\\n○feature extraction is done automatically.',\n",
       " 'What is Deep Learning?\\nDeep learning is a class of machine learning algorithms that:\\n●use a cascade of multiple layers of nonlinear processing units \\nfor feature extraction and transformation. Each successive \\nlayer uses the output from the previous layer as input.\\n●learn in supervised (e.g., classification) and/or unsupervised \\n(e.g., pattern analysis) manners.\\n●learn multiple levels of representations that correspond to \\ndifferent levels of abstraction; the levels form a hierarchy of \\nconcepts.\\n(Source: Wikipedia)',\n",
       " 'Artificial Neural Network\\n(Image Credit: Wikipedia)\\nInput\\nOutput\\nHidden Layers',\n",
       " 'Inputs and Outputs \\n256 X 256 \\nMatrix\\n4-Element Vector \\nDL model\\n1\\n2\\n3\\n4\\n5\\n6\\nA\\nC\\nT\\nG\\nM\\nF\\nWith deep learning, we are searching for a surjective \\n(or onto) function f from a set X to a set Y. \\nX\\nY',\n",
       " '18 \\nLearning Principle\\nx\\n1\\nx\\n2\\nx\\nn\\n…..\\n-\\nError:\\nOutput/Prediction\\nTarget Output\\nDataset\\n= 5',\n",
       " '19 \\nLearning Principle\\nx\\n1\\nx\\n2\\nx\\nn\\n…..\\n-\\nError:\\nOutput/Prediction\\nTarget Output\\n= 15',\n",
       " '20 \\nLearning Principle\\nx\\n1\\nx\\n2\\nx\\nn\\n…..\\n-\\nError:\\nOutput/Prediction\\nTarget Output\\n= 2.5',\n",
       " 'Supervised Deep Learning with Neural Networks\\nX3\\nX2\\nX1\\nY3\\nInput\\nOutput\\nHidden Layers\\nW1\\nW2\\nW3\\nFrom one layer to the next\\nf is the activation function,\\nWi is the weight, and bi is \\nthe bias.',\n",
       " 'Training - Minimizing the Loss \\nX3\\nX2\\nX1\\nY2\\nInput\\nOutput\\nW3, b3\\nThe loss function with regard to weights \\nand biases can be defined as\\nW2, b2\\nW1, b1\\nL\\nThe weight update is computed by moving \\na step to the opposite direction of the cost \\ngradient. \\nIterate until L stops decreasing.',\n",
       " 'Convolution in 2D\\n(Image Credit: Applied Deep Learning | Arden Dertat)',\n",
       " 'Convolution Kernel \\n(Image Credit: Applied Deep Learning | Arden Dertat)',\n",
       " 'Convolution on Image\\nImage Credit: Deep Learning Methods for Vision | CVPR 2012 Tutorial',\n",
       " 'Activation Functions\\nImage Credit: towardsdatascience.com',\n",
       " 'Introducing Non Linearity (ReLU)\\nImage Credit: Deep Learning Methods for Vision | CVPR 2012 Tutorial',\n",
       " 'Max Pooling \\n(Image Credit: Applied Deep Learning | Arden Dertat)',\n",
       " 'Pooling - Max-Pooling and Sum-Pooling\\nImage Credit: Deep Learning Methods for Vision | CVPR 2012 Tutorial',\n",
       " 'CNN Implementation - Drop Out\\n(Image Credit: Applied Deep Learning | Arden Dertat)\\nDropout is used to prevent overfitting. A neuron is temporarily \\n“dropped” or disabled with probability P during training.',\n",
       " 'CNN Implementation - Data Augmentation (DA)\\n(Image Credit: Applied Deep Learning | Arden Dertat)\\nDA helps to popular  \\nartificial training \\ninstances from the \\nexisting train data sets.',\n",
       " 'Convolutional Neural Networks\\nA convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward \\nartificial neural networks that explicitly assumes that the inputs are images, which allows \\nus to encode certain properties into the architecture.\\n(Image Credit: https://becominghuman.ai)',\n",
       " 'Deep Learning for Facial Recognition \\n(Image Credit: www.edureka.co)',\n",
       " 'MNIST - Introduction\\n●\\nMNIST (Mixed National \\nInstitute of Standards and \\nTechnology) is a database for \\nhandwritten digits, distributed \\nby Yann Lecun.\\n●\\n60,000 examples, and a test \\nset of 10,000 examples.\\n●\\n28x28 pixels each.\\n●\\nWidely used for research and \\neducational purposes.\\n(Image Credit: Wikipedia)',\n",
       " 'MNIST - CNN Visualization\\n(Image Credit: http://scs.ryerson.ca/~aharley/vis/)',\n",
       " 'Part II. Introduction to TensorFlow\\n36\\nTensorFlow Official Website\\nhttp://www.tensorflow.org',\n",
       " 'A Brief History of TensorFlow\\nTensorFlow is an end-to-end FOSS (free and open source software) \\nlibrary for dataflow, differentiable programming. TensorFlow is one of \\nthe most popular program frameworks for building machine learning \\napplications.\\n●\\nGoogle Brain built DistBelief in 2011 for internal usage.\\n●\\nTensorFlow 1.0.0 was released on Feb 11, 2017\\n●\\nTensorFlow 2.0 was released in Jan 2018.',\n",
       " 'TensorFlow, Keras, and PyTorch\\nKeras is a high-level \\nneural networks API, \\nwritten in Python and \\ncapable of running on \\ntop of TensorFlow, \\nCNTK, or Theano. It \\nwas developed with a \\nfocus on enabling fast \\nexperimentation.\\nTensorFlow is an \\nend-to-end open \\nsource platform for \\nmachine learning. It \\nhas a comprehensive, \\nflexible ecosystem to \\nbuild and deploy ML \\npowered applications.\\nPyTorch is an open \\nsource machine \\nlearning framework \\nthat accelerates the \\npath from research \\nprototyping to \\nproduction \\ndeployment.',\n",
       " 'Google Trends for Popular ML Frameworks\\n(Image Credit: https://trends.google.com/)',\n",
       " 'Programming Environment\\n(Image Credit: tensorflow.org)\\nIn TF 2.0, tf.keras is the \\nrecommended \\nhigh-level API.',\n",
       " '(Image Credit: Plumber Game by Mobiloids)\\nA Connected Pipeline for the Flow of Tensors',\n",
       " 'What is a Tensor in TensorFlow?\\n●\\nTensorFlow uses a tensor \\ndata structure to represent all \\ndata. A TensorFlow tensor as \\nan n-dimensional array or list. \\nA tensor has a static type, a \\nrank, and a shape.\\nName\\nRank\\nTensor\\nScalar\\n0\\n[5]\\nVector\\n1\\n[1 2 3]\\nMatrix\\n2\\n[[1 2 3 4],\\n[5 6 7 8]]\\nTensor\\n3\\n...',\n",
       " 'TensorFlow Data Types \\nBasic TensorFlow data types include:\\n●\\nint[8|16|32|64], float[16|32|64], double\\n●\\nbool \\n●\\nstring\\nwith tf.cast(), the data types of variables \\ncould be converted.',\n",
       " 'Hello World with TensorFlow\\nimport tensorflow as tf\\nv = tf.constant(\"Hello World!\")\\ntf.print(v)',\n",
       " 'TensorFlow Constants\\nimport tensorflow as tf\\nx = tf.constant(1, tf.int32)\\nzeros = tf.zeros([2, 3], tf.int32)\\nones = tf.ones([2, 3], tf.int32)\\ny = x *(zeros + ones + ones)\\ntf.print(y)\\nTensorFlow provides several operations to generate constant tensor.',\n",
       " 'TensorFlow Variables\\nTensorFlow variables can represent shared, persistent state manipulated by \\nyour program. Weights and biases are usually stored in variables.\\nimport tensorflow as tf\\nW = tf.Variable(tf.random.normal([2,2], stddev=0.1), \\nname = \"W\")\\nb = tf.Variable(tf.zeros(shape=(2)), name=\"b\")',\n",
       " 'Machine Learning Workflow with tf.keras\\nStep 1\\nPrepare Train Data\\nThe preprocessed data set needs \\nto be shuﬄed and splitted into \\ntraining and testing data.\\n  \\nStep 2\\nDeﬁne Model\\nA model could be deﬁned with \\ntf.keras Sequential model for a \\nlinear stack of layers or tf.keras \\nfunctional API for complex \\nnetwork.\\n  \\nStep 3\\nTraining Conﬁguration\\nThe conﬁguration of the training \\nprocess requires the \\nspeciﬁcation of an optimizer, a \\nloss function, and a list of \\nmetrics.\\n  \\nStep 4\\nTrain Model\\nThe training begins by calling the \\nﬁt function. The number of \\nepochs and batch size need to be \\nset. The measurement metrics \\nneed to be evaluated.',\n",
       " 'tf.keras Built-in Datasets\\n●\\ntf.keras provides many popular reference datasets that could be used \\nfor demonstrating and testing deep neural network models. To name a \\nfew,\\n○\\nBoston Housing (regression)\\n○\\nCIFAR100 (classification of 100 image labels)\\n○\\nMNIST (classification of 10 digits)\\n○\\nFashion-MNIST (classification of 10 fashion categories)\\n○\\nReuters News (multiclass text classification)\\n●\\nThe built-in datasets could be easily read in for training purpose. E.g.,\\nfrom tensorflow.keras.datasets import boston_housing\\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()',\n",
       " 'Prepare Datasets for tf.keras\\nIn order to train a deep neural network model with \\nKeras, the input data sets needs to be cleaned, \\nbalanced, transformed, scaled, and splitted.\\n●\\nBalance the classes. Unbalanced classes will \\ninterfere with training.\\n●\\nTransform the categorical variables into \\none-hot encoded variables. \\n●\\nExtract the X (variables) and y (targets) values \\nfor the training and testing datasets.\\n●\\nScale/normalize the variables.\\n●\\nShuffle and split the dataset into training and \\ntesting datasets\\nDog\\nCat\\nHorse\\n1\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\nDog\\nCat\\nHorse\\n1\\n2\\n3\\nOne-hot encoding\\nNumerical encoding',\n",
       " \"Create a tf.keras Model\\n●\\nLayers are the fundamental \\nbuilding blocks of tf.keras \\nmodels. \\n●\\nThe Sequential model is a \\nlinear stack of layers.\\n●\\nA Sequential model can be \\ncreated with a list of layer \\ninstances to the constructor or \\nadded with the .add() method.\\n●\\nThe input shape/dimension of \\nthe first layer need to be set.\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, \\nActivation\\nmodel = Sequential([\\n    Dense(64, activation='relu', input_dim=20),\\n    Dense(10, activation='softmax')\\n])\\nInput\\nOutput\\nHidden Layers\",\n",
       " 'Compile a tf.keras Model\\nThe compile method of a Keras model configures the learning \\nprocess before the model is trained. The following 3 arguments need \\nto be set (the optimizer and loss function are required).\\n●\\nAn optimizer: Adam, AdaGrad, SGD, RMSprop, etc.\\n●\\nA loss function: mean_squared_error, mean_absolute_error, \\nmean_squared_logarithmic_error, categorical_crossentropy, \\nkullback_leibler_divergence, etc.\\n●\\nA list of measurement metrics: accuracy, binary_accuracy, \\ncategorical_accuracy, etc.',\n",
       " 'Train and Evaluate a tf.keras Model\\nModel: \"sequential_1\"\\n_______________________________________________\\nLayer (type)                 Output Shape              Param #   \\n=============================================\\ndense_11 (Dense)             (None, 64)                1344      \\n_______________________________________________\\ndense_12 (Dense)             (None, 10)                650       \\n=============================================\\nTotal params: 1,994\\nTrainable params: 1,994\\nNon-trainable params: 0\\n_______________________________________________\\nNone\\ntf.keras is trained on NumPy arrays of input \\ndata and labels. The training is done with the \\n●\\nfit() function of the model class. In the fit \\nfunction, the following two \\nhyperparameters can be set:\\n○\\nnumber of epochs\\n○\\nbatch size\\n●\\nevaluate() function returns the loss value \\n& metrics values for the model in test \\nmode.\\n●\\nsummary() function prints out the \\nnetwork architecture.',\n",
       " 'Make Predictions and More\\nAfter the model is trained, \\n●\\npredict() function of the model class could be used to \\ngenerate output predictions for the input samples.\\n●\\nget_weights() function returns a list of all weight tensors in \\nthe model, as Numpy arrays.\\n●\\nto_json() returns a representation of the model as a JSON \\nstring. Note that the representation does not include the \\nweights, only the architecture. \\n●\\nsave_weights(filepath) saves the weights of the model as a \\nHDF5 file.',\n",
       " 'Hands-on Session #1\\nGetting Started with TensorFlow',\n",
       " 'Hands-on Session #2\\nClassify Handwritten Digits with \\nTensorFlow',\n",
       " 'Natural Language to Python Source Code using \\nTransformers  \\n \\nMeet Shah \\nInformation Technology \\nSardar Patel Institute of Technology \\nMumbai, India \\nmeet8june@gmail.com \\n \\nRajat Shenoy \\nInformation Technology \\nSardar Patel Institute of Technology \\nMumbai, India \\nrajatshenoy@gmail.com \\n \\nRadha Shankarmani \\nInformation Technology \\nSardar Patel Institute of Technology \\nMumbai, India \\nradha_shankarmani@spit.ac.in\\nAbstract—Writing code using natural language is a very exciting \\napplication of Neural Machine Translation. To achieve a small \\npart of such an application, in this paper, we try to generate python \\nsource code snippets from natural English language descriptions \\nusing the Django dataset. We trained the self-attention based \\ntransformer architecture on the snippets from the dataset. We \\nachieved a BLEU score of 64.29 \\nKeywords—Transformers, tokenizers, Django, English, Python \\n \\nI. INTRODUCTION \\nProgramming is a vast field of engineering. More and more',\n",
       " 'achieved a BLEU score of 64.29 \\nKeywords—Transformers, tokenizers, Django, English, Python \\n \\nI. INTRODUCTION \\nProgramming is a vast field of engineering. More and more \\npeople are joining this profession and the skill is in high \\ndemand. But most of the programmers face this problem that \\nthe field is too fast and there is too much adaptation required \\nif we want to stay in this field for a long time. Writing source \\ncode in different languages is very tough for programmers. \\nThey need to first refer to the documentation of those \\nprogramming languages, understand the syntax and then \\nwrite the source code for their program. \\nWe strongly believe that programming should be based on \\nthe application of logic and problem-solving skills more as \\ncompared to having the knowledge of a particular \\nprogramming language. That is why we propose this solution \\nwhere we translate natural language (English) to a \\nprogramming \\nlanguage \\n(Python). \\nThis \\nwill \\nallow',\n",
       " 'programming language. That is why we propose this solution \\nwhere we translate natural language (English) to a \\nprogramming \\nlanguage \\n(Python). \\nThis \\nwill \\nallow \\nprogrammers to write programs in natural language and not \\nworry about the syntax in various languages and the silly \\nsyntax errors that keep on occurring all the time.  \\nSemantic parsing is the problem of converting natural \\nlanguage constructs into logical constructs. One of the \\napplications of semantic parsing is machine translation. \\nMachine Translation uses software to convert one language \\nto another language. We are working on a small part of this \\nproblem by converting natural English language constructs \\ninto python programming language syntax. We will be using \\nthe Django dataset for this task. \\nIn the future, maybe when this proposed system is developed \\nenough, even non-programmers will be able to write \\nprograms with the help of just natural language if they',\n",
       " 'In the future, maybe when this proposed system is developed \\nenough, even non-programmers will be able to write \\nprograms with the help of just natural language if they \\nunderstand the logic to solve the problem. This paper is in the \\nprimitive stages of the research topic and is subject to further \\nimprovement and research. \\n \\nII. LITERATURE REVIEW \\nMany papers have been published that follows a deep \\nlearning approach for natural language to source code \\nconversion. [1] and [2] use attention-based encoder-decoder \\narchitectures. They use one or more than one LSTM layers \\nwith appropriate attention mechanisms. \\nEncoder/decoder based transformers introduced in [3] are \\nnow replacing LSTM based architectures. Transformer based \\narchitectures \\nare \\nnow \\noutperforming \\nLSTM \\nbased \\narchitectures. State-of-the-art performance has been achieved \\nin machine translation tasks[4], language modelling tasks, \\nclassification tasks [5],  grammar correction tasks [6], \\nsummarization \\ntasks[7],',\n",
       " 'in machine translation tasks[4], language modelling tasks, \\nclassification tasks [5],  grammar correction tasks [6], \\nsummarization \\ntasks[7], \\nentity \\nrecognition, \\ndialogue \\ngeneration tasks [8], and other NLP tasks as well using the \\nTransformer architecture. \\n \\nProgramming languages are of course not natural but many \\nNLP approaches are being applied to them. The programming \\nlanguages have grammar and syntax but with relatively lesser \\nvocabulary than natural languages. There also are \\nrelationships between the vocabulary. Therefore there are \\nmany opportunities to model these languages using NLP \\nmodels and tasks.  \\n \\nSuch NLP tasks applied to programming languages definitely \\nhave some uses in software development. Examples of which \\ninclude summarization/ translation of source code to generate \\ndocumentation or summaries[9][10], using natural language \\nquery for code search [11][12], patching  and detecting bugs \\nusing translation and error correction[13], code completion',\n",
       " \"documentation or summaries[9][10], using natural language \\nquery for code search [11][12], patching  and detecting bugs \\nusing translation and error correction[13], code completion \\nusing language modelling or generation[14][15] \\n \\nWe will be using the Django Dataset [16] in this paper. The \\nDjango Dataset provides a dataset for annotated code \\nsnippets. For example, the dataset will contain the natural \\nlanguage intent ‘call the params.get method with string ' \\nKEY_PREFIX ' and an empty string as arguments, substitute \\nthe result for self._key_prefix.’ mapped to the python code \\n‘self . key_prefix = params . get ( 'KEY_PREFIX' , '' )’. The \\nDjango dataset contains 18805 such pairs of natural language \\nintents mapped to python code. We explore how \\nencoder/decoder based transformer architecture performs on \\nthese datasets.\",\n",
       " 'III. METHODOLOGY \\n \\nThe main methodology behind translating natural language to \\ncode is machine translation. The basic idea is that a \\nprogramming language is just like a normal natural language \\nwith much less vocabulary. \\n \\nThere exist mature solutions to translate one natural language \\nto another. Some products like Google Translate do it really \\nwell. If programming languages are treated like natural \\nlanguages and the machine translation solutions are applied \\nto translate from natural language to programming language, \\nwe will reach a good solution at some point in the future. \\n \\nSo for machine translation, the solutions are recurrent neural \\nnetworks (LSTMs) and Transformers. Most of the literature \\nwe reviewed used RNNs as a solution. But transformers are \\nfaster and more robust as compared to LSTMs and hence we \\nchose to use transformers as a solution. \\n \\nThe transformers cannot process English or python directly.',\n",
       " 'faster and more robust as compared to LSTMs and hence we \\nchose to use transformers as a solution. \\n \\nThe transformers cannot process English or python directly. \\nThey need to be converted to numerical form to be able to use \\nthem. That is where tokenizers come in handy and we have \\nbuilt our custom tokenizer based on BertTokenizer and we \\nhave built our custom transformer which inherits from the \\nTensorFlow model. The vocabulary generated from the \\ntokenizer is used to train the transformer. \\n \\nIV. IMPLEMENTATION \\n \\nA. User Interface \\n \\nThe implementation consists of 3 parts. The first one is the \\nuser interface. It is built in React. React is a pretty popular \\njavascript library used to build AJAX based frontend of a web \\napp. The frontend consists of 3 user input types. The user can \\ngive the input via a file in which they can type in multiple \\nlines of natural language intents. They can also type single \\nline text intents. They can give audio inputs by speaking in',\n",
       " 'give the input via a file in which they can type in multiple \\nlines of natural language intents. They can also type single \\nline text intents. They can give audio inputs by speaking in \\nEnglish .They will get a corresponding python snippet as an \\noutput. We have also included a Code-editor for the user, \\nwhere a person can edit the predicted code as well and send \\nthe corrected code to us that we can see in Fig. 1 \\nFig. 1 – UI of our implementation \\nB. Server \\n \\nThe second part is the server side part. This part is made in \\nDjango. Django is a versatile server side python framework \\nthat has many tools for the convenience of web developers. \\nDjango consists of the basic routing of the web app and that \\nis where the saved ML model is used to translate the English \\ninput received from the client side to python and send it back \\nto the client. \\n \\nC. Model \\n \\n1) The Dataset \\n \\nThe third and the main part of the project is the model itself',\n",
       " 'input received from the client side to python and send it back \\nto the client. \\n \\nC. Model \\n \\n1) The Dataset \\n \\nThe third and the main part of the project is the model itself \\nthat does all the work behind the scenes. The dataset used is \\nthe ase15-django-dataset [16]. This dataset consists of 18805 \\nrecords of English and corresponding python snippets. The \\ndata is distributed across code snippets of various libraries \\nlike Django, celery, JSON, etc. show in Fig. 2 \\n \\nFig. 2 – The Django Dataset [16] \\n \\n2)  Pre-processing \\n \\nThis dataset is split into train and test with train dataset \\nconsisting of 15000 records and test dataset consisting of the \\nremaining 3805 records. Some preprocessing is done on the \\ndata first and then it is converted into the TensorFlow dataset \\nformat for better processing with the TensorFlow library. \\nTensorflow is an open-source library in python by Google \\nand it is used for ML and DL operations. It has a vast number',\n",
       " 'format for better processing with the TensorFlow library. \\nTensorflow is an open-source library in python by Google \\nand it is used for ML and DL operations. It has a vast number \\nof libraries built into it and is popular due to its versatility. \\nThe model training is done in a Google Colab GPU \\nenvironment. The TensorFlow version was 2.4.1.  \\n \\n3)    Tokenizer \\n \\nThen the bert_vocab_from_dataset library was loaded from \\ntensorflow_text for the purpose of vocabulary generation \\nfrom English and python. The vocabulary size for the bert \\nvocab was set to 4000. The vocabulary for python and \\nEnglish was created and stored in a text file. Two Bert \\ntokenizer models were trained using these txt files. One \\ncorresponds to English and the other corresponds to python.  \\n \\nA tokenizer is a tool that converts string input from the \\nvocabulary into a numerical format. Tokenization is \\nperformed on the input before feeding it to the transformer',\n",
       " 'A tokenizer is a tool that converts string input from the \\nvocabulary into a numerical format. Tokenization is \\nperformed on the input before feeding it to the transformer \\nmodel so that the model can understand the input. Also, the \\nnumerical output received from the transformer model is \\nconverted to a string or a human-readable format by',\n",
       " 'detokenizing using the tokenizer. Then a custom tokenizer is \\nbuilt and saved using TensorFlow’s save model function.  \\n \\n4)   Transformer \\nFig. 3: Transformer Architecture [3] \\n \\nThe tokenized pairs of English and python are divided into \\nbatches with a batch size of 64 and buffer size of 20000. Then \\npositional encoding is done. RNNs inherently know the \\nposition of a word in a sentence because it is a sequential \\nmodel. A single word input is accepted sequentially by RNN. \\nWhereas in transformers, the model itself does not have any \\nidea about the position of the word in the sentence because of \\nthe inherent nature of taking and processing all the words \\nsimultaneously. Hence, we need to add an extra piece of \\ninformation along with the word which contains details about \\nthe position of the word. This extra piece of information is \\nknown as positional encoding. Then we create a custom \\ntransformer model. \\n \\nThis transformer [3] consists of an encoder, a decoder and a',\n",
       " 'known as positional encoding. Then we create a custom \\ntransformer model. \\n \\nThis transformer [3] consists of an encoder, a decoder and a \\nfinal linear layer. The encoder first performs input \\nembedding, then does positional encoding and then sends the \\ndata through multiple encoding layers. An encoding layer \\nconsists of a multi-head attention sublayer and a pointwise \\nfeed-forward network sublayer. The multi-head attention \\noperation splits a linear layer into heads, then performs scaled \\ndot-product attention, and then concatenates the heads into a \\nfinal linear layer. The decoder performs an output \\nembedding, then it performs positional encoding and then the \\ndata goes through multiple decoder layers. A decoder layer \\nfirst performs masked multi-head attention, then performs \\nmulti-head attention with padding mask and then the data \\ngoes through  pointwise feed-forward networks. We can see \\nthis architecture in Fig. 3. \\n \\nWe then set our hyper parameters. We have used the Adam',\n",
       " 'goes through  pointwise feed-forward networks. We can see \\nthis architecture in Fig. 3. \\n \\nWe then set our hyper parameters. We have used the Adam \\nOptimizer with a customized learning rate scheduler in \\naccordance with the formula described in [3].  \\n \\nFig.4: Learning Rate Schedule \\n \\nWe then defined our loss function and our accuracy metrics. \\nWe used checkpointing while training our model. We had got \\naccess to a single Testa T4 GPU for training. We then train \\nthe model, running 50 epochs on the training dataset. After \\nthis, we evaluate the model, save the weights to use them in \\nthe server. \\n \\nV. RESULT \\nFig.5: Accuracy and Loss Metrics \\n \\nWe trained the transformer on 15000 pairs of intents and \\nsnippets. We obtained accuracy of 0.973 and a loss of about \\n0.0867 on the 50th epoch with each epoch taking on average  \\n \\nWe also calculated the BLEU Score of our model. To \\nevaluate the quality of machine translated natural language \\nfrom another language BLEU (bilingual evaluation',\n",
       " 'We also calculated the BLEU Score of our model. To \\nevaluate the quality of machine translated natural language \\nfrom another language BLEU (bilingual evaluation \\nunderstudy) is used. It used to determine how close is the \\nmachine translated text compared to the professional human \\ntranslation. It is one of the most popular and inexpensive \\nmetrics. We have shown some of our BLEU scores and our \\naverage BLEU score in Fig. 6',\n",
       " 'Fig.6: Our Average BLEU Score \\n \\nWe also tried giving new test cases to the model, some were \\nsuccessfully translated, some were not. The model struggles \\nwith variable names but fairly accurately predicts the syntax \\nof Python programming language. We can see in figure 7 \\nthat the model gave the correct prediction for the English \\nintent. But in figure 8 we can see that the model understood \\nthe syntax of classes but struggles with the variables a bit. \\n \\nFig.7: Correct prediction \\n \\nFig.8: Incorrect prediction \\nVI. CONCLUSION \\n \\nIn this paper, we presented a way to translate natural language \\n(English) to a programming language (python). We aimed to \\nsolve the “programming language barrier” in programming. \\nWe used the transformer model on the Django Dataset and \\nachieved a BLEU Score of 64.29. Our model fairly \\nunderstands the syntax of the Python Programming Language \\nbut struggles with variable names. \\n \\nThis topic is in primitive stages of research and needs more',\n",
       " 'understands the syntax of the Python Programming Language \\nbut struggles with variable names. \\n \\nThis topic is in primitive stages of research and needs more \\nimprovement. In future when this technology is mature \\nenough, any person who does not know a programming \\nlanguage, will be able to write programs in that programming \\nlanguage.  \\n \\nVII. FUTURE SCOPE \\n \\nThe prototype we presented in this paper can be converted \\ninto a full system by adding functionalities like login, voice \\nauthentication, history of programs written, etc. Also, \\ncreating an extension for all major IDEs and text editors \\nwould be a great way for programmers to adapt to this \\ntechnology. By this, programmers will be able to easily write \\ncode and edit it if required in their favourite editor. The model \\ncan be improved by training the model on a dataset with more \\nnumber of records. Also experimentation can be done by \\nusing different tokenizers to improve the model even further. \\n \\n REFERENCES',\n",
       " 'can be improved by training the model on a dataset with more \\nnumber of records. Also experimentation can be done by \\nusing different tokenizers to improve the model even further. \\n \\n REFERENCES \\n \\n[1] Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model \\nfor general-purpose code generation. arXiv preprint arXiv:1704.01696  \\n[2] Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract \\nsyntax networks for code generation and semantic parsing. arXiv \\npreprint arXiv:1704.07535 .  \\n[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion \\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. \\nAttention is all you need. In Advances in Neural Information \\nProcessing Systems. pages 5998–6008. \\n[4] \\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan \\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. \\nExploring the limits of transfer learning with a unified text-to-text \\ntransformer. arXiv preprint arXiv:1910.10683',\n",
       " 'Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. \\nExploring the limits of transfer learning with a unified text-to-text \\ntransformer. arXiv preprint arXiv:1910.10683 \\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. \\n2018. Bert: Pre-training of deep bidirectional transformers for language \\nunderstanding. arXiv preprint arXiv:1810.04805 \\n[6] \\nChristopher Bryant, Mariano Felice, and Edward Briscoe. 2017. \\nAutomatic annotation and evaluation of error types for grammatical \\nerror correction. Association for Computational Linguistics. \\n[7] Yang Liu and Mirella Lapata. 2019. Text summarization with pre \\ntrained encoders. arXiv preprint arXiv:1908.08345 \\n[8] Paweł Budzianowski and Ivan Vulic. 2019. Hello, ´ it’s gpt-2–how can \\ni help you? towards the use of pre trained language models for task \\noriented dialogue systems. arXiv preprint arXiv:1907.05774.  \\n[9] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq:',\n",
       " 'oriented dialogue systems. arXiv preprint arXiv:1907.05774.  \\n[9] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: \\nGenerating sequences from structured representations of code. arXiv \\npreprint arXiv:1808.01400.  \\n[10] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian \\nWu, and Philip S Yu. 2018. Improving automatic source code \\nsummarization via deep reinforcement learning. In Proceedings of the \\n33rd ACM/IEEE International Conference on Automated Software \\nEngineering, pages 397–407.',\n",
       " '[11] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code \\nsearch. In Proceedings of the 40th International Conference on \\nSoftware Engineering, ICSE ’18, page 933–944, New York, NY, USA. \\nAssociation for Computing Machinery. \\n[12] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and \\nMarc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the \\nstate of semantic code search. arXiv preprint arXiv:1909.09436.  \\n[13] Juan Zhai, Xiangzhe Xu, Yu Shi, Minxue Pan, Shiqing Ma, Lei Xu, \\nWeifeng Zhang, Lin Tan, and Xiangyu Zhang. 2019. Cpc: \\nautomatically classifying and propagating natural language comments \\nvia program analysis. \\n[14] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. \\n2019. Pythia: Ai-assisted code completion system. In Proceedings of \\nthe 25th ACM SIGKDD International Conference on Knowledge \\nDiscovery & Data Mining, pages 2727–2735.  \\n[15] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel',\n",
       " 'the 25th ACM SIGKDD International Conference on Knowledge \\nDiscovery & Data Mining, pages 2727–2735.  \\n[15] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel \\nSundaresan. 2020. Intellicode compose: Code generation using \\ntransformers. arXiv preprint arXiv:2005.08025.  \\n[16] Y. Oda et al., \"Learning to Generate Pseudo-Code from Source Code \\nUsing Statistical Machine Translation,\" 2015 30th IEEE/ACM \\nInternational Conference on Automated Software Engineering (ASE), \\nLincoln, NE, USA, 2015, pp. 574-584, doi: 10.1/109/ASE.2015.36. \\n.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [docs.page_content for docs in chunk]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06dbf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings 497 generated\n",
      "successfully added 497 documents to vector store\n",
      "Total documents in collection: 497\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_manager.generate(texts)\n",
    "vector_store.add_docs(chunk, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e463501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAG at 0x21794998510>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrivel Pipeline\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, vector_store:VectorDB, embedding_manager:Embedding_manager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "    \n",
    "    def retrieve(self, query:str, top_k:int = 5, threshold:float = 0.0)-> List[Dict[str, Any]]:\n",
    "        print(f\"Top_k = {top_k}, threshold = {threshold}\")\n",
    "        try:\n",
    "            query_embeddings = self.embedding_manager.generate([query])[0]\n",
    "            \n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings = [query_embeddings.tolist()],\n",
    "                n_results = top_k,\n",
    "                include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "            )\n",
    "            \n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results[\"documents\"] and results[\"documents\"][0]:\n",
    "                documents = results[\"documents\"][0]\n",
    "                metadatas = results[\"metadatas\"][0]\n",
    "                distances = results[\"distances\"][0]\n",
    "                ids = results[\"ids\"][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    similarity = 1 - distance\n",
    "                    if similarity < threshold:\n",
    "                        continue\n",
    "                    retrieved_docs.append({\n",
    "                        \"ids\":doc_id,\n",
    "                        \"content\":document,\n",
    "                        \"metadata\":metadata,\n",
    "                        \"distance\":distance,\n",
    "                        \"rank\":i+1\n",
    "                    })\n",
    "            if not retrieved_docs:\n",
    "                print(\"Document not found\")\n",
    "                \n",
    "            return retrieved_docs\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"error during retrivel |-> {e}\")\n",
    "            return []\n",
    "        \n",
    "rag = RAG(vector_store, embedding_manager)\n",
    "rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76b113dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top_k = 5, threshold = 0.0\n",
      "Embeddings 1 generated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ids': 'Doc_47ae39f46f_265',\n",
       "  'content': 'Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n136 \\n4. Solutions and Advancements in RAG Systems \\nRecent years have witnessed significant advancements in RAG systems, addressing key challenges \\nin retrieval efficiency, scalability, and knowledge integration. This section explores two innovative \\napproaches that represent the cutting edge of RAG technology. Self-RAG, introduced by Asai et al., \\nand represents a significant shift in RAG system design. It incorporates retrieval, generation, and \\nevaluation into a single framework, allowing the model to iteratively improve its own performance. \\nThis self-improving capability addresses challenges in query reformulation and result quality \\nassessment, potentially leading to more accurate and relevant responses over time [15]. \\nGraphRAG, developed by Microsoft, takes a different approach by leveraging graph structures for',\n",
       "  'metadata': {'modDate': \"D:20241225165951+08'00'\",\n",
       "   'subject': '',\n",
       "   'trapped': '',\n",
       "   'title': '',\n",
       "   'id': 265,\n",
       "   'keywords': '',\n",
       "   'author': 'DRP',\n",
       "   'creator': 'Microsoft® Word LTSC',\n",
       "   'content': 912,\n",
       "   'producer': 'Microsoft® Word LTSC',\n",
       "   'total_pages': 7,\n",
       "   'creationdate': '2024-12-25T16:59:51+08:00',\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'format': 'PDF 1.7',\n",
       "   'creationDate': \"D:20241225165951+08'00'\",\n",
       "   'page': 4,\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'moddate': '2024-12-25T16:59:51+08:00'},\n",
       "  'distance': 0.6327662467956543,\n",
       "  'rank': 1},\n",
       " {'ids': 'Doc_170042d271_250',\n",
       "  'content': 'Highlights in Science, Engineering and Technology \\nCSIC 2024\\nVolume 124 (2025) \\n \\n133 \\nexplores promising future research directions that could address current limitations but may also \\nintroduce new challenges in data processing and model design. \\n2. Architecture of RAG System \\nRAG systems consist of three primary components: the retrieval component, the generation \\ncomponent, and the knowledge base. Each plays a crucial role in producing accurate, relevant, and \\nup-to-date responses (Fig.1). The system retrieves relevant content based on user queries using this \\nembedded knowledge base. The retrieved chunks are then combined with the original query to form \\na prompt, which is processed by a LLM to generate the final response.  \\n \\nFig 1. The workflow of a RAG system (Photo/Picture credit: Original).  \\n2.1. Retrieval Component \\nThe retrieval component serves as the critical bridge between user input and the vast repository of',\n",
       "  'metadata': {'subject': '',\n",
       "   'total_pages': 7,\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'creator': 'Microsoft® Word LTSC',\n",
       "   'creationDate': \"D:20241225165951+08'00'\",\n",
       "   'format': 'PDF 1.7',\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'modDate': \"D:20241225165951+08'00'\",\n",
       "   'author': 'DRP',\n",
       "   'id': 250,\n",
       "   'keywords': '',\n",
       "   'content': 939,\n",
       "   'moddate': '2024-12-25T16:59:51+08:00',\n",
       "   'trapped': '',\n",
       "   'title': '',\n",
       "   'creationdate': '2024-12-25T16:59:51+08:00',\n",
       "   'producer': 'Microsoft® Word LTSC',\n",
       "   'page': 1},\n",
       "  'distance': 0.6427403688430786,\n",
       "  'rank': 2},\n",
       " {'ids': 'Doc_121ea32303_246',\n",
       "  'content': \"in this domain. \\nKeywords: Retrieval augmented generation, natural language processing, information retrieval, \\nknowledge base. \\n1. Introduction \\nRetrieval-Augmented Generation (RAG) systems have emerged as a groundbreaking approach in \\nnatural language processing, tackling the fundamental limitations of traditional Large Language \\nModels (LLMs). By leveraging the power of LLMs with dynamic access to external knowledge, RAG \\nsystems represent a significant advancement in AI and Natural Language Processing (NLP) [1]. This \\ninnovative approach allows for the generation of more accurate, relevant, and up-to-date responses \\nacross a wide range of applications. The significance of RAG research lies in its potential to transform \\nAI applications fundamentally. These systems enhance text accuracy and relevance, improve AI's \\ncapability to handle complex, knowledge-intensive tasks, and enable continuous knowledge updating\",\n",
       "  'metadata': {'author': 'DRP',\n",
       "   'format': 'PDF 1.7',\n",
       "   'creator': 'Microsoft® Word LTSC',\n",
       "   'keywords': '',\n",
       "   'trapped': '',\n",
       "   'total_pages': 7,\n",
       "   'content': 927,\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'creationdate': '2024-12-25T16:59:51+08:00',\n",
       "   'creationDate': \"D:20241225165951+08'00'\",\n",
       "   'id': 246,\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'page': 0,\n",
       "   'producer': 'Microsoft® Word LTSC',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'modDate': \"D:20241225165951+08'00'\",\n",
       "   'moddate': '2024-12-25T16:59:51+08:00'},\n",
       "  'distance': 0.70029217004776,\n",
       "  'rank': 3},\n",
       " {'ids': 'Doc_26608876e5_245',\n",
       "  'content': 'examines the challenges faced by RAG systems and their solutions. It delves into the central \\narchitecture of RAG systems, encompassing retrieval components, generative components, and \\nknowledge bases, with a particular focus on recent advancements that have expanded the \\nboundaries of performance and functionality. The study critically analyzes major challenges such as \\nretrieval efficiency and dynamic knowledge management. This paper evaluates various advanced \\nsolutions proposed in recent literature, comparing their efficacy and discussing the trade-offs \\ninvolved. Ultimately, this paper aims to provide researchers, developers, and users of RAG systems \\nwith a comprehensive perspective, fostering ongoing innovation and the expansion of applications \\nin this domain. \\nKeywords: Retrieval augmented generation, natural language processing, information retrieval, \\nknowledge base. \\n1. Introduction',\n",
       "  'metadata': {'creator': 'Microsoft® Word LTSC',\n",
       "   'trapped': '',\n",
       "   'content': 908,\n",
       "   'total_pages': 7,\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-12-25T16:59:51+08:00',\n",
       "   'title': '',\n",
       "   'modDate': \"D:20241225165951+08'00'\",\n",
       "   'page': 0,\n",
       "   'creationDate': \"D:20241225165951+08'00'\",\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'author': 'DRP',\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'subject': '',\n",
       "   'id': 245,\n",
       "   'creationdate': '2024-12-25T16:59:51+08:00',\n",
       "   'producer': 'Microsoft® Word LTSC',\n",
       "   'format': 'PDF 1.7'},\n",
       "  'distance': 0.7005153894424438,\n",
       "  'rank': 4},\n",
       " {'ids': 'Doc_433dbef121_251',\n",
       "  'content': 'Fig 1. The workflow of a RAG system (Photo/Picture credit: Original).  \\n2.1. Retrieval Component \\nThe retrieval component serves as the critical bridge between user input and the vast repository of \\ninformation stored in the knowledge base. Its primary function is to identify and extract relevant \\ninformation based on the input query. This process involves several complex steps, including query \\nunderstanding, eﬀicient searching, and relevance ranking. \\nRecent advancements in dense retrieval methods, such as those proposed by Karpukhin et al., have \\nsignificantly improved the effectiveness of this component [4]. These methods leverage dense vector \\nrepresentations of both queries and documents, enabling more nuanced semantic matching compared \\nto traditional lexical retrieval approaches. The main advantage of this component lies in its ability to \\naccess and utilize vast amounts of external knowledge, potentially overcoming the limitations of static',\n",
       "  'metadata': {'file_path': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'keywords': '',\n",
       "   'producer': 'Microsoft® Word LTSC',\n",
       "   'creator': 'Microsoft® Word LTSC',\n",
       "   'author': 'DRP',\n",
       "   'format': 'PDF 1.7',\n",
       "   'page': 1,\n",
       "   'creationDate': \"D:20241225165951+08'00'\",\n",
       "   'id': 251,\n",
       "   'modDate': \"D:20241225165951+08'00'\",\n",
       "   'total_pages': 7,\n",
       "   'creationdate': '2024-12-25T16:59:51+08:00',\n",
       "   'subject': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG.pdf',\n",
       "   'content': 963,\n",
       "   'trapped': '',\n",
       "   'moddate': '2024-12-25T16:59:51+08:00',\n",
       "   'title': ''},\n",
       "  'distance': 0.7022639513015747,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag.retrieve(\"what is RAG\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8511af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Next Plan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
